# %% [markdown]
# # Lift Test Example: Measuring Campaign Impact with Prior Lift Experiments
#
# Imagine you’re the data scientist at ABCD Retail,
# responsible for measuring how different marketing
# investments drive daily online revenue. You have two
# advertising channels (say, Search Ads and Social Media),
# but they’ve historically been purchased together—making
# it hard to know which channel is really moving the needle.
#
# Fortunately, last quarter your team ran two small lift tests (randomized spend changes) on each channel,
#  giving isolated measurements of their true impact.
#
# In this tutorial, you’ll learn how to:
#
# 1.	Load and inspect a synthetic  dataset: daily revenue plus two correlated ad‐spend time-series.
# 2.	Fit a basic Prophetverse model (trend + seasonality + channel effects) and see how correlation confounds estimates.
# 3.	Incorporate lift-test results via a LiftExperimentLikelihood to add a custom likelihood term.
# 4.	Refit the model and compare how lift tests sharpen your estimated channel effects.

# %% {"tags" : ["remove_cell"]}
import warnings

warnings.filterwarnings("ignore")


# %%[markdown]
# ## 1. Load the data
#
# We import this synthetic dataset from the package. The loader function returns
# four objects: the target variable `y`, the exogenous variables `X`,
# the lift test dataframe `lift_test`, and the true components of the timeseries,
# including the true effect of the exogenous variables.

# %%
import numpyro

numpyro.enable_x64()  # Enable 64-bit precision

import matplotlib.pyplot as plt

plt.style.use("seaborn-v0_8-whitegrid")

from prophetverse.datasets._mmm.lifttest import get_dataset

y, X, lift_test, true_components, model = get_dataset()


# %% [markdown]
# * **Why synthetic?** This lets us compare against known “true” effects and verify our approach.
# * **Lift tests** are recorded as small tables where on each test start‐date we changed spend,
# measured the relative change in revenue, and recorded `x_start`, `x_end` and `lift`
#
# ## 2. Quick data exploration
#
# ### 2.1 Revenue and spend over time
# %%

fig, ax = plt.subplots(figsize=(10, 10), nrows=3, sharex=True)
y.plot.line(ax=ax[0])
ax[0].set_title("Daily Revenue")
X.plot.line(alpha=0.9, ax=ax[1])
ax[1].set_title("Ad Spend per Channel")
true_components[["ad_spend_search", "ad_spend_social_media"]].plot.line(ax=ax[2])
ax[2].set_title("Ground-Truth Per-Channel Effects (not observed)")
fig.show()

# %% [markdown]
# * The top plot shows our target: daily revenue
# * The middle plot shows two correlated spend series - hard to tell which moves
# moves revenue more
#
# The correlation between the two ad spend series is high:

# %%
X[["ad_spend_search", "ad_spend_social_media"]].corr()

# %% [markdown]
# ### 2.2 Model components (truth)
#
# We can peek under the hood of the generative model to see its built-in trend and seasonality:
# %%
true_components[
    [
        "trend",
        "yearly_seasonality",
        "weekly_seasonality",
        "ad_spend_search",
        "ad_spend_social_media",
    ]
].plot(
    subplots=True,
    figsize=(10, 14),
    title=[
        "Trend",
        "Yearly Seasonality",
        "Weekly Seasonality",
        "Effect: Search Ads",
        "Effect: Social Media",
    ],
)
plt.tight_layout()
plt.show()
# %% [markdown]
# The lift test dataframe looks like below, with the information
# of how a delta in the exogenous variable (ad_spend_search and ad_spend_social_media)
# affects the KPI. Note that the lift test
# is assigned to dates, since the effect of the intervention can
# vary with time.


# %%

lift_test1, lift_test2 = lift_test
lift_test1

# %% [markdown]

# In the next figure, we visualize each of our lift‐test experiments
# on the Search Ads channel.
#
# Each circle marks the pre‐test spend (baseline revenue ratio = 1.0) and each
#  “×” marks the observed post‐test revenue ratio.  The arrows connect
# before‐and‐after points for each experiment, making it easy to see how
# different spend increases translated into revenue lift.

# %%
fig, ax = plt.subplots(figsize=(8, 6))

# Pre-test baseline (revenue ratio = 1)
ax.scatter(
    lift_test1["x_start"], [1] * len(lift_test1), label="Baseline Spend", alpha=0.6
)

# Post-test observed lift
ax.scatter(lift_test1["x_end"], lift_test1["lift"], label="Observed Lift", alpha=0.6)

# Draw arrows from baseline to observed
for _, row in lift_test1.iterrows():
    ax.annotate(
        "",
        xy=(row["x_end"], row["lift"]),
        xytext=(row["x_start"], 1),
        arrowprops=dict(arrowstyle="->", alpha=0.5),
    )

# Reference line at lift = 1
ax.axhline(1, linestyle="--", linewidth=1)

# Labels and title
ax.set_xlabel("Ad Spend")
ax.set_ylabel("Revenue Ratio (Post / Pre)")
ax.set_title("Illustration of Lift Test Experiments")
ax.legend()
plt.tight_layout()
plt.show()

# %% [markdown]
# Although we could in principle hand‐tune our Hill‐effect priors so that the maximum effect and curvature roughly match the observed lift, wrapping each channel’s base effect in a lift‐experiment likelihood lets the model:
#
# 1.	Respect prior uncertainty – we keep our original, weakly informative priors on curve shape and allow the data to speak.
# 2.	Formally weight evidence – the lift tests contribute an extra likelihood term, so large discrepancies between predicted and observed lift are down‐weighted only to the extent of their measurement noise.
# 3.	Integrate all sources jointly – time‐series and experimental data are fused in a single probabilistic model, yielding coherent uncertainty estimates.

# %% [markdown]
# ## 3. Baseline Model: Trend + Seasonality + Channel Effects
#
# First, let’s fit a standard Prophetverse model without lift-test data. We’ll include:
# * Piecewise linear trend
# * Three seasonalities (annual, monthly, weekly)
# * Two Hill effects for our advertising channels
# %%

import matplotlib.pyplot as plt
import numpyro.distributions as dist
from prophetverse.effects import (
    PiecewiseLinearTrend,
    LinearFourierSeasonality,
    ChainedEffects,
    GeometricAdstockEffect,
    HillEffect,
)
from prophetverse.sktime import Prophetverse
from prophetverse.engine import MAPInferenceEngine
from prophetverse.engine.optimizer import LBFGSSolver

# Define seasonalities
yearly = (
    "yearly_seasonality",
    LinearFourierSeasonality(
        freq="D",
        sp_list=[365.25],
        fourier_terms_list=[5],
        prior_scale=0.1,
        effect_mode="multiplicative",
    ),
    None,
)
weekly = (
    "weekly_seasonality",
    LinearFourierSeasonality(
        freq="D",
        sp_list=[7],
        fourier_terms_list=[3],
        prior_scale=0.05,
        effect_mode="multiplicative",
    ),
    None,
)
# Adstock + saturation effect
hill = HillEffect(
    half_max_prior=dist.HalfNormal(1),
    slope_prior=dist.InverseGamma(2, 1),
    max_effect_prior=dist.HalfNormal(1),
    effect_mode="additive",
    input_scale=1e6,
)
chained_search = (
    "ad_spend_search",
    ChainedEffects(
        [
            ("adstock", GeometricAdstockEffect()),
            ("saturation", hill),
        ]
    ),
    "ad_spend_search",
)
chained_social = (
    "ad_spend_social_media",
    ChainedEffects(
        [
            ("adstock", GeometricAdstockEffect()),
            ("saturation", hill),
        ]
    ),
    "ad_spend_social_media",
)

# Build and fit model
model = Prophetverse(
    trend=PiecewiseLinearTrend(changepoint_interval=100),
    exogenous_effects=[yearly, weekly, chained_search, chained_social],
    inference_engine=MAPInferenceEngine(
        num_steps=5000, optimizer=LBFGSSolver(memory_size=200, max_linesearch_steps=200)
    ),
)


model.fit(y=y, X=X)
components = model.predict_components(fh=X.index, X=X)

# %% [markdown]
# Note on inference: We use MAP (point-estimate) here for speed;
# you could swap in MCMC if you want full posteriors.
#
# ### 3.1 How Well Did We Fit?
# %%

fig, ax = plt.subplots(figsize=(10, 5))
y.plot.line(ax=ax, color="black", label="Sales")
components["mean"].to_frame("Forecast").plot.line(ax=ax)
fig.show()

# %% [markdown]
# Looks like the model tracks trend and seasonality, but how about the channel effects?

# %%

fig, axs = plt.subplots(figsize=(10, 10), nrows=2, sharex=True)

ax = axs[0]
X["ad_spend_search"].plot.line(ax=ax, label="True")
components["ad_spend_search"].plot.line(ax=ax, label="Inferred effect")
ax.set(
    title="ad_spend_search",
    ylabel="Effect",
    xlabel="Ad Spend",
)
ax.legend()

ax = axs[1]
X["ad_spend_social_media"].plot.line(ax=ax, label="True")
components["ad_spend_social_media"].plot.line(ax=ax, label="Inferred effect")
ax.set(
    title="ad_spend_social_media",
    ylabel="Effect",
    xlabel="Ad Spend",
)
ax.legend()
fig.show()
# %% [markdown]
# **Observation**: Because the two spend series move together, the model can’t tell if Search or Social is driving revenue—it “splits” credit arbitrarily, leading to biased effect curves.
# The saturation curves look different from the true ones.
# %%

fig, axs = plt.subplots(figsize=(10, 10), nrows=2, sharex=True)

ax = axs[0]

ax.scatter(X["ad_spend_search"], components["ad_spend_search"], label="Inferred effect")
ax.scatter(
    X["ad_spend_search"],
    true_components["ad_spend_search"],
    label="True effect",
    color="black",
)
ax.set(
    title="ad_spend_search",
    xlabel="Ad Spend",
    ylabel="Effect",
)
ax.legend()

ax = axs[1]
ax.scatter(X["ad_spend_social_media"], components["ad_spend_social_media"])
ax.scatter(
    X["ad_spend_social_media"], true_components["ad_spend_social_media"], color="black"
)
ax.set(title="ad_spend_social_media", xlabel="Ad Spend", ylabel="Effect")

fig.show()

# %% [markdown]

# ## Using lift test to improve the estimation
# We will use the lift test to improve the estimation of the effect of the exogenous variables.
# We wrap the original effects of `ad_spend_search` and `ad_spend_social_media` in a `LiftExperimentLikelihood` effect.
# This effect will use the lift test data to add a new likelihood term to the model.
#
# ## 4. Integrate Lift-Test Data
# Considering that our team had run two lift tests, we can use this info
# using `LiftExperimentLikelihood` to improve the estimation of the effect of
# the exogenous variables.
#
# ### 4.1 Creating the new model
# %%

from prophetverse.effects.lift_likelihood import LiftExperimentLikelihood

lift_experiment_effect1 = LiftExperimentLikelihood(
    effect=model.get_params()["ad_spend_search"],
    lift_test_results=lift_test1,
    prior_scale=0.01,
)

lift_experiment_effect2 = LiftExperimentLikelihood(
    effect=model.get_params()["ad_spend_social_media"],
    lift_test_results=lift_test2,
    prior_scale=0.01,
)

# %% [markdown]
# The `prior_scale` parameter controls that amount of noise we believe there is
# in the lift test data. Inter

# %% [markdown]
# ### 4.2 Fitting the new model

# %%
new_model = model.clone()
new_model.set_params(
    ad_spend_search=lift_experiment_effect1,
    ad_spend_social_media=lift_experiment_effect2,
)
new_model.fit(y=y, X=X)


# %% [markdown]
# ### 4.2 Compare forecasts

# %%
new_components = new_model.predict_components(fh=X.index, X=X)

fig, ax = plt.subplots(figsize=(10, 5))
y.plot.line(ax=ax, color="black")
components["obs"].plot.line(ax=ax, label="Baseline model")
new_components["obs"].plot.line(ax=ax, label="With lift tests")

# ### 4.3 Compare the effect curves
# %%
fig, axs = plt.subplots(figsize=(10, 10), nrows=2, sharex=True)

ax = axs[0]

ax.scatter(
    X["ad_spend_search"],
    components["ad_spend_search"],
    label="Baseline model",
    alpha=0.5,
)
ax.scatter(
    X["ad_spend_search"],
    new_components["ad_spend_search"],
    label="With lift tests",
    alpha=0.5,
)
ax.scatter(
    X["ad_spend_search"],
    true_components["ad_spend_search"],
    label="True effect",
    color="black",
)
ax.set_title("ad_spend_search")
ax.legend()

ax = axs[1]
ax.scatter(
    X["ad_spend_social_media"],
    components["ad_spend_social_media"],
    label="Baseline model",
    alpha=0.5,
)
ax.scatter(
    X["ad_spend_social_media"],
    new_components["ad_spend_social_media"],
    label="With lift tests",
    alpha=0.5,
)
ax.scatter(
    X["ad_spend_social_media"],
    true_components["ad_spend_social_media"],
    color="black",
    label="True effect",
)
ax.set_title("ad_spend_social_media")
ax.legend()
fig.show()


# %%

for component_name in [
    "trend",
    "yearly_seasonality",
    "ad_spend_search",
    "ad_spend_social_media",
]:
    fig, ax = plt.subplots(figsize=(10, 5))
    true_components[component_name].plot.line(ax=ax, label="True effect", color="black")
    components[component_name].plot.line(ax=ax, label="Baseline model")
    new_components[component_name].plot.line(ax=ax, label="With lift tests")
    ax.set_title(component_name)
    ax.legend()
    fig.show()

# %% [markdown]
#
# Much better: now the model can disentangle the two channels and
# estimate their effects more accurately.
#
# ## Attribution models and the road to Unified Marketing Measurement
#
# The lift test likelihood lets us disentangle correlated drivers,
# but we can still use more information to guide our model torwards
# Unified Marketing Measurement (UMM), with the idea of having a single,
# general model that can accurately estimate the effect of all channels.
#
# Now, we will add another piece of information to the model: the
# results of attribution models, that can say day by day how much
# was the effect of each channel. We will add a specific likelihood
# term for this, that will be used to estimate the effect of
# each channel.
#
# We will build a synthetic output of such attribution model:

# %%
import numpy as np

# For reproducibility
rng = np.random.default_rng(42)
attribution_results_search = true_components[["ad_spend_search"]] * rng.normal(
    1, 0.1, size=true_components.shape[0]
).reshape((-1, 1))

attribution_results_social_media = true_components[
    ["ad_spend_social_media"]
] * rng.normal(1, 0.1, size=true_components.shape[0]).reshape((-1, 1))

attribution_results_search

# %%

from prophetverse.effects import ExactLikelihood

likelihood_effect_search = (
    "attribution_search",
    ExactLikelihood(
        effect_name="ad_spend_search",
        reference_df=attribution_results_search,
        prior_scale=0.05,
    ),
    None,
)

likelihood_effect_social_media = (
    "attribution_social_media",
    ExactLikelihood(
        effect_name="ad_spend_social_media",
        reference_df=attribution_results_social_media,
        prior_scale=0.05,
    ),
    None,
)

model_umm = model.clone()
model_umm.set_params(
    exogenous_effects=model.get_params()["exogenous_effects"]
    + [likelihood_effect_search, likelihood_effect_social_media]
)
model_umm


# %%
model_umm.fit(y=y, X=X)
y_pred_components_umm = model_umm.predict_components(X=X, fh=X.index)


# %%

fig, axs = plt.subplots(figsize=(10, 10), nrows=2, sharex=True)

ax = axs[0]

ax.scatter(
    X["ad_spend_search"],
    components["ad_spend_search"],
    label="Baseline model",
    alpha=0.5,
)
ax.scatter(
    X["ad_spend_search"],
    new_components["ad_spend_search"],
    label="With lift tests",
    alpha=0.5,
)
ax.scatter(
    X["ad_spend_search"],
    y_pred_components_umm["ad_spend_search"],
    label="With attribution model",
    alpha=0.5,
)
ax.plot(
    X["ad_spend_search"],
    true_components["ad_spend_search"],
    label="True effect",
    color="black",
)
ax.set_title("ad_spend_search")
ax.legend()

ax = axs[1]
ax.scatter(
    X["ad_spend_social_media"],
    components["ad_spend_social_media"],
    label="Baseline model",
    alpha=0.5,
)
ax.scatter(
    X["ad_spend_social_media"],
    new_components["ad_spend_social_media"],
    label="With lift tests",
    alpha=0.5,
)
ax.scatter(
    X["ad_spend_social_media"],
    y_pred_components_umm["ad_spend_social_media"],
    label="With attribution model",
    alpha=0.5,
)
ax.plot(
    X["ad_spend_social_media"],
    true_components["ad_spend_social_media"],
    color="black",
    label="True effect",
)
ax.set_title("ad_spend_social_media")
ax.legend()
fig.show()

# %% [markdown]
# ## 5. Conclusions
# 1. Correlated drivers can’t be disentangled by observational data alone.
# 2. Lift tests provide isolated “delta” measurements that break correlation.
# 3. Wrapping your base effect in LiftExperimentLikelihood lets Prophetverse jointly fit both time-series and lift-test data.
# 4. Result: Sharper, more accurate estimates of each channel’s ROI.
# 5. You can also add attribution model results to the model, to improve the estimation of the effect of each channel.
#
# With Prophetverse’s lift-test integration, you can combine the strengths of time-series forecasting and experimental design for robust marketing mix modeling.
