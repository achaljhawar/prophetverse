[
  {
    "objectID": "howto/composite_effects.html",
    "href": "howto/composite_effects.html",
    "title": "Composition of effects",
    "section": "",
    "text": "In previous examples, we saw how to create a simple custom effect, which applies a simple transformation to the input data. However, the effect’s interface allows us to apply more complex transformations, such as using the output of previous components as input for the current component, or creating a composite effect that wraps an effect and applies some sort of transformation. This example will cover these topics.",
    "crumbs": [
      "Creating composite effects"
    ]
  },
  {
    "objectID": "howto/composite_effects.html#creating-a-custom-effect",
    "href": "howto/composite_effects.html#creating-a-custom-effect",
    "title": "Composition of effects",
    "section": "Creating a custom effect",
    "text": "Creating a custom effect\nThe idea here is to create an effect that uses another predicted component to scale the impact of an exogenous variable.\nOne classic use-case for this would be using seasonality to scale the effect of investment, that might be proportional to it. Marketing investments are a good example of this. We will implement such a composite effect in this section.\n\nExample dataset\nThe dataset we use is synthetic, and the relation between the exogenous variable and the target is known. However, let’s pretend we don’t know this relation, and analize the data to find some insights that motivate the creation of a custom effect. The dataset has a target variable, which is a time series, and an exogenous variable, which is the investment made for each date.\n\nimport numpyro\nimport numpyro.distributions as dist\nfrom matplotlib import pyplot as plt\nfrom sktime.split import temporal_train_test_split\nfrom sktime.utils.plotting import plot_series\n\nfrom prophetverse.datasets.synthetic import load_composite_effect_example\n\nnumpyro.enable_x64()\n\ny, X = load_composite_effect_example()\n\ny_train, y_test, X_train, X_test = temporal_train_test_split(y, X, test_size=365)\n\ndisplay(y_train.head())\ndisplay(X_train.head())\n\n\n\n\n\n\n\n\ntarget\n\n\ntime\n\n\n\n\n\n2010-01-01\n29.375431\n\n\n2010-01-02\n30.268786\n\n\n2010-01-03\n29.128912\n\n\n2010-01-04\n31.014165\n\n\n2010-01-05\n31.890928\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninvestment\n\n\ntime\n\n\n\n\n\n2010-01-01\n0.198274\n\n\n2010-01-02\n0.198274\n\n\n2010-01-03\n0.198274\n\n\n2010-01-04\n0.198274\n\n\n2010-01-05\n0.207695\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(8,5))\nplot_series(y_train, y_test, labels=[\"Train\", \"Test\"], title=\"Target series\", ax=ax)\nfig.show()\n\n/var/folders/_2/9y4tsvdd2n3gqjgd2zmlr1km0000gn/T/ipykernel_78959/2176855586.py:3: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n  fig.show()\n\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(8, 5))\nplot_series(X[\"investment\"], labels=[\"investment\"], title=\"Features\", ax=ax)\nfig.show()\n\n/var/folders/_2/9y4tsvdd2n3gqjgd2zmlr1km0000gn/T/ipykernel_78959/4106615624.py:3: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n  fig.show()\n\n\n\n\n\n\n\n\n\nThe timeseries has a yearly seasonality, and it seems that some oscillations are proportional to the investment. Below, we model the timeseries with a simple linear effect between the investment and the target, and a yearly seasonality based on fourier terms. Then, we will analize the residuals to see if there is any pattern that we can capture with a custom effect.\n\nfrom prophetverse.effects import LinearEffect\nfrom prophetverse.effects.fourier import LinearFourierSeasonality\nfrom prophetverse.effects.trend import PiecewiseLinearTrend\nfrom prophetverse.engine import MAPInferenceEngine\nfrom prophetverse.engine.optimizer import LBFGSSolver\nfrom prophetverse.sktime import Prophetverse\nfrom prophetverse.utils.regex import exact, no_input_columns\n\nmodel = Prophetverse(\n    trend=PiecewiseLinearTrend(\n        changepoint_interval=500,\n        changepoint_prior_scale=0.00001,\n        changepoint_range=-500,\n    ),\n    exogenous_effects=[\n        (\n            \"seasonality\",\n            LinearFourierSeasonality(\n                freq=\"D\",\n                sp_list=[365.25],\n                fourier_terms_list=[5],\n                prior_scale=1,\n                effect_mode=\"multiplicative\",\n            ),\n            no_input_columns,\n        ),\n        (\n            \"investment\",\n            LinearEffect(\"multiplicative\", prior=dist.Normal(0, 1)),\n            exact(\"investment\"),\n        ),\n    ],\n    inference_engine=MAPInferenceEngine(\n        optimizer=LBFGSSolver(memory_size=100, max_linesearch_steps=100),\n        progress_bar=True,\n    ),\n)\n\nmodel.fit(y=y_train, X=X_train)\nmodel\n\n/Users/felipeangelim/Workspace/prophetverse/src/prophetverse/sktime/univariate.py:214: UserWarning: No columns match the regex ^$\n  self._fit_effects(X, y)\n  0%|          | 0/1 [00:00&lt;?, ?it/s]100%|██████████| 1/1 [00:02&lt;00:00,  2.98s/it, init loss: -5352.1071, avg. loss [1-1]: -5352.1071]100%|██████████| 1/1 [00:02&lt;00:00,  2.98s/it, init loss: -5352.1071, avg. loss [1-1]: -5352.1071]\n\n\nProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[5],\n                                                          freq='D',\n                                                          prior_scale=1,\n                                                          sp_list=[365.25]),\n                                 '^$'),\n                                ('investment',\n                                 LinearEffect(prior=&lt;numpyro.distributions.continuous.Normal object at 0x329e29a50 with batch shape () and event shape ()&gt;),\n                                 '^investment$')],\n             inference_engine=MAPInferenceEngine(optimizer=LBFGSSolver(max_linesearch_steps=100,\n                                                                       memory_size=100),\n                                                 progress_bar=True),\n             trend=PiecewiseLinearTrend(changepoint_interval=500,\n                                        changepoint_prior_scale=1e-05,\n                                        changepoint_range=-500))Please rerun this cell to show the HTML repr or trust the notebook.ProphetverseProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[5],\n                                                          freq='D',\n                                                          prior_scale=1,\n                                                          sp_list=[365.25]),\n                                 '^$'),\n                                ('investment',\n                                 LinearEffect(prior=&lt;numpyro.distributions.continuous.Normal object at 0x329e29a50 with batch shape () and event shape ()&gt;),\n                                 '^investment$')],\n             inference_engine=MAPInferenceEngine(optimizer=LBFGSSolver(max_linesearch_steps=100,\n                                                                       memory_size=100),\n                                                 progress_bar=True),\n             trend=PiecewiseLinearTrend(changepoint_interval=500,\n                                        changepoint_prior_scale=1e-05,\n                                        changepoint_range=-500))effectsPiecewiseLinearTrendPiecewiseLinearTrend(changepoint_interval=500, changepoint_prior_scale=1e-05,\n                     changepoint_range=-500)LinearFourierSeasonalityLinearFourierSeasonality(effect_mode='multiplicative', fourier_terms_list=[5],\n                         freq='D', prior_scale=1, sp_list=[365.25])LinearEffectLinearEffect(prior=&lt;numpyro.distributions.continuous.Normal object at 0x329e29a50 with batch shape () and event shape ()&gt;)inference_engineMAPInferenceEngineMAPInferenceEngine(optimizer=LBFGSSolver(max_linesearch_steps=100,\n                                         memory_size=100),\n                   progress_bar=True)\n\n\nWe plot the predictions on training set to see if the model performs well.\n\ny_pred = model.predict(X=X_train, fh=y_train.index)\n\nfig, ax = plt.subplots(figsize=(8, 5))\nplot_series(y_train, y_pred, labels=[\"Train\", \"Pred\"], title=\"Target series\", ax=ax)\nfig.show()\n\n/var/folders/_2/9y4tsvdd2n3gqjgd2zmlr1km0000gn/T/ipykernel_78959/2970447741.py:5: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n  fig.show()\n\n\n\n\n\n\n\n\n\nWe can see that some peaks are not captured by the model. Our hypothesis to explain this phenomenon is that the investment has more impact on the target when it is done during the positive seasonality periods. To test this, we plot the residuals of the model against the investment, and color the points based on the seasonality component. We can see that slopes are different for positive and negative seasonality, which indicates that our hypothesis is possibly correct.\n\ncomponents = model.predict_components(X=X_train, fh=y_train.index)\n\nresidual = y_train[\"target\"] - components[\"mean\"]\n\nfig, ax = plt.subplots()\nax.scatter(\n    X_train[\"investment\"],\n    residual,\n    c=components[\"seasonality\"] &lt; 0,\n    cmap=\"Accent\",\n    alpha=0.9,\n)\n# Create legend manually\ncolors = plt.cm.get_cmap(\"Accent\").colors\nax.scatter([], [], color=colors[0], label=\"Positive seasonality\")\nax.scatter([], [], color=colors[1], label=\"Negative seasonality\")\nax.legend()\nax.set(xlabel=\"Investment\", ylabel=\"Residual\", title=\"Residuals vs Investment\")\nfig.show()\n\n/var/folders/_2/9y4tsvdd2n3gqjgd2zmlr1km0000gn/T/ipykernel_78959/182972736.py:14: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n  colors = plt.cm.get_cmap(\"Accent\").colors\n/var/folders/_2/9y4tsvdd2n3gqjgd2zmlr1km0000gn/T/ipykernel_78959/182972736.py:19: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n  fig.show()",
    "crumbs": [
      "Creating composite effects"
    ]
  },
  {
    "objectID": "howto/composite_effects.html#creating-the-composite-effect",
    "href": "howto/composite_effects.html#creating-the-composite-effect",
    "title": "Composition of effects",
    "section": "Creating the composite effect",
    "text": "Creating the composite effect\nTo model this behaviour with Prophetverse, we will create a custom effect, that scales a new effect by the output of a previous component. The _fit and _transform methods call the inner effect’s methods, and the predict method multiplies the inner effect’s predictions by the seasonality, which is passed as base_effect_name.\n\nfrom typing import Any, Dict, List\n\nimport jax.numpy as jnp\nimport pandas as pd\n\nfrom prophetverse.effects.base import BaseEffect\n\n\nclass WrapEffectAndScaleByAnother(BaseEffect):\n    \"\"\"Wrap an effect and scale it by another effect.\n\n    Parameters\n    ----------\n    effect : BaseEffect\n        The effect to wrap.\n\n    \"\"\"\n\n    _tags = {\"requires_X\": False, \"capability:panel\": False}\n\n    def __init__(\n        self,\n        effect: BaseEffect,\n        base_effect_name: str,\n    ):\n\n        self.effect = effect\n        self.base_effect_name = base_effect_name\n\n        super().__init__()\n\n        self.clone_tags(effect)\n\n    def _fit(self, y: pd.DataFrame, X: pd.DataFrame, scale: float = 1):\n        \"\"\"Initialize the effect.\"\"\"\n        self.effect.fit(X=X, y=y, scale=scale)\n\n    def _transform(self, X: pd.DataFrame, fh: pd.Index) -&gt; Dict[str, Any]:\n        \"\"\"Prepare input data to be passed to numpyro model.\"\"\"\n        return self.effect.transform(X=X, fh=fh)\n\n    def _predict(\n        self, data: Dict, predicted_effects: Dict[str, jnp.ndarray], *args, **kwargs\n    ) -&gt; jnp.ndarray:\n        \"\"\"Apply and return the effect values.\"\"\"\n        out = self.effect.predict(data=data, predicted_effects=predicted_effects)\n\n        base_effect = predicted_effects[self.base_effect_name]\n        return base_effect * out",
    "crumbs": [
      "Creating composite effects"
    ]
  },
  {
    "objectID": "howto/composite_effects.html#instantiating-the-model-with-the-composite-effect",
    "href": "howto/composite_effects.html#instantiating-the-model-with-the-composite-effect",
    "title": "Composition of effects",
    "section": "Instantiating the model with the composite effect",
    "text": "Instantiating the model with the composite effect\nTo create the model, we use the model instance we have, and the rshift operator to append the composite effect to the model.\n\nimport numpyro.distributions as dist\nfrom prophetverse.engine.optimizer import AdamOptimizer\n\ncomposite_effect_tuple = (\n    \"investment_seasonality\",  # The effect ID, can be what you want\n    WrapEffectAndScaleByAnother(\n        effect=LinearEffect(\"multiplicative\", prior=dist.HalfNormal(1)),\n        base_effect_name=\"seasonality\",\n    ),\n    exact(\"investment\"),\n)\n\n\n# We use the rshift operator to append an effect to the model\nmodel_composite = model &gt;&gt; composite_effect_tuple\n\nmodel_composite.fit(y=y_train, X=X_train)\ny_pred_composite = model_composite.predict(X=X_train, fh=y_train.index)\n\n/Users/felipeangelim/Workspace/prophetverse/src/prophetverse/sktime/univariate.py:214: UserWarning: No columns match the regex ^$\n  self._fit_effects(X, y)\n/Users/felipeangelim/Workspace/prophetverse/src/prophetverse/sktime/univariate.py:214: UserWarning: Columns {'investment'} are already set\n  self._fit_effects(X, y)\n  0%|          | 0/1 [00:00&lt;?, ?it/s]100%|██████████| 1/1 [00:03&lt;00:00,  3.87s/it, init loss: -6054.6937, avg. loss [1-1]: -6054.6937]100%|██████████| 1/1 [00:03&lt;00:00,  3.87s/it, init loss: -6054.6937, avg. loss [1-1]: -6054.6937]\n\n\nWe can see below how these oscilations are captured by the model correctly when adding this joint effect.\n\nfig, ax = plt.subplots(figsize=(8, 5))\nplot_series(y_train, y_pred_composite, labels=[\"Train\", \"Pred\"], title=\"Target series\",ax=ax)\nfig.show()\n\n/var/folders/_2/9y4tsvdd2n3gqjgd2zmlr1km0000gn/T/ipykernel_78959/1536142040.py:3: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n  fig.show()",
    "crumbs": [
      "Creating composite effects"
    ]
  },
  {
    "objectID": "howto/composite_effects.html#evaluating-the-model-on-test-set",
    "href": "howto/composite_effects.html#evaluating-the-model-on-test-set",
    "title": "Composition of effects",
    "section": "Evaluating the model on test set",
    "text": "Evaluating the model on test set\nWe compare to the previous model to see if the new effect improved the predictions on test set:\n\ny_pred_composite = model_composite.predict(X=X_test, fh=y_test.index)\ny_pred = model.predict(X=X_test, fh=y_test.index)\n\n\nfig, ax = plt.subplots(figsize=(8, 5))\nplot_series(\n    y_test,\n    y_pred,\n    y_pred_composite,\n    labels=[\"Test\", \"Pred\", \"Pred composite\"],\n    title=\"Target series\",\n    ax=ax,\n)\n\nplt.show()",
    "crumbs": [
      "Creating composite effects"
    ]
  },
  {
    "objectID": "howto/composite_effects.html#extracting-the-components",
    "href": "howto/composite_effects.html#extracting-the-components",
    "title": "Composition of effects",
    "section": "Extracting the components",
    "text": "Extracting the components\nThe components can be extracted as usual, with the predict_components method.\n\ncomponents = model_composite.predict_components(fh=y_test.index, X=X_test)\n\nfig, ax = plt.subplots(figsize=(10, 5))\ncomponents.plot.line(ax=ax)\nax.set_title(\"Predicted Components\")\nfig.show()\n\n/var/folders/_2/9y4tsvdd2n3gqjgd2zmlr1km0000gn/T/ipykernel_78959/2786771579.py:6: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n  fig.show()",
    "crumbs": [
      "Creating composite effects"
    ]
  },
  {
    "objectID": "howto/custom_effects.html",
    "href": "howto/custom_effects.html",
    "title": "Customizing Exogenous Effects in Prophetverse",
    "section": "",
    "text": "The exogenous effect API allows you to create custom exogenous components for the Prophetverse model. This is useful when we want to model specific patterns or relationships between the exogenous variables and the target variable. For example, enforcing a positive effect of a variable on the mean, or modeling a non-linear relationship.\nIf you have read the theory section, by effect we mean each function \\(f_i\\). You can implement those custom functions by subclassing the BaseEffect class, and then use them in the Prophetverse model. Some effects are already implemented in the library, and you can find them in the prophetverse.effects module.\nWhen creating a model instance, effects can be specified through exogenous_effects parameter of the Prophetverse model. This parameter is a list of tuples of three values: the name, the effect object, and a regex to filter columns related to that effect. The regex is what defines \\(x_i\\) in the previous section. The prophetverse.utils.regex module provides some useful functions to create regex patterns for common use cases, include starts_with, ends_with, contains, and no_input_columns.\nConsider the example below, where we create a model with a linear seasonality effect and a custom effect that uses the feature channel1_investment as input and transforms it with a hill curve, which is a common curve for capturing diminishing returns.\n\nfrom prophetverse.effects import HillEffect, LinearFourierSeasonality\nfrom prophetverse.sktime import Prophetverse\nfrom prophetverse.utils.regex import exact, no_input_columns, starts_with\n\nexogenous_effects = [\n    (\n        \"seasonality\",\n        LinearFourierSeasonality(\n            freq=\"D\",\n            sp_list=[7, 365.25],\n            fourier_terms_list=[3, 10],\n            prior_scale=0.1,\n            effect_mode=\"multiplicative\",\n        ),\n        no_input_columns,\n    ),\n    (\n        \"channel1_investment_incremental\",\n        HillEffect(effect_mode=\"additive\"),\n        exact(\"channel1_investment\"),\n    ),\n]\n\nmodel = Prophetverse(exogenous_effects=exogenous_effects)\n\n/Users/felipeangelim/Workspace/prophetverse/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\nCreating such models in Prophetverse is like creating buildings from lego blocks. You define how you model should work, and then you can leverage all the interface to carry out the forecasting and inference tasks.",
    "crumbs": [
      "Custom exogenous effect"
    ]
  },
  {
    "objectID": "howto/custom_effects.html#the-effects-api",
    "href": "howto/custom_effects.html#the-effects-api",
    "title": "Customizing Exogenous Effects in Prophetverse",
    "section": "",
    "text": "The exogenous effect API allows you to create custom exogenous components for the Prophetverse model. This is useful when we want to model specific patterns or relationships between the exogenous variables and the target variable. For example, enforcing a positive effect of a variable on the mean, or modeling a non-linear relationship.\nIf you have read the theory section, by effect we mean each function \\(f_i\\). You can implement those custom functions by subclassing the BaseEffect class, and then use them in the Prophetverse model. Some effects are already implemented in the library, and you can find them in the prophetverse.effects module.\nWhen creating a model instance, effects can be specified through exogenous_effects parameter of the Prophetverse model. This parameter is a list of tuples of three values: the name, the effect object, and a regex to filter columns related to that effect. The regex is what defines \\(x_i\\) in the previous section. The prophetverse.utils.regex module provides some useful functions to create regex patterns for common use cases, include starts_with, ends_with, contains, and no_input_columns.\nConsider the example below, where we create a model with a linear seasonality effect and a custom effect that uses the feature channel1_investment as input and transforms it with a hill curve, which is a common curve for capturing diminishing returns.\n\nfrom prophetverse.effects import HillEffect, LinearFourierSeasonality\nfrom prophetverse.sktime import Prophetverse\nfrom prophetverse.utils.regex import exact, no_input_columns, starts_with\n\nexogenous_effects = [\n    (\n        \"seasonality\",\n        LinearFourierSeasonality(\n            freq=\"D\",\n            sp_list=[7, 365.25],\n            fourier_terms_list=[3, 10],\n            prior_scale=0.1,\n            effect_mode=\"multiplicative\",\n        ),\n        no_input_columns,\n    ),\n    (\n        \"channel1_investment_incremental\",\n        HillEffect(effect_mode=\"additive\"),\n        exact(\"channel1_investment\"),\n    ),\n]\n\nmodel = Prophetverse(exogenous_effects=exogenous_effects)\n\n/Users/felipeangelim/Workspace/prophetverse/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\nCreating such models in Prophetverse is like creating buildings from lego blocks. You define how you model should work, and then you can leverage all the interface to carry out the forecasting and inference tasks.",
    "crumbs": [
      "Custom exogenous effect"
    ]
  },
  {
    "objectID": "howto/custom_effects.html#creating-a-custom-effect",
    "href": "howto/custom_effects.html#creating-a-custom-effect",
    "title": "Customizing Exogenous Effects in Prophetverse",
    "section": "Creating a Custom Effect",
    "text": "Creating a Custom Effect\nThe effects can be any object that implements the BaseEffect interface, and you can create your own effects by subclassing BaseEffect and implementing _fit, _transform and _predict methods.\n\n_fit (optional): This method is called during fit() of the forecasting and should be used to initialize any necessary parameters or data structures. It receives the exogenous variables dataframe X, the series y, and the scale factor scale that was used to scale the timeseries.\n_transform (optional): This method receives the exogenous variables dataframe, and should return an object containing the data needed for the effect. This object will be passed to the predict method as data. By default the columns of the dataframe that match the regex pattern are selected, and the result is converted to a jnp.ndarray.\n_predict (mandatory): This method receives the output of _transform and all previously computed effects. It should return the effect values as a jnp.ndarray\n\nIn many cases, the _fit and _transform steps are not needed to be implemented, since the default behaviour may be the desired one. In the example below, we implement a really simple SquaredEffect class, which leverages the default behaviour of the BaseEffect class.\n\nSquared Effect Class\nThe SquaredEffect class receives two hyperparameters: the prior distribution for the scale parameter, and the prior distribution for the offset parameter. If no prior is provided, it uses a Gamma(1, 1) for the scale and a Normal(0, 1) for the offset. Note that here we already see an interesting feature of Prophetverse: by adopting a Gamma Prior, we force the effect to be positive. Any other prior with positive support would work as well. If no such constraint is needed, we can use a Normal(0, 1) prior or any other distribution with support in the real line.\n\nfrom typing import Dict, Optional\n\nimport jax.numpy as jnp\nimport numpyro\nfrom numpyro import distributions as dist\nfrom numpyro.distributions import Distribution\n\nfrom prophetverse.effects.base import BaseEffect\n\n\nclass SquaredEffect(BaseEffect):\n    \"\"\"Represents a squared effect as effect = scale * (data - offset)^2.\"\"\"\n\n    def __init__(\n        self,\n        scale_prior: Optional[Distribution] = None,\n        offset_prior: Optional[Distribution] = None,\n    ):\n        self.scale_prior = scale_prior or dist.Gamma(1, 1)\n        self.offset_prior = offset_prior or dist.Normal(0, 1)\n        super().__init__()\n\n    def _predict(\n        self,\n        data: jnp.ndarray,\n        predicted_effects: Optional[Dict[str, jnp.ndarray]],\n        params: Dict,\n    ) -&gt; jnp.ndarray:\n        scale = numpyro.sample(\"log_scale\", self.scale_prior)\n        offset = numpyro.sample(\"offset\", self.offset_prior)\n        effect = scale * (data - offset) ** 2\n        return effect\n\nThe _fit and _transform methods are not implemented, and the default behaviour is preserved (the columns of the dataframe that match the regex pattern are selected, and the result is converted to a jnp.ndarray with key “data”).",
    "crumbs": [
      "Custom exogenous effect"
    ]
  },
  {
    "objectID": "howto/custom_effects.html#practical-example",
    "href": "howto/custom_effects.html#practical-example",
    "title": "Customizing Exogenous Effects in Prophetverse",
    "section": "Practical Example",
    "text": "Practical Example\nThe example below is, of course, a toy example, but I hope it illustrates the process of creating a custom effect. We load a synthetic dataset with a squared relationship between the exogenous variable and the target variable, and then we fit a model with the SquaredEffect. The true relationship is 2 * (x - 5) ** 2, and we will see if the model is able to recover it.\n\nLoading the Series\n\nimport matplotlib.pyplot as plt\nfrom sktime.split import temporal_train_test_split\nfrom sktime.utils.plotting import plot_series\n\nfrom prophetverse.datasets import load_synthetic_squared_exogenous\n\ny, X = load_synthetic_squared_exogenous()\ny_train, y_test, X_train, X_test = temporal_train_test_split(\n    y,\n    X,\n    test_size=0.2,\n)\n\ndisplay(y.head())\ndisplay(X.head())\n\nfig, ax = plot_series(\n    y_train, y_test, labels=[\"Train\", \"Test\"], title=\"Target variable\"\n)\nfig.show()\nfig, ax = plot_series(\n    X_train, X_test, labels=[\"Train\", \"Test\"], title=\"Exogenous variable\"\n)\nfig.show()\n\n\n\n\n\n\n\n\ntarget\n\n\ntime\n\n\n\n\n\n2010-01-01\n14.956419\n\n\n2010-01-02\n1.694310\n\n\n2010-01-03\n28.520329\n\n\n2010-01-04\n15.180486\n\n\n2010-01-05\n20.784949\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nexogenous\n\n\ntime\n\n\n\n\n\n2010-01-01\n7.965604\n\n\n2010-01-02\n4.949906\n\n\n2010-01-03\n8.727381\n\n\n2010-01-04\n7.276312\n\n\n2010-01-05\n1.847596\n\n\n\n\n\n\n\n/var/folders/_2/9y4tsvdd2n3gqjgd2zmlr1km0000gn/T/ipykernel_79276/2331088200.py:20: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n  fig.show()\n/var/folders/_2/9y4tsvdd2n3gqjgd2zmlr1km0000gn/T/ipykernel_79276/2331088200.py:24: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n  fig.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreating the Model\n\nfrom prophetverse.effects.trend import PiecewiseLinearTrend\nfrom prophetverse.engine import MAPInferenceEngine\nfrom prophetverse.sktime import Prophetverse\nfrom prophetverse.utils.regex import exact\n\nmodel = (\n    Prophetverse()\n    &gt;&gt; PiecewiseLinearTrend(\n        changepoint_interval=100,\n        changepoint_prior_scale=0.1,\n        changepoint_range=-100,\n    )\n    &gt;&gt; MAPInferenceEngine()\n) &gt;&gt; (\n    \"exog_effect\",\n    SquaredEffect(\n        scale_prior=dist.Normal(0, 10),\n        offset_prior=dist.Normal(0, 10),\n    ),\n    exact(\"exogenous\"),\n)\nmodel\n\nProphetverse(exogenous_effects=[('exog_effect',\n                                 SquaredEffect(offset_prior=&lt;numpyro.distributions.continuous.Normal object at 0x13514c990 with batch shape () and event shape ()&gt;,\n                                               scale_prior=&lt;numpyro.distributions.continuous.Normal object at 0x137575250 with batch shape () and event shape ()&gt;),\n                                 '^exogenous$')],\n             inference_engine=MAPInferenceEngine(),\n             trend=PiecewiseLinearTrend(changepoint_interval=100,\n                                        changepoint_prior_scale=0.1,\n                                        changepoint_range=-100))Please rerun this cell to show the HTML repr or trust the notebook.ProphetverseProphetverse(exogenous_effects=[('exog_effect',\n                                 SquaredEffect(offset_prior=&lt;numpyro.distributions.continuous.Normal object at 0x13514c990 with batch shape () and event shape ()&gt;,\n                                               scale_prior=&lt;numpyro.distributions.continuous.Normal object at 0x137575250 with batch shape () and event shape ()&gt;),\n                                 '^exogenous$')],\n             inference_engine=MAPInferenceEngine(),\n             trend=PiecewiseLinearTrend(changepoint_interval=100,\n                                        changepoint_prior_scale=0.1,\n                                        changepoint_range=-100))effectsPiecewiseLinearTrendPiecewiseLinearTrend(changepoint_interval=100, changepoint_prior_scale=0.1,\n                     changepoint_range=-100)SquaredEffectSquaredEffect(offset_prior=&lt;numpyro.distributions.continuous.Normal object at 0x13514c990 with batch shape () and event shape ()&gt;,\n              scale_prior=&lt;numpyro.distributions.continuous.Normal object at 0x137575250 with batch shape () and event shape ()&gt;)inference_engineMAPInferenceEngineMAPInferenceEngine()\n\n\nTo fit and plot, we use always the same interface, from sktime library.\n\nmodel.fit(y=y_train, X=X_train)\ny_pred = model.predict(fh=y_test.index, X=X)\ny_pred.head()\n\n\n\n\n\n\n\n\ntarget\n\n\n\n\n2011-07-15\n31.401798\n\n\n2011-07-16\n16.797144\n\n\n2011-07-17\n36.026203\n\n\n2011-07-18\n16.221083\n\n\n2011-07-19\n16.970106\n\n\n\n\n\n\n\n\nplot_series(y, y_pred, labels=[\"True\", \"Predicted\"], title=\"True vs Predicted\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nRecovering the Predicted Effect and Components\nThis library adds extra methods to the sktime interface, such as predict_components, which behaves similarly to predict, but returns the components of the forecast as components of the output.\nThe name of the effect in the output dataframe is equal to the one we have passed as first item in the tuple when creating the model. In this case, the name is “exog_effect”.\n\ncomponents = model.predict_components(fh=y.index, X=X)\ncomponents.head()\n\n\n\n\n\n\n\n\nexog_effect\nmean\nobs\ntrend\n\n\n\n\n2010-01-01\n17.592978\n17.405348\n17.368708\n-0.187630\n\n\n2010-01-02\n0.006445\n-0.164905\n-0.166329\n-0.171350\n\n\n2010-01-03\n27.817160\n27.662096\n27.744015\n-0.155071\n\n\n2010-01-04\n10.351295\n10.212508\n10.226997\n-0.138792\n\n\n2010-01-05\n20.050062\n19.927542\n20.027119\n-0.122512\n\n\n\n\n\n\n\nNow, let’s compare it with the true effect. We will plot the true effect and the predicted effect in the same plot.\n\nfig, ax = plt.subplots()\nax.scatter(\n    X[\"exogenous\"], 2 * (X[\"exogenous\"] - 5) ** 2, color=\"black\", label=\"True effect\"\n)\nax.scatter(\n    X[\"exogenous\"],\n    components[\"exog_effect\"],\n    marker=\"x\",\n    color=\"red\",\n    s=10,\n    label=\"Predicted effect\",\n)\nax.set(\n    xlabel=\"Exogenous variable\",\n    ylabel=\"Effect\",\n    title=\"True effect vs Predicted effect\",\n)\nax.legend()\nfig.show()\n\n/var/folders/_2/9y4tsvdd2n3gqjgd2zmlr1km0000gn/T/ipykernel_79276/3918923059.py:19: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n  fig.show()",
    "crumbs": [
      "Custom exogenous effect"
    ]
  },
  {
    "objectID": "the-theory.html",
    "href": "the-theory.html",
    "title": "Mathematical formulation",
    "section": "",
    "text": "Figure 1: Generalized Additive Models are versatile. Prophet is one of the many models that can be built on top of it. The idea of Prophetverse is giving access to that universe.\nProphetverse leverages the Generalized Additive Model (GAM) idea in the original Prophet model and extends it to be more flexible and customizable. The core principle of GAMs is to model the expected value \\(y_{mean}\\) of the endogenous variable \\(Y\\) as the sum of many functions \\(\\{f_i\\}_{i=1}^n\\) of exogenous variables \\(\\{x_i\\}_{i=1}^n\\).\n\\[\ny_{mean} = f_1(x_1) + f_2(x_2) + \\ldots + f_n(t)\\text{, }\\quad n \\in \\mathbb{N}\n\\]\nThe innovation in Prophet is the use of Bayesian GAMs to model time series data. Instead of approximating the time series through auto-regressive models, Prophet treats it as a curve-fitting exercise. This approach results in fast, interpretable, and accurate forecasts. The Prophet formulation is:\n\\[\ny_{\\text{mean}} = \\begin{cases} y_{\\text{mean}}(t) = \\tau(t) + s(t) + h(t) + v(t) & \\text{if additive} \\\\ y_{\\text{mean}}(t) = \\tau(t) + \\tau(t) \\cdot s(t) + \\tau(t) \\cdot h(t) + \\tau(t) \\cdot v(t) & \\text{if multiplicative} \\end{cases}\n\\]\nwhere \\(\\tau(t)\\) is the trend component, \\(s(t)\\) is the seasonality component, \\(h(t)\\) is the holiday component, and \\(v(t)\\) is other regressors components. Those components are hard-coded as linear in the original formulation of Facebook Prophet, but in Prophetverse they are versatile and can be defined by the user. This is the first main difference between Prophet and Prophetverse. The \\(f_i\\) functions are defined by the Effects API , where the user can create their own components and priors, by using the already available ones or by creating new BaseEffect subclasses.\n\\[\\begin{align}\ny_{mean} &= \\sum\\limits_{i=1}^n f_i(x_i(t), \\{f_j(x_j)\\}_{j&lt;i}) \\\\\n         &= f_1(x_1(t)) + f_2(x_2(t), f_1(x_1(t))) + \\ldots + f_n(t, \\{f_j(x_j)\\}_{j&lt;n})\n\\end{align}\\]\nwhere\nThis definition superseeds the Prophet formulation because effects are ordered, so that the output of previous effects can be used as input for the next ones. This allows for complex interactions between exogenous variables."
  },
  {
    "objectID": "the-theory.html#likelihood",
    "href": "the-theory.html#likelihood",
    "title": "Mathematical formulation",
    "section": "Likelihood",
    "text": "Likelihood\nIn the original Prophet, the likelihood is a Normal distribution, but in Prophetverse it can be Normal, Gamma, or Negative Binomial.\n\\[\ny \\sim \\mathcal{likelihood}(\\phi(\\hat{y}_{mean}), \\sigma^2)\\quad \\text{where} \\quad\n\\sigma \\sim HalfNormal(\\sigma_{hyper})\n\\]\nwhere \\(\\sigma_{hyper}\\) is a hyperparameter and \\(\\phi\\) is a function that maps the mean to the support of the likelihood. For normal likelihood, \\(\\phi\\) is the identity function, but for Gamma and Negative Binomial, it is\n\\[\n\\phi(k) = \\begin{cases}\nk & \\text{if } k &gt; z \\\\\nz\\exp(k-z) & \\text{if } k \\leq z\n\\end{cases}\n\\]\nfor some small threshold \\(z\\). We set \\(z = 10^{-5}\\) in our implementation. The reason for this is to avoid zero or negative values in the support of the likelihood, which can lead to error."
  },
  {
    "objectID": "the-theory.html#trend",
    "href": "the-theory.html#trend",
    "title": "Mathematical formulation",
    "section": "Trend",
    "text": "Trend\nThere are mainly two types of trends supported: linear and logistic. We will first take a look at the original mathematical formulation of Prophet’s paper, and then simplify it to obtain a simpler and more interpretable version.\n\nLinear trend\n\nOriginal formulation\nThe linear trend is modeled as a piecewise linear functions with changepoints. Let \\(M\\) be the number of changepoints, \\(\\delta \\in \\mathbb{R}^M\\) be the rate adjustment at each changepoint, \\(\\{\\kappa_i\\}_{i=1}^M\\) be the changepoint times, and be \\(a(t) \\in \\{0,1\\}^M\\) be a vector which assumes, at each index, 1 if the corresponding changepoint is greater than \\(t\\) and 0 otherwise. In addition, let \\(k\\) represent the global rate and \\(m\\) the global offset. Then, the linear trend is defined as:\n\\[\n\\tau(t) = (k + a(t)^T\\delta)t + (m + a(t)^T\\gamma), \\quad \\text{where} \\quad \\gamma_i = \\kappa_i\\delta_i\n\\]\nThe first part accounts for the rate adjustment at each changepoint, and the second part corrects the offset at each changepoint, so that the trend is continuous.\n\n\nProphetverse’s equivalent formulation\nThis can be simplified as a first-order spline regression with \\(M\\) knots (changepoints). Let \\(b(t) \\in \\mathbb{R}^M\\) be a vector so that \\(b(t)_i = (t - \\kappa_i)_+\\) (the positive part of \\(t - \\kappa_i\\)). Then, the piecewise linear trend value for time \\(t\\) can be written as:\n\\[\n\\tau(t) = b(t)^T \\delta + kt + m\n\\]\nWe can also write the trend for all \\(t \\in \\{t_1,\\dots, t_T\\}\\) as a matrix multiplication. Let \\(\\mathbf{B} \\in \\mathbb{R}^{T \\times M+2}\\) be the matrix whose rows are \\(b'(t) = \\left[ b(t), t, 1 \\right]\\) for each time \\(t\\). In other words, it is the spline basis matrix. The \\(t\\) and \\(1\\) at the end of the vector are included to account for the global rate and offset. Furthermore, consider the vector \\(\\delta' = \\left[ \\delta, k, m \\right]\\). Then, the trend vector \\(G \\in \\mathbb{R}^T\\), \\(G_i = \\tau(\\mathbf{t}_i)\\), can be written as:\n\\[\\begin{align}\nG &= \\mathbf{B}\\delta' \\\\\n\\end{align}\\]\n\\[\\begin{align}\nG &=  \\begin{bmatrix}\n(t_0 - \\kappa_0)_+ & (t_0 - \\kappa_1)_+ & \\ldots & (t_0 - \\kappa_{M-1})_+ & t_0 & 1 \\\\\n(t_1 - \\kappa_0)_+ & (t_1 - \\kappa_1)_+ & \\ldots & (t_1 - \\kappa_{M-1})_+ & t_1 & 1 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\vdots \\\\\n(t_{T-1} - \\kappa_0)_+ & (t_{T-1} - \\kappa_1)_+ & \\ldots & (t_{T-1} - \\kappa_{M-1})_+ & t_{T-1} & 1 \\\\\n\\end{bmatrix} \\begin{bmatrix}\n\\delta_0 \\\\\n\\delta_1 \\\\\n\\vdots \\\\\n\\delta_{M-1} \\\\\nk \\\\\nm \\\\\n\\end{bmatrix}\n\\end{align}\\]\n\n\n\n\n\n\nExample\n\n\n\nOne possible realization of \\(\\mathbf{B}\\) is:\n\\[\n\\begin{bmatrix}\n0 & 0 & 0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 0 & 1 & 1 \\\\\n1 & 0 & 0 & 0 & 2 & 1 \\\\\n2 & 1 & 0 & 0 & 3 & 1 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\nT-1 & T-2 & T-3 & T-4 & T-1 & 1 \\\\\n\\end{bmatrix}\n\\]\n\n\n\n\n\nLogistic trend\nThe logistic trend of the original model uses the piecewise logistic linear trend to change the rate at which \\(t\\) grows. We will not explain the mathematical formulation of the original paper here, and will already leverage what we have learned about the linear trend to simplify it.\n\\[\nG = \\frac{C}{1 + \\exp(-\\mathbf{B}\\delta')}\n\\]\nwhere \\(C\\) is the logistic capacity, which should be passed as input to Prophet, but is a random variable in Prophetverse.\n\n\nChangepoint priors\nA Laplace prior is put on the rate adjustment \\(\\delta_i \\sim Laplace(0, \\sigma_{\\delta})\\) where \\(\\sigma_{\\delta}\\) is a hyperparameter. The changepoint times \\(\\kappa_i\\) can be predefined by the user, or can be uniformly distributed in the training data. The offset and rate prior location are set in a “smart” way, by checking analytically what would be the values that fit the maximum and minimum points of the time series.\n\n\n\n\n\n\nNote\n\n\n\nAlthough those trend are the ones that come with the library, the user can define any trend, including a trend that depends on some exogenous variable. Flexibility is the key here."
  },
  {
    "objectID": "the-theory.html#seasonality",
    "href": "the-theory.html#seasonality",
    "title": "Mathematical formulation",
    "section": "Seasonality",
    "text": "Seasonality\nTo model seasonality, Prophetverse uses a Fourier series to approximate periodic functions, allowing the model to fit complex seasonal patterns flexibly. This approach involves determining the number of Fourier terms (K), which corresponds to the complexity of the seasonality. The formula for a seasonal component s(t) in terms of a Fourier series is given as:\n\\[\ns(t) = \\sum_{k=1}^K \\left( a_k \\cos\\left(\\frac{2\\pi kt}{P}\\right) + b_k \\sin\\left(\\frac{2\\pi kt}{P}\\right) \\right)\n\\]\nHere, P is the period (e.g., 365.25 for yearly seasonality), and \\(a_k\\) and \\(b_k\\) are the Fourier coefficients that the model estimates. The choice of K depends on the granularity of the seasonal changes one wishes to capture. A Normal prior is placed on the coefficients, \\(a_k, b_k \\sim \\mathcal{N}(0, \\sigma_s)\\), where \\(\\sigma_s\\) is a hyperparameter.\nSee LinearFourierSeasonality for more details on the hyperparameters of the effect\n\nMatrix Formulation of Fourier Series\nTo efficiently compute the seasonality for multiple time points, we can represent the Fourier series in a matrix form. This method is especially useful for handling large datasets and simplifies the implementation of the model in computational software. Let \\(T\\) be the number of time points, and create a design matrix \\(X\\) of size \\(T \\times 2K\\). Each row of \\(X\\) corresponds to a time point and contains all Fourier basis functions evaluated at that time:\n\\[\n\\mathbf{X} = \\begin{bmatrix}\n\\cos\\left(\\frac{2\\pi \\cdot 1 \\cdot t_1}{P}\\right) & \\sin\\left(\\frac{2\\pi \\cdot 1 \\cdot t_1}{P}\\right) & \\cdots & \\cos\\left(\\frac{2\\pi \\cdot K \\cdot t_1}{P}\\right) & \\sin\\left(\\frac{2\\pi \\cdot K \\cdot t_1}{P}\\right) \\\\\n\\cos\\left(\\frac{2\\pi \\cdot 1 \\cdot t_2}{P}\\right) & \\sin\\left(\\frac{2\\pi \\cdot 1 \\cdot t_2}{P}\\right) & \\cdots & \\cos\\left(\\frac{2\\pi \\cdot K \\cdot t_2}{P}\\right) & \\sin\\left(\\frac{2\\pi \\cdot K \\cdot t_2}{P}\\right) \\\\\n\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n\\cos\\left(\\frac{2\\pi \\cdot 1 \\cdot t_T}{P}\\right) & \\sin\\left(\\frac{2\\pi \\cdot 1 \\cdot t_T}{P}\\right) & \\cdots & \\cos\\left(\\frac{2\\pi \\cdot K \\cdot t_T}{P}\\right) & \\sin\\left(\\frac{2\\pi \\cdot K \\cdot t_T}{P}\\right)\n\\end{bmatrix}\n\\]\nCoefficient Vector:\nDefine a vector \\(\\beta\\) of length 2K containing the coefficients \\(a_1, b_1, \\dots, a_K, b_K\\):\n\\[\n\\mathbf{\\beta} = \\begin{bmatrix}\na_1 \\\\\nb_1 \\\\\n\\vdots \\\\\na_K \\\\\nb_K\n\\end{bmatrix}\n\\]\nThe seasonality for all time points can then be computed through the matrix product of \\(X\\) and \\(\\beta\\):\n\\[\n\\mathbf{s} = \\mathbf{X} \\mathbf{\\beta}\n\\]\nEach element of vector \\(s\\), denoted as \\(s_i\\), represents the seasonality at time \\(t_i\\).\nThis matrix approach not only makes the computation faster and more scalable but also simplifies integration with other components of the forecasting model. One drawback is that it assumes a constant seasonality, but an user can also define a seasonality that changes with time in Prophetverse, by creating a custom Effect class."
  },
  {
    "objectID": "the-theory.html#multivariate-model",
    "href": "the-theory.html#multivariate-model",
    "title": "Mathematical formulation",
    "section": "Multivariate model",
    "text": "Multivariate model\nProphetverse also supports multivariate forecasting. In this case, the model is essentially the same, but for now only Normal Likelihood is supported. Depending on the usage of the library, we may add other likelihoods in the future (please open an issue if you need it!). In that case, all other components are estimated in the same way, but the likelihood is a multivariate distribution. The mean of the distribution is a vector, and the covariance matrix prior is a LKJ distribution."
  },
  {
    "objectID": "tutorial/univariate.html",
    "href": "tutorial/univariate.html",
    "title": "Basic Univariate Forecasting with Prophetverse",
    "section": "",
    "text": "This example shows how to use Prophetverse to perform univariate forecasting with a time series dataset, using sktime-style interface.\nBecause of this compatibility, you can benefit from all the features of sktime, such as hierarchical reconciliation, ensemble models, pipelines, etc. There are two main methods to use Prophetverse with sktime:\n\nfit(y, X=None): This method is used to fit the model. It takes as input a time series y and an optional exogenous variable X. The y time series must be a pd.Series or a pd.DataFrame. The X variable must be a pd.DataFrame or None.\npredict(fh, X=None): This method is used to make predictions. It takes as input a forecast horizon fh and an optional exogenous variable X. The fh forecast horizon can be a relative or an absolute forecast horizon. The X variable must be a pd.DataFrame or None, according to the X variable used in the fit method.\n\nLater in this example, we will also show additional methods to make predictions, such as predict_quantiles and predict_components.\n\nimport warnings\nwarnings.simplefilter(action=\"ignore\")\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom numpyro import distributions as dist\n\n\n\nWe import a dataset from Prophet’s original repository. We then put it into sktime-friendly format, where the index is a pd.PeriodIndex and the colums are the time series.\n\nfrom prophetverse.datasets.loaders import load_peyton_manning\n\ny = load_peyton_manning()\ndisplay(y.head())\n\n\n\n\n\n\n\n\ny\n\n\nds\n\n\n\n\n\n2007-12-10\n9.590761\n\n\n2007-12-11\n8.519590\n\n\n2007-12-12\n8.183677\n\n\n2007-12-13\n8.072467\n\n\n2007-12-14\n7.893572\n\n\n\n\n\n\n\nThe full dataset looks like this:\n\ny.plot.line(figsize=(12, 6))\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nHere, we will show how you can fit a simple model with Prophetverse. We first fit a model without seasonal components, and then fit a full model. We also show how easy it is to switch between Maximum A Posteriori (MAP) inference and Markov Chain Monte Carlo (MCMC).\n\n\n\nfrom prophetverse.effects.trend import PiecewiseLinearTrend\nfrom prophetverse.engine import MAPInferenceEngine\nfrom prophetverse.sktime import Prophetverse\nfrom prophetverse.utils import no_input_columns\n\nmodel = Prophetverse(\n    trend=PiecewiseLinearTrend(\n        changepoint_interval=500,\n        changepoint_prior_scale=0.00001,\n        changepoint_range=-250,\n    ),\n    inference_engine=MAPInferenceEngine(),\n)\nmodel.fit(y=y)\n\nProphetverse(inference_engine=MAPInferenceEngine(),\n             trend=PiecewiseLinearTrend(changepoint_interval=500,\n                                        changepoint_prior_scale=1e-05,\n                                        changepoint_range=-250))Please rerun this cell to show the HTML repr or trust the notebook.ProphetverseProphetverse(inference_engine=MAPInferenceEngine(),\n             trend=PiecewiseLinearTrend(changepoint_interval=500,\n                                        changepoint_prior_scale=1e-05,\n                                        changepoint_range=-250))effectsPiecewiseLinearTrendPiecewiseLinearTrend(changepoint_interval=500, changepoint_prior_scale=1e-05,\n                     changepoint_range=-250)inference_engineMAPInferenceEngineMAPInferenceEngine()\n\n\n\nforecast_horizon = pd.period_range(\"2007-01-01\", \"2018-01-01\", freq=\"D\")\nfig, ax = plt.subplots(figsize=(10, 5))\npreds = model.predict(fh=forecast_horizon)\npreds.plot.line(ax=ax)\nax.scatter(y.index, y, marker=\"o\", color=\"k\", s=2, alpha=0.5)\n\n\n\n\n\n\n\n\n\n\n\nHere, we fit the univariate Prophet and pass an exogenous effect as hyperparameter. The exogenous_effects parameter let us add new components to the model and control the relationship between exogenous variables and the target variable.\nIn this case, the LinearFourierSeasonality effect creates sinusoidal and cosine terms to model the seasonality of the time series, which are then multiplied by linear coefficients and added to the model.\nThis argument is a list of tuples of the form (effect_name, effect, regex_to_filter_relevant_columns), where effect_name is a string and effect is an instance of a subclass of prophetverse.effects.BaseEffect. The regex is used to filter the columns of X that are relevant for the effect, but can also be None (or its alias prophetverse.utils.no_input_columns) if no input in X is needed for the effect.\n\nfrom prophetverse.effects.fourier import LinearFourierSeasonality\nfrom prophetverse.utils import no_input_columns\n\nmodel = Prophetverse(\n    trend=PiecewiseLinearTrend(\n        changepoint_interval=500,\n        changepoint_prior_scale=0.00001,\n        changepoint_range=-500,\n    ),\n    exogenous_effects=[\n        (\n            \"seasonality\",\n            LinearFourierSeasonality(\n                freq=\"D\",\n                sp_list=[7, 365.25],\n                fourier_terms_list=[3, 10],\n                prior_scale=0.1,\n                effect_mode=\"multiplicative\",\n            ),\n            no_input_columns,\n        ),\n    ],\n    inference_engine=MAPInferenceEngine(),\n)\nmodel.fit(y=y)\n\nProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3,\n                                                                              10],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[7, 365.25]),\n                                 '^$')],\n             inference_engine=MAPInferenceEngine(),\n             trend=PiecewiseLinearTrend(changepoint_interval=500,\n                                        changepoint_prior_scale=1e-05,\n                                        changepoint_range=-500))Please rerun this cell to show the HTML repr or trust the notebook.ProphetverseProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3,\n                                                                              10],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[7, 365.25]),\n                                 '^$')],\n             inference_engine=MAPInferenceEngine(),\n             trend=PiecewiseLinearTrend(changepoint_interval=500,\n                                        changepoint_prior_scale=1e-05,\n                                        changepoint_range=-500))effectsPiecewiseLinearTrendPiecewiseLinearTrend(changepoint_interval=500, changepoint_prior_scale=1e-05,\n                     changepoint_range=-500)LinearFourierSeasonalityLinearFourierSeasonality(effect_mode='multiplicative',\n                         fourier_terms_list=[3, 10], freq='D', prior_scale=0.1,\n                         sp_list=[7, 365.25])inference_engineMAPInferenceEngineMAPInferenceEngine()\n\n\n\nforecast_horizon = pd.period_range(\"2007-01-01\", \"2018-01-01\", freq=\"D\")\nfig, ax = plt.subplots(figsize=(10, 5))\npreds = model.predict(fh=forecast_horizon)\npreds.plot.line(ax=ax)\nax.scatter(y.index, y, marker=\"o\", color=\"k\", s=2, alpha=0.5)\n\n\n\n\n\n\n\n\n\n\n\n\nWe can also make probabilistic forecasts with Prophetverse, in sktime fashion. The predict_quantiles method returns the quantiles of the predictive distribution in a pd.DataFrame\n\nquantiles = model.predict_quantiles(fh=forecast_horizon, alpha=[0.1, 0.9])\nquantiles.head()\n\n\n\n\n\n\n\n\ny\n\n\n\n0.1\n0.9\n\n\n\n\n2007-01-01\n8.066615\n9.367413\n\n\n2007-01-02\n7.873573\n9.173687\n\n\n2007-01-03\n7.761128\n9.031336\n\n\n2007-01-04\n7.715733\n9.067670\n\n\n2007-01-05\n7.747439\n9.096114\n\n\n\n\n\n\n\nThe plot below shows the (0.1, 0.9) quantiles of the predictive distribution\n\nfig, ax = plt.subplots(figsize=(10, 5))\n# Plot area between quantiles\nax.fill_between(\n    quantiles.index.to_timestamp(),\n    quantiles.iloc[:, 0],\n    quantiles.iloc[:, -1],\n    alpha=0.5,\n)\nax.scatter(y.index, y, marker=\"o\", color=\"k\", s=2, alpha=1)\n\n\n\n\n\n\n\n\n\n\n\nWe can easily extract the components of the time series with the predict_components method. This method, in particular, is not implemented in sktime’s BaseForecaster, but it is a method of prophetverse.Prophetverse class.\n\nsites = model.predict_components(fh=forecast_horizon)\nsites.head()\n\n\n\n\n\n\n\n\nmean\nobs\nseasonality\ntrend\n\n\n\n\n2007-01-01\n8.716851\n8.730150\n0.916735\n7.800113\n\n\n2007-01-02\n8.521084\n8.512227\n0.720966\n7.800113\n\n\n2007-01-03\n8.367682\n8.385690\n0.567568\n7.800113\n\n\n2007-01-04\n8.386809\n8.394879\n0.586693\n7.800113\n\n\n2007-01-05\n8.418292\n8.409329\n0.618176\n7.800113\n\n\n\n\n\n\n\n\nfor column in sites.columns:\n    fig, ax = plt.subplots(figsize=(8, 2))\n    ax.plot(sites.index.to_timestamp(), sites[column], label=column)\n    ax.set_title(column)\n    fig.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the previous examples, we used MAP inference to fit the model. However, we can also use Markov Chain Monte Carlo (MCMC) to fit the model. To do this, we just need to change the inference_engine parameter to MCMCInferenceEngine. The rest of the code remains the same.\nThe set_params method is used to set the parameters of the model, in sklearn fashion.\n\nfrom prophetverse.engine import MCMCInferenceEngine\n\nmodel.set_params(inference_engine=MCMCInferenceEngine())\n\nimport numpyro\n\nnumpyro.enable_x64()  # To avoid computational issues with MCMC\n\nmodel.fit(y=y)\n\n  0%|          | 0/1200 [00:00&lt;?, ?it/s]warmup:   0%|          | 1/1200 [00:01&lt;30:13,  1.51s/it, 1 steps of size 2.34e+00. acc. prob=0.00]warmup:   4%|▎         | 42/1200 [00:01&lt;00:32, 35.77it/s, 511 steps of size 2.03e-07. acc. prob=0.63]warmup:   5%|▌         | 65/1200 [00:01&lt;00:22, 50.36it/s, 255 steps of size 1.37e-07. acc. prob=0.67]warmup:   7%|▋         | 82/1200 [00:02&lt;00:18, 59.91it/s, 127 steps of size 1.95e-07. acc. prob=0.69]warmup:   8%|▊         | 97/1200 [00:02&lt;00:16, 66.39it/s, 255 steps of size 1.00e-07. acc. prob=0.70]warmup:   9%|▉         | 110/1200 [00:02&lt;00:17, 63.81it/s, 255 steps of size 2.11e-06. acc. prob=0.71]warmup:  10%|█         | 121/1200 [00:02&lt;00:17, 60.71it/s, 1023 steps of size 2.63e-06. acc. prob=0.72]warmup:  11%|█         | 130/1200 [00:02&lt;00:17, 61.40it/s, 511 steps of size 4.79e-06. acc. prob=0.73] warmup:  12%|█▏        | 138/1200 [00:02&lt;00:17, 61.93it/s, 255 steps of size 2.37e-05. acc. prob=0.74]warmup:  12%|█▏        | 146/1200 [00:02&lt;00:16, 63.99it/s, 255 steps of size 1.22e-05. acc. prob=0.74]warmup:  13%|█▎        | 154/1200 [00:03&lt;00:15, 66.23it/s, 1023 steps of size 1.17e-05. acc. prob=0.74]warmup:  14%|█▍        | 166/1200 [00:03&lt;00:14, 72.53it/s, 1023 steps of size 4.71e-06. acc. prob=0.74]warmup:  14%|█▍        | 174/1200 [00:03&lt;00:14, 71.44it/s, 127 steps of size 2.49e-05. acc. prob=0.74] warmup:  15%|█▌        | 182/1200 [00:03&lt;00:15, 64.32it/s, 255 steps of size 2.54e-05. acc. prob=0.75]warmup:  16%|█▌        | 190/1200 [00:03&lt;00:14, 67.36it/s, 255 steps of size 1.76e-05. acc. prob=0.75]warmup:  16%|█▋        | 198/1200 [00:03&lt;00:14, 68.30it/s, 127 steps of size 2.14e-05. acc. prob=0.75]sample:  17%|█▋        | 208/1200 [00:03&lt;00:13, 76.09it/s, 255 steps of size 9.97e-06. acc. prob=0.96]sample:  18%|█▊        | 216/1200 [00:03&lt;00:13, 72.02it/s, 255 steps of size 9.97e-06. acc. prob=0.95]sample:  19%|█▉        | 225/1200 [00:04&lt;00:12, 76.12it/s, 255 steps of size 9.97e-06. acc. prob=0.95]sample:  19%|█▉        | 233/1200 [00:04&lt;00:12, 76.40it/s, 511 steps of size 9.97e-06. acc. prob=0.95]sample:  20%|██        | 241/1200 [00:04&lt;00:12, 74.99it/s, 1023 steps of size 9.97e-06. acc. prob=0.95]sample:  21%|██        | 250/1200 [00:04&lt;00:12, 75.92it/s, 511 steps of size 9.97e-06. acc. prob=0.95] sample:  22%|██▏       | 258/1200 [00:04&lt;00:14, 65.97it/s, 255 steps of size 9.97e-06. acc. prob=0.95]sample:  22%|██▏       | 268/1200 [00:04&lt;00:12, 73.55it/s, 255 steps of size 9.97e-06. acc. prob=0.95]sample:  23%|██▎       | 278/1200 [00:04&lt;00:11, 77.25it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  24%|██▍       | 286/1200 [00:04&lt;00:11, 76.87it/s, 255 steps of size 9.97e-06. acc. prob=0.96]sample:  25%|██▍       | 296/1200 [00:04&lt;00:10, 82.54it/s, 255 steps of size 9.97e-06. acc. prob=0.96]sample:  25%|██▌       | 305/1200 [00:05&lt;00:11, 81.01it/s, 255 steps of size 9.97e-06. acc. prob=0.96]sample:  26%|██▌       | 314/1200 [00:05&lt;00:11, 74.95it/s, 255 steps of size 9.97e-06. acc. prob=0.95]sample:  27%|██▋       | 322/1200 [00:05&lt;00:11, 75.81it/s, 255 steps of size 9.97e-06. acc. prob=0.95]sample:  28%|██▊       | 330/1200 [00:05&lt;00:12, 72.21it/s, 255 steps of size 9.97e-06. acc. prob=0.95]sample:  28%|██▊       | 338/1200 [00:05&lt;00:11, 73.37it/s, 511 steps of size 9.97e-06. acc. prob=0.95]sample:  29%|██▉       | 346/1200 [00:05&lt;00:12, 70.55it/s, 255 steps of size 9.97e-06. acc. prob=0.95]sample:  30%|██▉       | 354/1200 [00:05&lt;00:11, 70.80it/s, 255 steps of size 9.97e-06. acc. prob=0.96]sample:  30%|███       | 362/1200 [00:05&lt;00:12, 69.18it/s, 255 steps of size 9.97e-06. acc. prob=0.96]sample:  31%|███       | 370/1200 [00:06&lt;00:11, 71.51it/s, 255 steps of size 9.97e-06. acc. prob=0.96]sample:  32%|███▏      | 378/1200 [00:06&lt;00:11, 71.39it/s, 255 steps of size 9.97e-06. acc. prob=0.96]sample:  32%|███▏      | 386/1200 [00:06&lt;00:11, 69.84it/s, 255 steps of size 9.97e-06. acc. prob=0.96]sample:  33%|███▎      | 394/1200 [00:06&lt;00:11, 68.64it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  34%|███▎      | 403/1200 [00:06&lt;00:11, 72.05it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  34%|███▍      | 412/1200 [00:06&lt;00:10, 76.42it/s, 255 steps of size 9.97e-06. acc. prob=0.96]sample:  35%|███▌      | 420/1200 [00:06&lt;00:11, 65.38it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  36%|███▌      | 428/1200 [00:06&lt;00:11, 68.48it/s, 255 steps of size 9.97e-06. acc. prob=0.96] sample:  36%|███▋      | 436/1200 [00:06&lt;00:11, 67.90it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  37%|███▋      | 443/1200 [00:07&lt;00:11, 67.93it/s, 255 steps of size 9.97e-06. acc. prob=0.96]sample:  38%|███▊      | 450/1200 [00:07&lt;00:11, 67.59it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  38%|███▊      | 457/1200 [00:07&lt;00:11, 65.67it/s, 255 steps of size 9.97e-06. acc. prob=0.96] sample:  39%|███▊      | 464/1200 [00:07&lt;00:11, 64.49it/s, 255 steps of size 9.97e-06. acc. prob=0.96]sample:  39%|███▉      | 472/1200 [00:07&lt;00:10, 67.85it/s, 255 steps of size 9.97e-06. acc. prob=0.96]sample:  40%|███▉      | 479/1200 [00:07&lt;00:11, 64.10it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  40%|████      | 486/1200 [00:07&lt;00:11, 61.78it/s, 255 steps of size 9.97e-06. acc. prob=0.96]sample:  41%|████      | 493/1200 [00:07&lt;00:12, 57.70it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  42%|████▏     | 499/1200 [00:08&lt;00:13, 51.33it/s, 255 steps of size 9.97e-06. acc. prob=0.96]sample:  42%|████▏     | 505/1200 [00:08&lt;00:13, 50.97it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  43%|████▎     | 512/1200 [00:08&lt;00:12, 55.06it/s, 255 steps of size 9.97e-06. acc. prob=0.96]sample:  43%|████▎     | 519/1200 [00:08&lt;00:11, 58.26it/s, 255 steps of size 9.97e-06. acc. prob=0.96]sample:  44%|████▍     | 526/1200 [00:08&lt;00:11, 60.60it/s, 255 steps of size 9.97e-06. acc. prob=0.96]sample:  44%|████▍     | 533/1200 [00:08&lt;00:11, 59.73it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  45%|████▌     | 540/1200 [00:08&lt;00:10, 60.29it/s, 255 steps of size 9.97e-06. acc. prob=0.96]sample:  46%|████▌     | 547/1200 [00:08&lt;00:11, 57.61it/s, 255 steps of size 9.97e-06. acc. prob=0.96]sample:  46%|████▌     | 553/1200 [00:08&lt;00:11, 54.58it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  47%|████▋     | 561/1200 [00:09&lt;00:10, 60.35it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  47%|████▋     | 568/1200 [00:09&lt;00:11, 56.79it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  48%|████▊     | 574/1200 [00:09&lt;00:10, 57.14it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  48%|████▊     | 580/1200 [00:09&lt;00:10, 57.28it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  49%|████▉     | 587/1200 [00:09&lt;00:10, 58.44it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  49%|████▉     | 593/1200 [00:09&lt;00:11, 52.93it/s, 255 steps of size 9.97e-06. acc. prob=0.96]sample:  50%|████▉     | 599/1200 [00:09&lt;00:11, 52.84it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  50%|█████     | 606/1200 [00:09&lt;00:10, 56.75it/s, 255 steps of size 9.97e-06. acc. prob=0.96]sample:  51%|█████     | 612/1200 [00:10&lt;00:10, 55.38it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  52%|█████▏    | 618/1200 [00:10&lt;00:10, 55.74it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  52%|█████▏    | 624/1200 [00:10&lt;00:10, 56.33it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  52%|█████▎    | 630/1200 [00:10&lt;00:12, 46.22it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  53%|█████▎    | 636/1200 [00:10&lt;00:11, 48.06it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  54%|█████▎    | 642/1200 [00:10&lt;00:11, 49.33it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  54%|█████▍    | 649/1200 [00:10&lt;00:10, 52.87it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  55%|█████▍    | 655/1200 [00:10&lt;00:11, 49.30it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  55%|█████▌    | 661/1200 [00:11&lt;00:11, 48.24it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  56%|█████▌    | 667/1200 [00:11&lt;00:10, 49.63it/s, 511 steps of size 9.97e-06. acc. prob=0.96] sample:  56%|█████▌    | 673/1200 [00:11&lt;00:10, 50.83it/s, 255 steps of size 9.97e-06. acc. prob=0.96]sample:  57%|█████▋    | 679/1200 [00:11&lt;00:10, 51.80it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  57%|█████▋    | 685/1200 [00:11&lt;00:10, 49.97it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  58%|█████▊    | 691/1200 [00:11&lt;00:10, 48.82it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  58%|█████▊    | 696/1200 [00:11&lt;00:11, 45.66it/s, 511 steps of size 9.97e-06. acc. prob=0.96] sample:  58%|█████▊    | 702/1200 [00:11&lt;00:10, 47.98it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  59%|█████▉    | 707/1200 [00:12&lt;00:11, 43.09it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  59%|█████▉    | 712/1200 [00:12&lt;00:11, 42.71it/s, 511 steps of size 9.97e-06. acc. prob=0.96] sample:  60%|█████▉    | 718/1200 [00:12&lt;00:10, 46.76it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  60%|██████    | 723/1200 [00:12&lt;00:10, 44.28it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  61%|██████    | 729/1200 [00:12&lt;00:09, 48.12it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  61%|██████    | 734/1200 [00:12&lt;00:09, 47.27it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  62%|██████▏   | 739/1200 [00:12&lt;00:09, 46.66it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  62%|██████▏   | 745/1200 [00:12&lt;00:09, 47.60it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  63%|██████▎   | 751/1200 [00:12&lt;00:08, 50.37it/s, 255 steps of size 9.97e-06. acc. prob=0.96] sample:  63%|██████▎   | 757/1200 [00:13&lt;00:09, 46.87it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  64%|██████▎   | 763/1200 [00:13&lt;00:08, 48.68it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  64%|██████▍   | 768/1200 [00:13&lt;00:09, 45.14it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  64%|██████▍   | 774/1200 [00:13&lt;00:08, 48.76it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  65%|██████▍   | 779/1200 [00:13&lt;00:09, 43.38it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  65%|██████▌   | 785/1200 [00:13&lt;00:09, 45.21it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  66%|██████▌   | 790/1200 [00:13&lt;00:09, 45.17it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  66%|██████▋   | 795/1200 [00:13&lt;00:09, 44.89it/s, 511 steps of size 9.97e-06. acc. prob=0.96] sample:  67%|██████▋   | 801/1200 [00:14&lt;00:08, 47.48it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  67%|██████▋   | 806/1200 [00:14&lt;00:08, 44.72it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  68%|██████▊   | 811/1200 [00:14&lt;00:09, 42.86it/s, 511 steps of size 9.97e-06. acc. prob=0.96] sample:  68%|██████▊   | 816/1200 [00:14&lt;00:08, 43.51it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  68%|██████▊   | 822/1200 [00:14&lt;00:08, 46.57it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  69%|██████▉   | 827/1200 [00:14&lt;00:08, 46.19it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  69%|██████▉   | 833/1200 [00:14&lt;00:07, 48.56it/s, 511 steps of size 9.97e-06. acc. prob=0.96] sample:  70%|██████▉   | 838/1200 [00:14&lt;00:07, 45.42it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  70%|███████   | 843/1200 [00:14&lt;00:07, 45.27it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  71%|███████   | 849/1200 [00:15&lt;00:07, 47.56it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  71%|███████   | 854/1200 [00:15&lt;00:07, 46.79it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  72%|███████▏  | 859/1200 [00:15&lt;00:07, 46.26it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  72%|███████▏  | 864/1200 [00:15&lt;00:08, 41.86it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  72%|███████▏  | 869/1200 [00:15&lt;00:07, 42.81it/s, 511 steps of size 9.97e-06. acc. prob=0.96] sample:  73%|███████▎  | 875/1200 [00:15&lt;00:07, 46.01it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  73%|███████▎  | 880/1200 [00:15&lt;00:06, 45.73it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  74%|███████▍  | 885/1200 [00:15&lt;00:06, 45.59it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  74%|███████▍  | 890/1200 [00:16&lt;00:06, 45.48it/s, 511 steps of size 9.97e-06. acc. prob=0.96] sample:  75%|███████▍  | 895/1200 [00:16&lt;00:07, 41.36it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  75%|███████▌  | 901/1200 [00:16&lt;00:07, 42.61it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  76%|███████▌  | 906/1200 [00:16&lt;00:07, 40.60it/s, 511 steps of size 9.97e-06. acc. prob=0.96] sample:  76%|███████▌  | 911/1200 [00:16&lt;00:06, 41.68it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  76%|███████▋  | 916/1200 [00:16&lt;00:06, 40.73it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  77%|███████▋  | 922/1200 [00:16&lt;00:06, 42.47it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  77%|███████▋  | 927/1200 [00:16&lt;00:06, 43.19it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  78%|███████▊  | 932/1200 [00:17&lt;00:06, 38.48it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  78%|███████▊  | 936/1200 [00:17&lt;00:06, 37.93it/s, 511 steps of size 9.97e-06. acc. prob=0.96] sample:  78%|███████▊  | 940/1200 [00:17&lt;00:06, 38.30it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  79%|███████▊  | 944/1200 [00:17&lt;00:06, 37.59it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  79%|███████▉  | 949/1200 [00:17&lt;00:06, 37.65it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  80%|███████▉  | 955/1200 [00:17&lt;00:05, 41.97it/s, 511 steps of size 9.97e-06. acc. prob=0.96] sample:  80%|████████  | 960/1200 [00:17&lt;00:06, 38.89it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  80%|████████  | 965/1200 [00:17&lt;00:05, 40.57it/s, 511 steps of size 9.97e-06. acc. prob=0.96] sample:  81%|████████  | 970/1200 [00:18&lt;00:05, 39.95it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  81%|████████▏ | 975/1200 [00:18&lt;00:05, 39.47it/s, 511 steps of size 9.97e-06. acc. prob=0.96] sample:  82%|████████▏ | 980/1200 [00:18&lt;00:05, 40.75it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  82%|████████▏ | 985/1200 [00:18&lt;00:06, 35.10it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  82%|████████▏ | 989/1200 [00:18&lt;00:05, 35.39it/s, 511 steps of size 9.97e-06. acc. prob=0.96] sample:  83%|████████▎ | 994/1200 [00:18&lt;00:05, 37.93it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  83%|████████▎ | 998/1200 [00:18&lt;00:05, 35.85it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  84%|████████▎ | 1002/1200 [00:18&lt;00:05, 34.41it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  84%|████████▍ | 1006/1200 [00:19&lt;00:05, 34.42it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  84%|████████▍ | 1010/1200 [00:19&lt;00:05, 34.93it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  84%|████████▍ | 1014/1200 [00:19&lt;00:05, 33.71it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  85%|████████▍ | 1018/1200 [00:19&lt;00:05, 32.86it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  85%|████████▌ | 1022/1200 [00:19&lt;00:05, 32.36it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  86%|████████▌ | 1026/1200 [00:19&lt;00:05, 31.94it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  86%|████████▌ | 1031/1200 [00:19&lt;00:04, 35.54it/s, 511 steps of size 9.97e-06. acc. prob=0.96] sample:  86%|████████▋ | 1035/1200 [00:19&lt;00:04, 33.66it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  87%|████████▋ | 1039/1200 [00:20&lt;00:04, 32.82it/s, 511 steps of size 9.97e-06. acc. prob=0.96] sample:  87%|████████▋ | 1043/1200 [00:20&lt;00:05, 30.95it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  87%|████████▋ | 1047/1200 [00:20&lt;00:04, 32.17it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  88%|████████▊ | 1051/1200 [00:20&lt;00:04, 31.62it/s, 511 steps of size 9.97e-06. acc. prob=0.96] sample:  88%|████████▊ | 1055/1200 [00:20&lt;00:04, 29.32it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  88%|████████▊ | 1058/1200 [00:20&lt;00:05, 27.07it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  88%|████████▊ | 1061/1200 [00:20&lt;00:05, 27.08it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  89%|████████▉ | 1065/1200 [00:20&lt;00:04, 28.25it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  89%|████████▉ | 1068/1200 [00:21&lt;00:04, 27.96it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  89%|████████▉ | 1071/1200 [00:21&lt;00:04, 27.56it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  90%|████████▉ | 1074/1200 [00:21&lt;00:04, 27.28it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  90%|████████▉ | 1077/1200 [00:21&lt;00:04, 27.17it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  90%|█████████ | 1080/1200 [00:21&lt;00:04, 27.14it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  90%|█████████ | 1083/1200 [00:21&lt;00:04, 27.11it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  90%|█████████ | 1086/1200 [00:21&lt;00:04, 27.08it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  91%|█████████ | 1090/1200 [00:21&lt;00:03, 28.32it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  91%|█████████ | 1093/1200 [00:22&lt;00:03, 27.97it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  91%|█████████▏| 1096/1200 [00:22&lt;00:03, 27.70it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  92%|█████████▏| 1100/1200 [00:22&lt;00:03, 28.80it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  92%|█████████▏| 1103/1200 [00:22&lt;00:03, 28.32it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  92%|█████████▏| 1106/1200 [00:22&lt;00:03, 27.80it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  92%|█████████▏| 1109/1200 [00:22&lt;00:03, 27.30it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  93%|█████████▎| 1112/1200 [00:22&lt;00:03, 27.11it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  93%|█████████▎| 1115/1200 [00:22&lt;00:03, 26.92it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  93%|█████████▎| 1118/1200 [00:22&lt;00:03, 26.71it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  93%|█████████▎| 1121/1200 [00:23&lt;00:02, 26.55it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  94%|█████████▎| 1124/1200 [00:23&lt;00:02, 26.48it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  94%|█████████▍| 1127/1200 [00:23&lt;00:02, 26.45it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  94%|█████████▍| 1130/1200 [00:23&lt;00:02, 26.52it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  94%|█████████▍| 1133/1200 [00:23&lt;00:02, 26.56it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  95%|█████████▍| 1136/1200 [00:23&lt;00:02, 26.67it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  95%|█████████▍| 1139/1200 [00:23&lt;00:02, 26.77it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  95%|█████████▌| 1142/1200 [00:23&lt;00:02, 26.71it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  95%|█████████▌| 1145/1200 [00:23&lt;00:02, 25.81it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  96%|█████████▌| 1148/1200 [00:24&lt;00:01, 26.11it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  96%|█████████▌| 1151/1200 [00:24&lt;00:01, 26.31it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  96%|█████████▌| 1154/1200 [00:24&lt;00:01, 26.42it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  96%|█████████▋| 1157/1200 [00:24&lt;00:01, 26.44it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  97%|█████████▋| 1160/1200 [00:24&lt;00:01, 26.20it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  97%|█████████▋| 1163/1200 [00:24&lt;00:01, 26.30it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  97%|█████████▋| 1166/1200 [00:24&lt;00:01, 26.14it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  97%|█████████▋| 1169/1200 [00:24&lt;00:01, 26.34it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  98%|█████████▊| 1172/1200 [00:24&lt;00:01, 26.48it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  98%|█████████▊| 1175/1200 [00:25&lt;00:00, 26.63it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  98%|█████████▊| 1178/1200 [00:25&lt;00:00, 26.81it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  98%|█████████▊| 1181/1200 [00:25&lt;00:00, 26.91it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  99%|█████████▊| 1184/1200 [00:25&lt;00:00, 26.86it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  99%|█████████▉| 1187/1200 [00:25&lt;00:00, 26.75it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  99%|█████████▉| 1190/1200 [00:25&lt;00:00, 25.89it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  99%|█████████▉| 1193/1200 [00:25&lt;00:00, 26.17it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample: 100%|█████████▉| 1196/1200 [00:25&lt;00:00, 25.86it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample: 100%|█████████▉| 1199/1200 [00:25&lt;00:00, 26.12it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample: 100%|██████████| 1200/1200 [00:26&lt;00:00, 46.10it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]\n\n\nProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3,\n                                                                              10],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[7, 365.25]),\n                                 '^$')],\n             inference_engine=MCMCInferenceEngine(),\n             trend=PiecewiseLinearTrend(changepoint_interval=500,\n                                        changepoint_prior_scale=1e-05,\n                                        changepoint_range=-500))Please rerun this cell to show the HTML repr or trust the notebook.ProphetverseProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3,\n                                                                              10],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[7, 365.25]),\n                                 '^$')],\n             inference_engine=MCMCInferenceEngine(),\n             trend=PiecewiseLinearTrend(changepoint_interval=500,\n                                        changepoint_prior_scale=1e-05,\n                                        changepoint_range=-500))effectsPiecewiseLinearTrendPiecewiseLinearTrend(changepoint_interval=500, changepoint_prior_scale=1e-05,\n                     changepoint_range=-500)LinearFourierSeasonalityLinearFourierSeasonality(effect_mode='multiplicative',\n                         fourier_terms_list=[3, 10], freq='D', prior_scale=0.1,\n                         sp_list=[7, 365.25])inference_engineMCMCInferenceEngineMCMCInferenceEngine()\n\n\n\nquantiles = model.predict_quantiles(fh=forecast_horizon, alpha=[0.75, 0.25])\nfig, ax = plt.subplots(figsize=(10, 5))\n# Plot area between quantiles\nax.fill_between(\n    quantiles.index.to_timestamp(),\n    quantiles.iloc[:, 0],\n    quantiles.iloc[:, -1],\n    alpha=0.5,\n)\nax.scatter(y.index, y, marker=\"o\", color=\"k\", s=2, alpha=1)\n\n\n\n\n\n\n\n\nOne interesting feature of MCMC is that it allows us to obtain samples from the posterior distribution of the parameters. In other words, we can also obtain probabilistic forecasts for the TS components.\n\nsamples = model.predict_component_samples(fh=forecast_horizon)\nsamples\n\n\n\n\n\n\n\n\n\nmean\nobs\nseasonality\ntrend\n\n\nsample\n\n\n\n\n\n\n\n\n\n0\n2007-01-01\n13.216102\n13.196515\n4.843813\n8.372289\n\n\n2007-01-02\n13.454115\n13.701294\n5.081825\n8.372289\n\n\n2007-01-03\n11.383386\n11.329435\n3.011097\n8.372289\n\n\n2007-01-04\n10.875955\n10.969802\n2.503665\n8.372289\n\n\n2007-01-05\n12.315873\n12.678445\n3.943584\n8.372289\n\n\n...\n...\n...\n...\n...\n...\n\n\n999\n2017-12-28\n7.777440\n7.705639\n0.620472\n7.156968\n\n\n2017-12-29\n8.144931\n8.526028\n0.988648\n7.156283\n\n\n2017-12-30\n7.878786\n7.964994\n0.723189\n7.155597\n\n\n2017-12-31\n7.821858\n7.957323\n0.666947\n7.154911\n\n\n2018-01-01\n8.740103\n8.565384\n1.585878\n7.154225\n\n\n\n\n4019000 rows × 4 columns\n\n\n\n\n\n\nIn Prophetverse, we’ve implemented the &gt;&gt; operator, which makes it easier to set trend, exogenous_effects and inference_engine parameters.\n\ntrend = PiecewiseLinearTrend(\n    changepoint_interval=300,\n    changepoint_prior_scale=0.0001,\n    changepoint_range=0.8,\n)\nexogenous_effects = [\n    (\n        \"seasonality\",\n        LinearFourierSeasonality(\n            freq=\"D\",\n            sp_list=[7, 365.25],\n            fourier_terms_list=[3, 10],\n            prior_scale=0.1,\n            effect_mode=\"multiplicative\",\n        ),\n        no_input_columns,\n    ),\n]\ninference_engine = MAPInferenceEngine()\n\nmodel = Prophetverse() &gt;&gt; trend &gt;&gt; exogenous_effects &gt;&gt; inference_engine\nmodel.fit(y=y)\n\nProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3,\n                                                                              10],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[7, 365.25]),\n                                 '^$')],\n             inference_engine=MAPInferenceEngine(),\n             trend=PiecewiseLinearTrend(changepoint_interval=300,\n                                        changepoint_prior_scale=0.0001))Please rerun this cell to show the HTML repr or trust the notebook.ProphetverseProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3,\n                                                                              10],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[7, 365.25]),\n                                 '^$')],\n             inference_engine=MAPInferenceEngine(),\n             trend=PiecewiseLinearTrend(changepoint_interval=300,\n                                        changepoint_prior_scale=0.0001))effectsPiecewiseLinearTrendPiecewiseLinearTrend(changepoint_interval=300, changepoint_prior_scale=0.0001)LinearFourierSeasonalityLinearFourierSeasonality(effect_mode='multiplicative',\n                         fourier_terms_list=[3, 10], freq='D', prior_scale=0.1,\n                         sp_list=[7, 365.25])inference_engineMAPInferenceEngineMAPInferenceEngine()\n\n\n\nforecast_horizon = pd.period_range(\"2007-01-01\", \"2018-01-01\", freq=\"D\")\nfig, ax = plt.subplots(figsize=(10, 5))\npreds = model.predict(fh=forecast_horizon)\npreds.plot.line(ax=ax)\nax.scatter(y.index, y, marker=\"o\", color=\"k\", s=2, alpha=0.5)",
    "crumbs": [
      "Home",
      "Tutorial",
      "Basic Univariate Forecasting with Prophetverse"
    ]
  },
  {
    "objectID": "tutorial/univariate.html#import-dataset",
    "href": "tutorial/univariate.html#import-dataset",
    "title": "Basic Univariate Forecasting with Prophetverse",
    "section": "",
    "text": "We import a dataset from Prophet’s original repository. We then put it into sktime-friendly format, where the index is a pd.PeriodIndex and the colums are the time series.\n\nfrom prophetverse.datasets.loaders import load_peyton_manning\n\ny = load_peyton_manning()\ndisplay(y.head())\n\n\n\n\n\n\n\n\ny\n\n\nds\n\n\n\n\n\n2007-12-10\n9.590761\n\n\n2007-12-11\n8.519590\n\n\n2007-12-12\n8.183677\n\n\n2007-12-13\n8.072467\n\n\n2007-12-14\n7.893572\n\n\n\n\n\n\n\nThe full dataset looks like this:\n\ny.plot.line(figsize=(12, 6))\nplt.show()",
    "crumbs": [
      "Home",
      "Tutorial",
      "Basic Univariate Forecasting with Prophetverse"
    ]
  },
  {
    "objectID": "tutorial/univariate.html#fit-model",
    "href": "tutorial/univariate.html#fit-model",
    "title": "Basic Univariate Forecasting with Prophetverse",
    "section": "",
    "text": "Here, we will show how you can fit a simple model with Prophetverse. We first fit a model without seasonal components, and then fit a full model. We also show how easy it is to switch between Maximum A Posteriori (MAP) inference and Markov Chain Monte Carlo (MCMC).\n\n\n\nfrom prophetverse.effects.trend import PiecewiseLinearTrend\nfrom prophetverse.engine import MAPInferenceEngine\nfrom prophetverse.sktime import Prophetverse\nfrom prophetverse.utils import no_input_columns\n\nmodel = Prophetverse(\n    trend=PiecewiseLinearTrend(\n        changepoint_interval=500,\n        changepoint_prior_scale=0.00001,\n        changepoint_range=-250,\n    ),\n    inference_engine=MAPInferenceEngine(),\n)\nmodel.fit(y=y)\n\nProphetverse(inference_engine=MAPInferenceEngine(),\n             trend=PiecewiseLinearTrend(changepoint_interval=500,\n                                        changepoint_prior_scale=1e-05,\n                                        changepoint_range=-250))Please rerun this cell to show the HTML repr or trust the notebook.ProphetverseProphetverse(inference_engine=MAPInferenceEngine(),\n             trend=PiecewiseLinearTrend(changepoint_interval=500,\n                                        changepoint_prior_scale=1e-05,\n                                        changepoint_range=-250))effectsPiecewiseLinearTrendPiecewiseLinearTrend(changepoint_interval=500, changepoint_prior_scale=1e-05,\n                     changepoint_range=-250)inference_engineMAPInferenceEngineMAPInferenceEngine()\n\n\n\nforecast_horizon = pd.period_range(\"2007-01-01\", \"2018-01-01\", freq=\"D\")\nfig, ax = plt.subplots(figsize=(10, 5))\npreds = model.predict(fh=forecast_horizon)\npreds.plot.line(ax=ax)\nax.scatter(y.index, y, marker=\"o\", color=\"k\", s=2, alpha=0.5)\n\n\n\n\n\n\n\n\n\n\n\nHere, we fit the univariate Prophet and pass an exogenous effect as hyperparameter. The exogenous_effects parameter let us add new components to the model and control the relationship between exogenous variables and the target variable.\nIn this case, the LinearFourierSeasonality effect creates sinusoidal and cosine terms to model the seasonality of the time series, which are then multiplied by linear coefficients and added to the model.\nThis argument is a list of tuples of the form (effect_name, effect, regex_to_filter_relevant_columns), where effect_name is a string and effect is an instance of a subclass of prophetverse.effects.BaseEffect. The regex is used to filter the columns of X that are relevant for the effect, but can also be None (or its alias prophetverse.utils.no_input_columns) if no input in X is needed for the effect.\n\nfrom prophetverse.effects.fourier import LinearFourierSeasonality\nfrom prophetverse.utils import no_input_columns\n\nmodel = Prophetverse(\n    trend=PiecewiseLinearTrend(\n        changepoint_interval=500,\n        changepoint_prior_scale=0.00001,\n        changepoint_range=-500,\n    ),\n    exogenous_effects=[\n        (\n            \"seasonality\",\n            LinearFourierSeasonality(\n                freq=\"D\",\n                sp_list=[7, 365.25],\n                fourier_terms_list=[3, 10],\n                prior_scale=0.1,\n                effect_mode=\"multiplicative\",\n            ),\n            no_input_columns,\n        ),\n    ],\n    inference_engine=MAPInferenceEngine(),\n)\nmodel.fit(y=y)\n\nProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3,\n                                                                              10],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[7, 365.25]),\n                                 '^$')],\n             inference_engine=MAPInferenceEngine(),\n             trend=PiecewiseLinearTrend(changepoint_interval=500,\n                                        changepoint_prior_scale=1e-05,\n                                        changepoint_range=-500))Please rerun this cell to show the HTML repr or trust the notebook.ProphetverseProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3,\n                                                                              10],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[7, 365.25]),\n                                 '^$')],\n             inference_engine=MAPInferenceEngine(),\n             trend=PiecewiseLinearTrend(changepoint_interval=500,\n                                        changepoint_prior_scale=1e-05,\n                                        changepoint_range=-500))effectsPiecewiseLinearTrendPiecewiseLinearTrend(changepoint_interval=500, changepoint_prior_scale=1e-05,\n                     changepoint_range=-500)LinearFourierSeasonalityLinearFourierSeasonality(effect_mode='multiplicative',\n                         fourier_terms_list=[3, 10], freq='D', prior_scale=0.1,\n                         sp_list=[7, 365.25])inference_engineMAPInferenceEngineMAPInferenceEngine()\n\n\n\nforecast_horizon = pd.period_range(\"2007-01-01\", \"2018-01-01\", freq=\"D\")\nfig, ax = plt.subplots(figsize=(10, 5))\npreds = model.predict(fh=forecast_horizon)\npreds.plot.line(ax=ax)\nax.scatter(y.index, y, marker=\"o\", color=\"k\", s=2, alpha=0.5)",
    "crumbs": [
      "Home",
      "Tutorial",
      "Basic Univariate Forecasting with Prophetverse"
    ]
  },
  {
    "objectID": "tutorial/univariate.html#probabilistic-forecasting",
    "href": "tutorial/univariate.html#probabilistic-forecasting",
    "title": "Basic Univariate Forecasting with Prophetverse",
    "section": "",
    "text": "We can also make probabilistic forecasts with Prophetverse, in sktime fashion. The predict_quantiles method returns the quantiles of the predictive distribution in a pd.DataFrame\n\nquantiles = model.predict_quantiles(fh=forecast_horizon, alpha=[0.1, 0.9])\nquantiles.head()\n\n\n\n\n\n\n\n\ny\n\n\n\n0.1\n0.9\n\n\n\n\n2007-01-01\n8.066615\n9.367413\n\n\n2007-01-02\n7.873573\n9.173687\n\n\n2007-01-03\n7.761128\n9.031336\n\n\n2007-01-04\n7.715733\n9.067670\n\n\n2007-01-05\n7.747439\n9.096114\n\n\n\n\n\n\n\nThe plot below shows the (0.1, 0.9) quantiles of the predictive distribution\n\nfig, ax = plt.subplots(figsize=(10, 5))\n# Plot area between quantiles\nax.fill_between(\n    quantiles.index.to_timestamp(),\n    quantiles.iloc[:, 0],\n    quantiles.iloc[:, -1],\n    alpha=0.5,\n)\nax.scatter(y.index, y, marker=\"o\", color=\"k\", s=2, alpha=1)",
    "crumbs": [
      "Home",
      "Tutorial",
      "Basic Univariate Forecasting with Prophetverse"
    ]
  },
  {
    "objectID": "tutorial/univariate.html#timeseries-decomposition",
    "href": "tutorial/univariate.html#timeseries-decomposition",
    "title": "Basic Univariate Forecasting with Prophetverse",
    "section": "",
    "text": "We can easily extract the components of the time series with the predict_components method. This method, in particular, is not implemented in sktime’s BaseForecaster, but it is a method of prophetverse.Prophetverse class.\n\nsites = model.predict_components(fh=forecast_horizon)\nsites.head()\n\n\n\n\n\n\n\n\nmean\nobs\nseasonality\ntrend\n\n\n\n\n2007-01-01\n8.716851\n8.730150\n0.916735\n7.800113\n\n\n2007-01-02\n8.521084\n8.512227\n0.720966\n7.800113\n\n\n2007-01-03\n8.367682\n8.385690\n0.567568\n7.800113\n\n\n2007-01-04\n8.386809\n8.394879\n0.586693\n7.800113\n\n\n2007-01-05\n8.418292\n8.409329\n0.618176\n7.800113\n\n\n\n\n\n\n\n\nfor column in sites.columns:\n    fig, ax = plt.subplots(figsize=(8, 2))\n    ax.plot(sites.index.to_timestamp(), sites[column], label=column)\n    ax.set_title(column)\n    fig.show()",
    "crumbs": [
      "Home",
      "Tutorial",
      "Basic Univariate Forecasting with Prophetverse"
    ]
  },
  {
    "objectID": "tutorial/univariate.html#fitting-with-mcmc",
    "href": "tutorial/univariate.html#fitting-with-mcmc",
    "title": "Basic Univariate Forecasting with Prophetverse",
    "section": "",
    "text": "In the previous examples, we used MAP inference to fit the model. However, we can also use Markov Chain Monte Carlo (MCMC) to fit the model. To do this, we just need to change the inference_engine parameter to MCMCInferenceEngine. The rest of the code remains the same.\nThe set_params method is used to set the parameters of the model, in sklearn fashion.\n\nfrom prophetverse.engine import MCMCInferenceEngine\n\nmodel.set_params(inference_engine=MCMCInferenceEngine())\n\nimport numpyro\n\nnumpyro.enable_x64()  # To avoid computational issues with MCMC\n\nmodel.fit(y=y)\n\n  0%|          | 0/1200 [00:00&lt;?, ?it/s]warmup:   0%|          | 1/1200 [00:01&lt;30:13,  1.51s/it, 1 steps of size 2.34e+00. acc. prob=0.00]warmup:   4%|▎         | 42/1200 [00:01&lt;00:32, 35.77it/s, 511 steps of size 2.03e-07. acc. prob=0.63]warmup:   5%|▌         | 65/1200 [00:01&lt;00:22, 50.36it/s, 255 steps of size 1.37e-07. acc. prob=0.67]warmup:   7%|▋         | 82/1200 [00:02&lt;00:18, 59.91it/s, 127 steps of size 1.95e-07. acc. prob=0.69]warmup:   8%|▊         | 97/1200 [00:02&lt;00:16, 66.39it/s, 255 steps of size 1.00e-07. acc. prob=0.70]warmup:   9%|▉         | 110/1200 [00:02&lt;00:17, 63.81it/s, 255 steps of size 2.11e-06. acc. prob=0.71]warmup:  10%|█         | 121/1200 [00:02&lt;00:17, 60.71it/s, 1023 steps of size 2.63e-06. acc. prob=0.72]warmup:  11%|█         | 130/1200 [00:02&lt;00:17, 61.40it/s, 511 steps of size 4.79e-06. acc. prob=0.73] warmup:  12%|█▏        | 138/1200 [00:02&lt;00:17, 61.93it/s, 255 steps of size 2.37e-05. acc. prob=0.74]warmup:  12%|█▏        | 146/1200 [00:02&lt;00:16, 63.99it/s, 255 steps of size 1.22e-05. acc. prob=0.74]warmup:  13%|█▎        | 154/1200 [00:03&lt;00:15, 66.23it/s, 1023 steps of size 1.17e-05. acc. prob=0.74]warmup:  14%|█▍        | 166/1200 [00:03&lt;00:14, 72.53it/s, 1023 steps of size 4.71e-06. acc. prob=0.74]warmup:  14%|█▍        | 174/1200 [00:03&lt;00:14, 71.44it/s, 127 steps of size 2.49e-05. acc. prob=0.74] warmup:  15%|█▌        | 182/1200 [00:03&lt;00:15, 64.32it/s, 255 steps of size 2.54e-05. acc. prob=0.75]warmup:  16%|█▌        | 190/1200 [00:03&lt;00:14, 67.36it/s, 255 steps of size 1.76e-05. acc. prob=0.75]warmup:  16%|█▋        | 198/1200 [00:03&lt;00:14, 68.30it/s, 127 steps of size 2.14e-05. acc. prob=0.75]sample:  17%|█▋        | 208/1200 [00:03&lt;00:13, 76.09it/s, 255 steps of size 9.97e-06. acc. prob=0.96]sample:  18%|█▊        | 216/1200 [00:03&lt;00:13, 72.02it/s, 255 steps of size 9.97e-06. acc. prob=0.95]sample:  19%|█▉        | 225/1200 [00:04&lt;00:12, 76.12it/s, 255 steps of size 9.97e-06. acc. prob=0.95]sample:  19%|█▉        | 233/1200 [00:04&lt;00:12, 76.40it/s, 511 steps of size 9.97e-06. acc. prob=0.95]sample:  20%|██        | 241/1200 [00:04&lt;00:12, 74.99it/s, 1023 steps of size 9.97e-06. acc. prob=0.95]sample:  21%|██        | 250/1200 [00:04&lt;00:12, 75.92it/s, 511 steps of size 9.97e-06. acc. prob=0.95] sample:  22%|██▏       | 258/1200 [00:04&lt;00:14, 65.97it/s, 255 steps of size 9.97e-06. acc. prob=0.95]sample:  22%|██▏       | 268/1200 [00:04&lt;00:12, 73.55it/s, 255 steps of size 9.97e-06. acc. prob=0.95]sample:  23%|██▎       | 278/1200 [00:04&lt;00:11, 77.25it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  24%|██▍       | 286/1200 [00:04&lt;00:11, 76.87it/s, 255 steps of size 9.97e-06. acc. prob=0.96]sample:  25%|██▍       | 296/1200 [00:04&lt;00:10, 82.54it/s, 255 steps of size 9.97e-06. acc. prob=0.96]sample:  25%|██▌       | 305/1200 [00:05&lt;00:11, 81.01it/s, 255 steps of size 9.97e-06. acc. prob=0.96]sample:  26%|██▌       | 314/1200 [00:05&lt;00:11, 74.95it/s, 255 steps of size 9.97e-06. acc. prob=0.95]sample:  27%|██▋       | 322/1200 [00:05&lt;00:11, 75.81it/s, 255 steps of size 9.97e-06. acc. prob=0.95]sample:  28%|██▊       | 330/1200 [00:05&lt;00:12, 72.21it/s, 255 steps of size 9.97e-06. acc. prob=0.95]sample:  28%|██▊       | 338/1200 [00:05&lt;00:11, 73.37it/s, 511 steps of size 9.97e-06. acc. prob=0.95]sample:  29%|██▉       | 346/1200 [00:05&lt;00:12, 70.55it/s, 255 steps of size 9.97e-06. acc. prob=0.95]sample:  30%|██▉       | 354/1200 [00:05&lt;00:11, 70.80it/s, 255 steps of size 9.97e-06. acc. prob=0.96]sample:  30%|███       | 362/1200 [00:05&lt;00:12, 69.18it/s, 255 steps of size 9.97e-06. acc. prob=0.96]sample:  31%|███       | 370/1200 [00:06&lt;00:11, 71.51it/s, 255 steps of size 9.97e-06. acc. prob=0.96]sample:  32%|███▏      | 378/1200 [00:06&lt;00:11, 71.39it/s, 255 steps of size 9.97e-06. acc. prob=0.96]sample:  32%|███▏      | 386/1200 [00:06&lt;00:11, 69.84it/s, 255 steps of size 9.97e-06. acc. prob=0.96]sample:  33%|███▎      | 394/1200 [00:06&lt;00:11, 68.64it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  34%|███▎      | 403/1200 [00:06&lt;00:11, 72.05it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  34%|███▍      | 412/1200 [00:06&lt;00:10, 76.42it/s, 255 steps of size 9.97e-06. acc. prob=0.96]sample:  35%|███▌      | 420/1200 [00:06&lt;00:11, 65.38it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  36%|███▌      | 428/1200 [00:06&lt;00:11, 68.48it/s, 255 steps of size 9.97e-06. acc. prob=0.96] sample:  36%|███▋      | 436/1200 [00:06&lt;00:11, 67.90it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  37%|███▋      | 443/1200 [00:07&lt;00:11, 67.93it/s, 255 steps of size 9.97e-06. acc. prob=0.96]sample:  38%|███▊      | 450/1200 [00:07&lt;00:11, 67.59it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  38%|███▊      | 457/1200 [00:07&lt;00:11, 65.67it/s, 255 steps of size 9.97e-06. acc. prob=0.96] sample:  39%|███▊      | 464/1200 [00:07&lt;00:11, 64.49it/s, 255 steps of size 9.97e-06. acc. prob=0.96]sample:  39%|███▉      | 472/1200 [00:07&lt;00:10, 67.85it/s, 255 steps of size 9.97e-06. acc. prob=0.96]sample:  40%|███▉      | 479/1200 [00:07&lt;00:11, 64.10it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  40%|████      | 486/1200 [00:07&lt;00:11, 61.78it/s, 255 steps of size 9.97e-06. acc. prob=0.96]sample:  41%|████      | 493/1200 [00:07&lt;00:12, 57.70it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  42%|████▏     | 499/1200 [00:08&lt;00:13, 51.33it/s, 255 steps of size 9.97e-06. acc. prob=0.96]sample:  42%|████▏     | 505/1200 [00:08&lt;00:13, 50.97it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  43%|████▎     | 512/1200 [00:08&lt;00:12, 55.06it/s, 255 steps of size 9.97e-06. acc. prob=0.96]sample:  43%|████▎     | 519/1200 [00:08&lt;00:11, 58.26it/s, 255 steps of size 9.97e-06. acc. prob=0.96]sample:  44%|████▍     | 526/1200 [00:08&lt;00:11, 60.60it/s, 255 steps of size 9.97e-06. acc. prob=0.96]sample:  44%|████▍     | 533/1200 [00:08&lt;00:11, 59.73it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  45%|████▌     | 540/1200 [00:08&lt;00:10, 60.29it/s, 255 steps of size 9.97e-06. acc. prob=0.96]sample:  46%|████▌     | 547/1200 [00:08&lt;00:11, 57.61it/s, 255 steps of size 9.97e-06. acc. prob=0.96]sample:  46%|████▌     | 553/1200 [00:08&lt;00:11, 54.58it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  47%|████▋     | 561/1200 [00:09&lt;00:10, 60.35it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  47%|████▋     | 568/1200 [00:09&lt;00:11, 56.79it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  48%|████▊     | 574/1200 [00:09&lt;00:10, 57.14it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  48%|████▊     | 580/1200 [00:09&lt;00:10, 57.28it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  49%|████▉     | 587/1200 [00:09&lt;00:10, 58.44it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  49%|████▉     | 593/1200 [00:09&lt;00:11, 52.93it/s, 255 steps of size 9.97e-06. acc. prob=0.96]sample:  50%|████▉     | 599/1200 [00:09&lt;00:11, 52.84it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  50%|█████     | 606/1200 [00:09&lt;00:10, 56.75it/s, 255 steps of size 9.97e-06. acc. prob=0.96]sample:  51%|█████     | 612/1200 [00:10&lt;00:10, 55.38it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  52%|█████▏    | 618/1200 [00:10&lt;00:10, 55.74it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  52%|█████▏    | 624/1200 [00:10&lt;00:10, 56.33it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  52%|█████▎    | 630/1200 [00:10&lt;00:12, 46.22it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  53%|█████▎    | 636/1200 [00:10&lt;00:11, 48.06it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  54%|█████▎    | 642/1200 [00:10&lt;00:11, 49.33it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  54%|█████▍    | 649/1200 [00:10&lt;00:10, 52.87it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  55%|█████▍    | 655/1200 [00:10&lt;00:11, 49.30it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  55%|█████▌    | 661/1200 [00:11&lt;00:11, 48.24it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  56%|█████▌    | 667/1200 [00:11&lt;00:10, 49.63it/s, 511 steps of size 9.97e-06. acc. prob=0.96] sample:  56%|█████▌    | 673/1200 [00:11&lt;00:10, 50.83it/s, 255 steps of size 9.97e-06. acc. prob=0.96]sample:  57%|█████▋    | 679/1200 [00:11&lt;00:10, 51.80it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  57%|█████▋    | 685/1200 [00:11&lt;00:10, 49.97it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  58%|█████▊    | 691/1200 [00:11&lt;00:10, 48.82it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  58%|█████▊    | 696/1200 [00:11&lt;00:11, 45.66it/s, 511 steps of size 9.97e-06. acc. prob=0.96] sample:  58%|█████▊    | 702/1200 [00:11&lt;00:10, 47.98it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  59%|█████▉    | 707/1200 [00:12&lt;00:11, 43.09it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  59%|█████▉    | 712/1200 [00:12&lt;00:11, 42.71it/s, 511 steps of size 9.97e-06. acc. prob=0.96] sample:  60%|█████▉    | 718/1200 [00:12&lt;00:10, 46.76it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  60%|██████    | 723/1200 [00:12&lt;00:10, 44.28it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  61%|██████    | 729/1200 [00:12&lt;00:09, 48.12it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  61%|██████    | 734/1200 [00:12&lt;00:09, 47.27it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  62%|██████▏   | 739/1200 [00:12&lt;00:09, 46.66it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  62%|██████▏   | 745/1200 [00:12&lt;00:09, 47.60it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  63%|██████▎   | 751/1200 [00:12&lt;00:08, 50.37it/s, 255 steps of size 9.97e-06. acc. prob=0.96] sample:  63%|██████▎   | 757/1200 [00:13&lt;00:09, 46.87it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  64%|██████▎   | 763/1200 [00:13&lt;00:08, 48.68it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  64%|██████▍   | 768/1200 [00:13&lt;00:09, 45.14it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  64%|██████▍   | 774/1200 [00:13&lt;00:08, 48.76it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  65%|██████▍   | 779/1200 [00:13&lt;00:09, 43.38it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  65%|██████▌   | 785/1200 [00:13&lt;00:09, 45.21it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  66%|██████▌   | 790/1200 [00:13&lt;00:09, 45.17it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  66%|██████▋   | 795/1200 [00:13&lt;00:09, 44.89it/s, 511 steps of size 9.97e-06. acc. prob=0.96] sample:  67%|██████▋   | 801/1200 [00:14&lt;00:08, 47.48it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  67%|██████▋   | 806/1200 [00:14&lt;00:08, 44.72it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  68%|██████▊   | 811/1200 [00:14&lt;00:09, 42.86it/s, 511 steps of size 9.97e-06. acc. prob=0.96] sample:  68%|██████▊   | 816/1200 [00:14&lt;00:08, 43.51it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  68%|██████▊   | 822/1200 [00:14&lt;00:08, 46.57it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  69%|██████▉   | 827/1200 [00:14&lt;00:08, 46.19it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  69%|██████▉   | 833/1200 [00:14&lt;00:07, 48.56it/s, 511 steps of size 9.97e-06. acc. prob=0.96] sample:  70%|██████▉   | 838/1200 [00:14&lt;00:07, 45.42it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  70%|███████   | 843/1200 [00:14&lt;00:07, 45.27it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  71%|███████   | 849/1200 [00:15&lt;00:07, 47.56it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  71%|███████   | 854/1200 [00:15&lt;00:07, 46.79it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  72%|███████▏  | 859/1200 [00:15&lt;00:07, 46.26it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  72%|███████▏  | 864/1200 [00:15&lt;00:08, 41.86it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  72%|███████▏  | 869/1200 [00:15&lt;00:07, 42.81it/s, 511 steps of size 9.97e-06. acc. prob=0.96] sample:  73%|███████▎  | 875/1200 [00:15&lt;00:07, 46.01it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  73%|███████▎  | 880/1200 [00:15&lt;00:06, 45.73it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  74%|███████▍  | 885/1200 [00:15&lt;00:06, 45.59it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  74%|███████▍  | 890/1200 [00:16&lt;00:06, 45.48it/s, 511 steps of size 9.97e-06. acc. prob=0.96] sample:  75%|███████▍  | 895/1200 [00:16&lt;00:07, 41.36it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  75%|███████▌  | 901/1200 [00:16&lt;00:07, 42.61it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  76%|███████▌  | 906/1200 [00:16&lt;00:07, 40.60it/s, 511 steps of size 9.97e-06. acc. prob=0.96] sample:  76%|███████▌  | 911/1200 [00:16&lt;00:06, 41.68it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  76%|███████▋  | 916/1200 [00:16&lt;00:06, 40.73it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  77%|███████▋  | 922/1200 [00:16&lt;00:06, 42.47it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  77%|███████▋  | 927/1200 [00:16&lt;00:06, 43.19it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  78%|███████▊  | 932/1200 [00:17&lt;00:06, 38.48it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  78%|███████▊  | 936/1200 [00:17&lt;00:06, 37.93it/s, 511 steps of size 9.97e-06. acc. prob=0.96] sample:  78%|███████▊  | 940/1200 [00:17&lt;00:06, 38.30it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  79%|███████▊  | 944/1200 [00:17&lt;00:06, 37.59it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  79%|███████▉  | 949/1200 [00:17&lt;00:06, 37.65it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  80%|███████▉  | 955/1200 [00:17&lt;00:05, 41.97it/s, 511 steps of size 9.97e-06. acc. prob=0.96] sample:  80%|████████  | 960/1200 [00:17&lt;00:06, 38.89it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  80%|████████  | 965/1200 [00:17&lt;00:05, 40.57it/s, 511 steps of size 9.97e-06. acc. prob=0.96] sample:  81%|████████  | 970/1200 [00:18&lt;00:05, 39.95it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  81%|████████▏ | 975/1200 [00:18&lt;00:05, 39.47it/s, 511 steps of size 9.97e-06. acc. prob=0.96] sample:  82%|████████▏ | 980/1200 [00:18&lt;00:05, 40.75it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  82%|████████▏ | 985/1200 [00:18&lt;00:06, 35.10it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  82%|████████▏ | 989/1200 [00:18&lt;00:05, 35.39it/s, 511 steps of size 9.97e-06. acc. prob=0.96] sample:  83%|████████▎ | 994/1200 [00:18&lt;00:05, 37.93it/s, 511 steps of size 9.97e-06. acc. prob=0.96]sample:  83%|████████▎ | 998/1200 [00:18&lt;00:05, 35.85it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  84%|████████▎ | 1002/1200 [00:18&lt;00:05, 34.41it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  84%|████████▍ | 1006/1200 [00:19&lt;00:05, 34.42it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  84%|████████▍ | 1010/1200 [00:19&lt;00:05, 34.93it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  84%|████████▍ | 1014/1200 [00:19&lt;00:05, 33.71it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  85%|████████▍ | 1018/1200 [00:19&lt;00:05, 32.86it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  85%|████████▌ | 1022/1200 [00:19&lt;00:05, 32.36it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  86%|████████▌ | 1026/1200 [00:19&lt;00:05, 31.94it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  86%|████████▌ | 1031/1200 [00:19&lt;00:04, 35.54it/s, 511 steps of size 9.97e-06. acc. prob=0.96] sample:  86%|████████▋ | 1035/1200 [00:19&lt;00:04, 33.66it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  87%|████████▋ | 1039/1200 [00:20&lt;00:04, 32.82it/s, 511 steps of size 9.97e-06. acc. prob=0.96] sample:  87%|████████▋ | 1043/1200 [00:20&lt;00:05, 30.95it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  87%|████████▋ | 1047/1200 [00:20&lt;00:04, 32.17it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  88%|████████▊ | 1051/1200 [00:20&lt;00:04, 31.62it/s, 511 steps of size 9.97e-06. acc. prob=0.96] sample:  88%|████████▊ | 1055/1200 [00:20&lt;00:04, 29.32it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  88%|████████▊ | 1058/1200 [00:20&lt;00:05, 27.07it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  88%|████████▊ | 1061/1200 [00:20&lt;00:05, 27.08it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  89%|████████▉ | 1065/1200 [00:20&lt;00:04, 28.25it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  89%|████████▉ | 1068/1200 [00:21&lt;00:04, 27.96it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  89%|████████▉ | 1071/1200 [00:21&lt;00:04, 27.56it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  90%|████████▉ | 1074/1200 [00:21&lt;00:04, 27.28it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  90%|████████▉ | 1077/1200 [00:21&lt;00:04, 27.17it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  90%|█████████ | 1080/1200 [00:21&lt;00:04, 27.14it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  90%|█████████ | 1083/1200 [00:21&lt;00:04, 27.11it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  90%|█████████ | 1086/1200 [00:21&lt;00:04, 27.08it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  91%|█████████ | 1090/1200 [00:21&lt;00:03, 28.32it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  91%|█████████ | 1093/1200 [00:22&lt;00:03, 27.97it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  91%|█████████▏| 1096/1200 [00:22&lt;00:03, 27.70it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  92%|█████████▏| 1100/1200 [00:22&lt;00:03, 28.80it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  92%|█████████▏| 1103/1200 [00:22&lt;00:03, 28.32it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  92%|█████████▏| 1106/1200 [00:22&lt;00:03, 27.80it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  92%|█████████▏| 1109/1200 [00:22&lt;00:03, 27.30it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  93%|█████████▎| 1112/1200 [00:22&lt;00:03, 27.11it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  93%|█████████▎| 1115/1200 [00:22&lt;00:03, 26.92it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  93%|█████████▎| 1118/1200 [00:22&lt;00:03, 26.71it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  93%|█████████▎| 1121/1200 [00:23&lt;00:02, 26.55it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  94%|█████████▎| 1124/1200 [00:23&lt;00:02, 26.48it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  94%|█████████▍| 1127/1200 [00:23&lt;00:02, 26.45it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  94%|█████████▍| 1130/1200 [00:23&lt;00:02, 26.52it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  94%|█████████▍| 1133/1200 [00:23&lt;00:02, 26.56it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  95%|█████████▍| 1136/1200 [00:23&lt;00:02, 26.67it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  95%|█████████▍| 1139/1200 [00:23&lt;00:02, 26.77it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  95%|█████████▌| 1142/1200 [00:23&lt;00:02, 26.71it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  95%|█████████▌| 1145/1200 [00:23&lt;00:02, 25.81it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  96%|█████████▌| 1148/1200 [00:24&lt;00:01, 26.11it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  96%|█████████▌| 1151/1200 [00:24&lt;00:01, 26.31it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  96%|█████████▌| 1154/1200 [00:24&lt;00:01, 26.42it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  96%|█████████▋| 1157/1200 [00:24&lt;00:01, 26.44it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  97%|█████████▋| 1160/1200 [00:24&lt;00:01, 26.20it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  97%|█████████▋| 1163/1200 [00:24&lt;00:01, 26.30it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  97%|█████████▋| 1166/1200 [00:24&lt;00:01, 26.14it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  97%|█████████▋| 1169/1200 [00:24&lt;00:01, 26.34it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  98%|█████████▊| 1172/1200 [00:24&lt;00:01, 26.48it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  98%|█████████▊| 1175/1200 [00:25&lt;00:00, 26.63it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  98%|█████████▊| 1178/1200 [00:25&lt;00:00, 26.81it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  98%|█████████▊| 1181/1200 [00:25&lt;00:00, 26.91it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  99%|█████████▊| 1184/1200 [00:25&lt;00:00, 26.86it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  99%|█████████▉| 1187/1200 [00:25&lt;00:00, 26.75it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  99%|█████████▉| 1190/1200 [00:25&lt;00:00, 25.89it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample:  99%|█████████▉| 1193/1200 [00:25&lt;00:00, 26.17it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample: 100%|█████████▉| 1196/1200 [00:25&lt;00:00, 25.86it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample: 100%|█████████▉| 1199/1200 [00:25&lt;00:00, 26.12it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]sample: 100%|██████████| 1200/1200 [00:26&lt;00:00, 46.10it/s, 1023 steps of size 9.97e-06. acc. prob=0.96]\n\n\nProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3,\n                                                                              10],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[7, 365.25]),\n                                 '^$')],\n             inference_engine=MCMCInferenceEngine(),\n             trend=PiecewiseLinearTrend(changepoint_interval=500,\n                                        changepoint_prior_scale=1e-05,\n                                        changepoint_range=-500))Please rerun this cell to show the HTML repr or trust the notebook.ProphetverseProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3,\n                                                                              10],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[7, 365.25]),\n                                 '^$')],\n             inference_engine=MCMCInferenceEngine(),\n             trend=PiecewiseLinearTrend(changepoint_interval=500,\n                                        changepoint_prior_scale=1e-05,\n                                        changepoint_range=-500))effectsPiecewiseLinearTrendPiecewiseLinearTrend(changepoint_interval=500, changepoint_prior_scale=1e-05,\n                     changepoint_range=-500)LinearFourierSeasonalityLinearFourierSeasonality(effect_mode='multiplicative',\n                         fourier_terms_list=[3, 10], freq='D', prior_scale=0.1,\n                         sp_list=[7, 365.25])inference_engineMCMCInferenceEngineMCMCInferenceEngine()\n\n\n\nquantiles = model.predict_quantiles(fh=forecast_horizon, alpha=[0.75, 0.25])\nfig, ax = plt.subplots(figsize=(10, 5))\n# Plot area between quantiles\nax.fill_between(\n    quantiles.index.to_timestamp(),\n    quantiles.iloc[:, 0],\n    quantiles.iloc[:, -1],\n    alpha=0.5,\n)\nax.scatter(y.index, y, marker=\"o\", color=\"k\", s=2, alpha=1)\n\n\n\n\n\n\n\n\nOne interesting feature of MCMC is that it allows us to obtain samples from the posterior distribution of the parameters. In other words, we can also obtain probabilistic forecasts for the TS components.\n\nsamples = model.predict_component_samples(fh=forecast_horizon)\nsamples\n\n\n\n\n\n\n\n\n\nmean\nobs\nseasonality\ntrend\n\n\nsample\n\n\n\n\n\n\n\n\n\n0\n2007-01-01\n13.216102\n13.196515\n4.843813\n8.372289\n\n\n2007-01-02\n13.454115\n13.701294\n5.081825\n8.372289\n\n\n2007-01-03\n11.383386\n11.329435\n3.011097\n8.372289\n\n\n2007-01-04\n10.875955\n10.969802\n2.503665\n8.372289\n\n\n2007-01-05\n12.315873\n12.678445\n3.943584\n8.372289\n\n\n...\n...\n...\n...\n...\n...\n\n\n999\n2017-12-28\n7.777440\n7.705639\n0.620472\n7.156968\n\n\n2017-12-29\n8.144931\n8.526028\n0.988648\n7.156283\n\n\n2017-12-30\n7.878786\n7.964994\n0.723189\n7.155597\n\n\n2017-12-31\n7.821858\n7.957323\n0.666947\n7.154911\n\n\n2018-01-01\n8.740103\n8.565384\n1.585878\n7.154225\n\n\n\n\n4019000 rows × 4 columns",
    "crumbs": [
      "Home",
      "Tutorial",
      "Basic Univariate Forecasting with Prophetverse"
    ]
  },
  {
    "objectID": "tutorial/univariate.html#extra-syntax-sugar",
    "href": "tutorial/univariate.html#extra-syntax-sugar",
    "title": "Basic Univariate Forecasting with Prophetverse",
    "section": "",
    "text": "In Prophetverse, we’ve implemented the &gt;&gt; operator, which makes it easier to set trend, exogenous_effects and inference_engine parameters.\n\ntrend = PiecewiseLinearTrend(\n    changepoint_interval=300,\n    changepoint_prior_scale=0.0001,\n    changepoint_range=0.8,\n)\nexogenous_effects = [\n    (\n        \"seasonality\",\n        LinearFourierSeasonality(\n            freq=\"D\",\n            sp_list=[7, 365.25],\n            fourier_terms_list=[3, 10],\n            prior_scale=0.1,\n            effect_mode=\"multiplicative\",\n        ),\n        no_input_columns,\n    ),\n]\ninference_engine = MAPInferenceEngine()\n\nmodel = Prophetverse() &gt;&gt; trend &gt;&gt; exogenous_effects &gt;&gt; inference_engine\nmodel.fit(y=y)\n\nProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3,\n                                                                              10],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[7, 365.25]),\n                                 '^$')],\n             inference_engine=MAPInferenceEngine(),\n             trend=PiecewiseLinearTrend(changepoint_interval=300,\n                                        changepoint_prior_scale=0.0001))Please rerun this cell to show the HTML repr or trust the notebook.ProphetverseProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3,\n                                                                              10],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[7, 365.25]),\n                                 '^$')],\n             inference_engine=MAPInferenceEngine(),\n             trend=PiecewiseLinearTrend(changepoint_interval=300,\n                                        changepoint_prior_scale=0.0001))effectsPiecewiseLinearTrendPiecewiseLinearTrend(changepoint_interval=300, changepoint_prior_scale=0.0001)LinearFourierSeasonalityLinearFourierSeasonality(effect_mode='multiplicative',\n                         fourier_terms_list=[3, 10], freq='D', prior_scale=0.1,\n                         sp_list=[7, 365.25])inference_engineMAPInferenceEngineMAPInferenceEngine()\n\n\n\nforecast_horizon = pd.period_range(\"2007-01-01\", \"2018-01-01\", freq=\"D\")\nfig, ax = plt.subplots(figsize=(10, 5))\npreds = model.predict(fh=forecast_horizon)\npreds.plot.line(ax=ax)\nax.scatter(y.index, y, marker=\"o\", color=\"k\", s=2, alpha=0.5)",
    "crumbs": [
      "Home",
      "Tutorial",
      "Basic Univariate Forecasting with Prophetverse"
    ]
  },
  {
    "objectID": "mmm/index.html",
    "href": "mmm/index.html",
    "title": "Marketing Mix Modeling",
    "section": "",
    "text": "Marketing Mix Modeling (MMM) is a statistical analysis technique that helps in obtaining insights and planning marketing strategies. It is tightly related to Time Series Analysis — we can think of MMM as a special case of Time Series forecasting, where the goal is to understand the incrementality of different exogenous variables on the target variable.\nWhen Prophetverse was created, the objective was to provide a more up-to-date implementation of Facebook’s Prophet model and to add features and customization options that were not available in the original implementation. However, as the library evolved, it became clear that it could be used for more than just forecasting and that it could be a powerful tool for MMM.\nProphetverse has the following features that make it a great choice for MMM:\nThe following effects may be of interest if you are working on MMM:",
    "crumbs": [
      "Tutorials",
      "Introduction"
    ]
  },
  {
    "objectID": "mmm/index.html#related-libraries",
    "href": "mmm/index.html#related-libraries",
    "title": "Marketing Mix Modeling",
    "section": "Related Libraries",
    "text": "Related Libraries\nI invite you to check out other libraries for MMM. Two of them are:\n\nPyMC-Marketing: This is an amazing project by PyMC’s developers. It is a library that provides a set of tools for building Bayesian models for marketing analytics. The documentation is very comprehensive and a great source of information.\nLightweight-MMM: This library, as far as I know, was created by Google developers based on NumPyro. Now, they are developing a new one called Meridian.",
    "crumbs": [
      "Tutorials",
      "Introduction"
    ]
  },
  {
    "objectID": "mmm/index.html#future-work",
    "href": "mmm/index.html#future-work",
    "title": "Marketing Mix Modeling",
    "section": "Future Work",
    "text": "Future Work\nIn future releases of Prophetverse, we aim to provide more tools for MMM, particularly a modular interface for running budget optimization in MMM models. If you are interested in other features, please let us know by opening an issue in the repository.",
    "crumbs": [
      "Tutorials",
      "Introduction"
    ]
  },
  {
    "objectID": "mmm/fitting_and_calibration.html",
    "href": "mmm/fitting_and_calibration.html",
    "title": "Forecasting, Calibration, and Unified Marketing Measurement",
    "section": "",
    "text": "In this tutorial, we walk through the lifecycle of a modern Marketing Mix Model (MMM), from time-series forecasting to incorporating causal evidence like lift tests and attribution.\nYou will learn:\n👉 Why this matters: MMMs are foundational for budget allocation. But good predictions are not enough — we need credible effect estimates to make real-world decisions.\nLet’s get started!\nSetting up some libraries, float64 precision, and plot style:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpyro\nimport numpyro.distributions as dist\n\nplt.style.use(\"seaborn-v0_8-whitegrid\")\nnumpyro.enable_x64()\nfrom prophetverse.datasets._mmm.dataset1 import get_dataset\n\ny, X, lift_tests, true_components, _ = get_dataset()\nlift_test_search, lift_test_social = lift_tests\n\nprint(f\"y shape: {y.shape}, X shape: {X.shape}\")\nX.head()\n\ny shape: (1828,), X shape: (1828, 2)\n\n\n\n\n\n\n\n\n\nad_spend_search\nad_spend_social_media\n\n\n\n\n2000-01-01\n89076.191178\n98587.488958\n\n\n2000-01-02\n88891.993106\n99066.321168\n\n\n2000-01-03\n89784.955064\n97334.106903\n\n\n2000-01-04\n89931.220681\n101747.300585\n\n\n2000-01-05\n89184.319596\n93825.221809",
    "crumbs": [
      "Fitting, calibrating and Unified Marketing Measurement"
    ]
  },
  {
    "objectID": "mmm/fitting_and_calibration.html#part-1-forecasting-with-adstock-saturation-effects",
    "href": "mmm/fitting_and_calibration.html#part-1-forecasting-with-adstock-saturation-effects",
    "title": "Forecasting, Calibration, and Unified Marketing Measurement",
    "section": "Part 1: Forecasting with Adstock & Saturation Effects",
    "text": "Part 1: Forecasting with Adstock & Saturation Effects\nHere we’ll build a time-series forecasting model that includes:\n\nTrend and seasonality\n\nLagged media effects (Adstock)\n\nDiminishing returns (Saturation / Hill curves)\n\n🔎 Why this matters:\nRaw spend is not immediately effective, and it doesn’t convert linearly.\nCapturing these dynamics is essential to make ROI estimates realistic.\n\nfrom prophetverse.effects import (\n    PiecewiseLinearTrend,\n    LinearFourierSeasonality,\n    ChainedEffects,\n    GeometricAdstockEffect,\n    HillEffect,\n)\nfrom prophetverse.sktime import Prophetverse\nfrom prophetverse.engine import MAPInferenceEngine\nfrom prophetverse.engine.optimizer import LBFGSSolver\n\nyearly = (\n    \"yearly_seasonality\",\n    LinearFourierSeasonality(freq=\"D\", sp_list=[365.25], fourier_terms_list=[5], prior_scale=0.1, effect_mode=\"multiplicative\"),\n    None,\n)\n\nweekly = (\n    \"weekly_seasonality\",\n    LinearFourierSeasonality(freq=\"D\", sp_list=[7], fourier_terms_list=[3], prior_scale=0.05, effect_mode=\"multiplicative\"),\n    None,\n)\n\nhill = HillEffect(\n    half_max_prior=dist.HalfNormal(1),\n    slope_prior=dist.InverseGamma(2, 1),\n    max_effect_prior=dist.HalfNormal(1),\n    effect_mode=\"additive\",\n    input_scale=1e6,\n)\n\nchained_search = (\n    \"ad_spend_search\",\n    ChainedEffects([(\"adstock\", GeometricAdstockEffect()), (\"saturation\", hill)]),\n    \"ad_spend_search\",\n)\nchained_social = (\n    \"ad_spend_social_media\",\n    ChainedEffects([(\"adstock\", GeometricAdstockEffect()), (\"saturation\", hill)]),\n    \"ad_spend_social_media\",\n)\n\n\nbaseline_model = Prophetverse(\n    trend=PiecewiseLinearTrend(changepoint_interval=100),\n    exogenous_effects=[yearly, weekly, chained_search, chained_social],\n    inference_engine=MAPInferenceEngine(\n        num_steps=5000,\n        optimizer=LBFGSSolver(memory_size=300, max_linesearch_steps=300),\n    ),\n)\nbaseline_model.fit(y=y, X=X)\n\nProphetverse(exogenous_effects=[('yearly_seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[5],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[365.25]),\n                                 None),\n                                ('weekly_seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3],\n                                                          freq='D',\n                                                          prior_scale=0.05,\n                                                          sp_list=[7]),\n                                 None),\n                                ('ad_spend_search',\n                                 Chained...\n                                                                   max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x106ca3a90 with batch shape () and event shape ()&gt;,\n                                                                   slope_prior=&lt;numpyro.distributions.continuous.InverseGamma object at 0x320cc1850 with batch shape () and event shape ()&gt;))]),\n                                 'ad_spend_social_media')],\n             inference_engine=MAPInferenceEngine(num_steps=5000,\n                                                 optimizer=LBFGSSolver(max_linesearch_steps=300,\n                                                                       memory_size=300)),\n             trend=PiecewiseLinearTrend(changepoint_interval=100))Please rerun this cell to show the HTML repr or trust the notebook.ProphetverseProphetverse(exogenous_effects=[('yearly_seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[5],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[365.25]),\n                                 None),\n                                ('weekly_seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3],\n                                                          freq='D',\n                                                          prior_scale=0.05,\n                                                          sp_list=[7]),\n                                 None),\n                                ('ad_spend_search',\n                                 Chained...\n                                                                   max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x106ca3a90 with batch shape () and event shape ()&gt;,\n                                                                   slope_prior=&lt;numpyro.distributions.continuous.InverseGamma object at 0x320cc1850 with batch shape () and event shape ()&gt;))]),\n                                 'ad_spend_social_media')],\n             inference_engine=MAPInferenceEngine(num_steps=5000,\n                                                 optimizer=LBFGSSolver(max_linesearch_steps=300,\n                                                                       memory_size=300)),\n             trend=PiecewiseLinearTrend(changepoint_interval=100))effectsPiecewiseLinearTrendPiecewiseLinearTrend(changepoint_interval=100)LinearFourierSeasonalityLinearFourierSeasonality(effect_mode='multiplicative', fourier_terms_list=[5],\n                         freq='D', prior_scale=0.1, sp_list=[365.25])LinearFourierSeasonalityLinearFourierSeasonality(effect_mode='multiplicative', fourier_terms_list=[3],\n                         freq='D', prior_scale=0.05, sp_list=[7])ad_spend_searchChainedEffects(steps=[('adstock', GeometricAdstockEffect()),\n                      ('saturation',\n                       HillEffect(effect_mode='additive',\n                                  half_max_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x33bbe1190 with batch shape () and event shape ()&gt;,\n                                  input_scale=1000000.0,\n                                  max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x106ca3a90 with batch shape () and event shape ()&gt;,\n                                  slope_prior=&lt;numpyro.distributions.continuous.InverseGamma object at 0x320cc1850 with batch shape () and event shape ()&gt;))])GeometricAdstockEffectGeometricAdstockEffect()HillEffectHillEffect(effect_mode='additive',\n           half_max_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x33bbe1190 with batch shape () and event shape ()&gt;,\n           input_scale=1000000.0,\n           max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x106ca3a90 with batch shape () and event shape ()&gt;,\n           slope_prior=&lt;numpyro.distributions.continuous.InverseGamma object at 0x320cc1850 with batch shape () and event shape ()&gt;)ad_spend_social_mediaChainedEffects(steps=[('adstock', GeometricAdstockEffect()),\n                      ('saturation',\n                       HillEffect(effect_mode='additive',\n                                  half_max_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x33bbe1190 with batch shape () and event shape ()&gt;,\n                                  input_scale=1000000.0,\n                                  max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x106ca3a90 with batch shape () and event shape ()&gt;,\n                                  slope_prior=&lt;numpyro.distributions.continuous.InverseGamma object at 0x320cc1850 with batch shape () and event shape ()&gt;))])GeometricAdstockEffectGeometricAdstockEffect()HillEffectHillEffect(effect_mode='additive',\n           half_max_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x33bbe1190 with batch shape () and event shape ()&gt;,\n           input_scale=1000000.0,\n           max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x106ca3a90 with batch shape () and event shape ()&gt;,\n           slope_prior=&lt;numpyro.distributions.continuous.InverseGamma object at 0x320cc1850 with batch shape () and event shape ()&gt;)inference_engineMAPInferenceEngineMAPInferenceEngine(num_steps=5000,\n                   optimizer=LBFGSSolver(max_linesearch_steps=300,\n                                         memory_size=300))\n\n\n\ny_pred = baseline_model.predict(X=X, fh=X.index)\n\nplt.figure(figsize=(8, 4))\ny.plot(label=\"Observed\")\ny_pred.plot(label=\"Predicted\")\nplt.title(\"In-Sample Forecast: Observed vs Predicted\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n1.1 Component-Level Diagnostics\nWith predict_components, we can obtain the model’s components.\n\ny_pred_components = baseline_model.predict_components(X=X, fh=X.index)\ny_pred_components.head()\n\n\n\n\n\n\n\n\nad_spend_search\nad_spend_social_media\nmean\nobs\ntrend\nweekly_seasonality\nyearly_seasonality\n\n\n\n\n2000-01-01\n1.792009e+06\n1.423322e+07\n1.563165e+07\n1.566967e+07\n-426419.189046\n-19380.533078\n52228.256891\n\n\n2000-01-02\n2.021111e+06\n1.561362e+07\n1.726576e+07\n1.719370e+07\n-418338.557148\n-908.819733\n50277.625946\n\n\n2000-01-03\n2.112164e+06\n1.602304e+07\n1.776811e+07\n1.782520e+07\n-410257.925250\n-5097.266245\n48267.224770\n\n\n2000-01-04\n2.151723e+06\n1.624324e+07\n1.805897e+07\n1.800843e+07\n-402177.293353\n19976.279051\n46204.710114\n\n\n2000-01-05\n2.166407e+06\n1.627997e+07\n1.809866e+07\n1.808857e+07\n-394096.661455\n2280.652824\n44097.765648\n\n\n\n\n\n\n\nIn a real use-casee, you would not have access to the ground truth of the components. We use them here to show how the model behaves, and how incorporing extra information can improve it.\n\nfig, axs = plt.subplots(4, 1, figsize=(8, 12), sharex=True)\nfor i, name in enumerate(\n    [\"trend\", \"yearly_seasonality\", \"ad_spend_search\", \"ad_spend_social_media\"]\n):\n    true_components[name].plot(ax=axs[i], label=\"True\", color=\"black\")\n    y_pred_components[name].plot(ax=axs[i], label=\"Estimated\")\n    axs[i].set_title(name)\n    axs[i].legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n1.2 Backtesting with Cross-Validation\nWe use rolling-window CV to assess out-of-sample accuracy using MAPE.\n🧠 Caution: Low error ≠ correct attribution. But high error often indicates a bad model.\n\nfrom sktime.split import ExpandingWindowSplitter\nfrom sktime.performance_metrics.forecasting import MeanAbsolutePercentageError\nfrom sktime.forecasting.model_evaluation import evaluate\n\nmetric = MeanAbsolutePercentageError()\ncv = ExpandingWindowSplitter(\n    initial_window=365 * 3, step_length=180, fh=list(range(1, 180))\n)\ncv_results = evaluate(\n    forecaster=baseline_model, y=y, X=X, cv=cv, scoring=metric, return_data=True\n)\ncv_results\n\n\n\n\n\n\n\n\ntest_MeanAbsolutePercentageError\nfit_time\npred_time\nlen_train_window\ncutoff\ny_train\ny_test\ny_pred\n\n\n\n\n0\n0.064381\n36.515280\n0.578369\n1095\n2002-12-30\n2000-01-01 1.636995e+07 2000-01-02 1.738...\n2002-12-31 1.325823e+07 2003-01-01 1.451...\n2002-12-31 1.135125e+07 2003-01-01 1.192...\n\n\n1\n0.052696\n15.986574\n0.505935\n1275\n2003-06-28\n2000-01-01 1.636995e+07 2000-01-02 1.738...\n2003-06-29 1.751748e+07 2003-06-30 1.725...\n2003-06-29 1.643335e+07 2003-06-30 1.674...\n\n\n2\n0.043550\n32.623263\n0.511573\n1455\n2003-12-25\n2000-01-01 1.636995e+07 2000-01-02 1.738...\n2003-12-26 3.101882e+07 2003-12-27 3.088...\n2003-12-26 3.038854e+07 2003-12-27 3.079...\n\n\n3\n0.051377\n15.142913\n0.509556\n1635\n2004-06-22\n2000-01-01 1.636995e+07 2000-01-02 1.738...\n2004-06-23 3.534162e+07 2004-06-24 3.499...\n2004-06-23 3.677154e+07 2004-06-24 3.641...\n\n\n\n\n\n\n\nThe average error across folds is:\n\ncv_results[\"test_MeanAbsolutePercentageError\"].mean()\n\nnp.float64(0.053000841789052275)\n\n\nWe can visualize them by iterating the dataframe:\n\nfor idx, row in cv_results.iterrows():\n    plt.figure(figsize=(8, 2))\n    observed = pd.concat([row[\"y_train\"].iloc[-100:], row[\"y_test\"]])\n    observed.plot(label=\"Observed\", color=\"black\")\n    row[\"y_pred\"].plot(label=\"Prediction\")\n    plt.title(f\"Fold {idx + 1} – MAPE: {row['test_MeanAbsolutePercentageError']:.2%}\")\n    plt.legend()\n    plt.show()\n    if idx &gt; 3:\n        break\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1.3 Saturation Curves\nThese curves show diminishing marginal effect as spend increases.\n🔍 Insight: This shape helps guide budget allocation decisions (e.g. where additional spend will have little return).\nNote how the model captures a saturation effect, but it is still far from the correct shape.\nThis is why, in many situations, you will need calibration to correct the model’s behavior. This is what we will do in the next section.\n\nfig, axs = plt.subplots(figsize=(8, 6), nrows=1, ncols=2)\n\nfor ax, channel in zip(axs, [\"ad_spend_search\", \"ad_spend_social_media\"]):\n    ax.scatter(\n        X[channel],\n        y_pred_components[channel],\n        alpha=0.6,\n        label=channel,\n    )\n    ax.scatter(\n        X[channel],\n        true_components[channel],\n        color=\"black\",\n        label=\"True Effect\",\n    )\n    ax.set(\n        xlabel=\"Daily Spend\",\n        ylabel=\"Incremental Effect\",\n        title=f\"{channel} - Saturation Curve\",\n    )\n    ax.legend()\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Fitting, calibrating and Unified Marketing Measurement"
    ]
  },
  {
    "objectID": "mmm/fitting_and_calibration.html#part-2-calibration-with-causal-evidence",
    "href": "mmm/fitting_and_calibration.html#part-2-calibration-with-causal-evidence",
    "title": "Forecasting, Calibration, and Unified Marketing Measurement",
    "section": "Part 2: Calibration with Causal Evidence",
    "text": "Part 2: Calibration with Causal Evidence\nTime-series alone cannot disentangle correlated channels.\nWe integrate lift tests (local experiments) and attribution models (high-resolution signal) to correct this.\n\nlift_test_search\n\n\n\n\n\n\n\n\nlift\nx_start\nx_end\n\n\n\n\n2000-09-04\n0.986209\n88991.849098\n88616.674279\n\n\n2003-07-17\n1.602655\n25147.476284\n30137.721832\n\n\n2004-04-11\n0.762057\n58323.761863\n46723.500000\n\n\n2003-01-06\n0.758514\n23857.995845\n21913.505457\n\n\n2003-03-07\n1.332419\n29036.114641\n31676.191662\n\n\n2001-01-17\n1.142551\n71184.337297\n80915.848594\n\n\n2003-04-12\n1.654931\n36588.754410\n46777.071912\n\n\n2002-02-16\n0.825554\n67892.914951\n55030.729840\n\n\n2001-10-05\n0.929293\n69504.770296\n65128.356512\n\n\n2000-10-02\n0.994120\n85176.526599\n88936.541885\n\n\n2003-06-05\n0.760863\n27988.449143\n24654.281482\n\n\n2000-07-07\n0.897929\n97458.154049\n79856.919150\n\n\n2003-09-28\n1.570214\n42328.076114\n54497.175586\n\n\n2002-08-24\n0.863911\n35824.789963\n33256.068138\n\n\n2000-11-28\n1.242113\n69593.024187\n92943.044427\n\n\n2001-12-28\n0.881569\n71500.973097\n69504.085790\n\n\n2001-09-02\n1.197270\n69966.800254\n93695.953636\n\n\n2002-12-16\n1.660649\n7810.413156\n9322.237192\n\n\n2004-08-30\n1.286178\n47599.593625\n56735.271957\n\n\n2001-02-18\n1.254430\n64837.275444\n81784.074961\n\n\n2000-05-15\n1.044025\n100464.794301\n121779.610581\n\n\n2004-08-13\n0.967493\n57864.200257\n56080.324427\n\n\n2001-02-02\n1.212479\n67857.642170\n88071.282712\n\n\n2001-05-21\n1.178636\n71337.927880\n93445.016948\n\n\n2003-05-06\n0.644814\n25609.287364\n21843.082411\n\n\n2000-07-31\n1.012828\n92617.293678\n85766.681515\n\n\n2002-07-03\n1.267595\n57443.596207\n66595.562035\n\n\n2004-09-27\n1.369890\n41546.443755\n50176.823146\n\n\n2000-10-25\n0.985062\n72757.343228\n70195.986606\n\n\n2002-11-02\n0.834577\n19043.272758\n18373.397899\n\n\n\n\n\n\n\n\n2.1 Visualizing Lift Tests\nEach experiment records: pre-spend (x_start), post-spend (x_end), and measured lift. These give us causal “ground truth” deltas.\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\n# Scatter plot for pre-spend and observed lift\nax.scatter(lift_test_search[\"x_start\"], [1] * len(lift_test_search), label=\"Pre-Spend\", alpha=0.6)\nax.scatter(lift_test_search[\"x_end\"], lift_test_search[\"lift\"], label=\"Observed Lift\", alpha=0.6)\n\n# Annotate with arrows to show lift effect\nfor _, row in lift_test_search.iterrows():\n    ax.annotate(\n        \"\",\n        xy=(row[\"x_end\"], row[\"lift\"]),\n        xytext=(row[\"x_start\"], 1),\n        arrowprops=dict(arrowstyle=\"-&gt;\", alpha=0.5),\n    )\n\n# Add horizontal line and labels\nax.axhline(1, linestyle=\"--\", color=\"gray\", alpha=0.7)\nax.set(\n    title=\"Search Ads Lift Tests\",\n    xlabel=\"Spend\",\n    ylabel=\"Revenue Ratio\",\n)\n\n# Add legend and finalize layout\nax.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n2.2 Improve Estimates via LiftExperimentLikelihood\nThis adds a new likelihood term that makes the model match lift observations.\n🔁 Still Bayesian: It incorporates test variance and model uncertainty.\nSince we use sktime interface, we have access to get_params() and set_params(**kwargs) methods. This allows us to easily swap effects and likelihoods. When we define our model, the effect’s name become a key in the model’s get_params() dictionary. We can use this to set the effect’s parameters directly.\n\nfrom prophetverse.effects.lift_likelihood import LiftExperimentLikelihood\n\nmodel_lift = baseline_model.clone()\nmodel_lift.set_params(\n    ad_spend_search=LiftExperimentLikelihood(\n        effect=baseline_model.get_params()[\"ad_spend_search\"],\n        lift_test_results=lift_test_search,\n        prior_scale=0.05,\n    ),\n    ad_spend_social_media=LiftExperimentLikelihood(\n        effect=baseline_model.get_params()[\"ad_spend_social_media\"],\n        lift_test_results=lift_test_social,\n        prior_scale=0.05,\n    ),\n)\n\nmodel_lift.fit(y=y, X=X)\n\nProphetverse(exogenous_effects=[('yearly_seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[5],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[365.25]),\n                                 None),\n                                ('weekly_seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3],\n                                                          freq='D',\n                                                          prior_scale=0.05,\n                                                          sp_list=[7]),\n                                 None),\n                                ('ad_spend_search',\n                                 LiftExp...\n2002-07-03  1.053316   25537.869750   38025.254409\n2004-09-27  1.010456    8523.749415   11037.466255\n2000-10-25  0.980559   50964.957119   65038.733878\n2002-11-02  1.355164    1195.765141    1603.253926,\n                                                          prior_scale=0.05),\n                                 'ad_spend_social_media')],\n             inference_engine=MAPInferenceEngine(num_steps=5000,\n                                                 optimizer=LBFGSSolver(max_linesearch_steps=300,\n                                                                       memory_size=300)),\n             trend=PiecewiseLinearTrend(changepoint_interval=100))Please rerun this cell to show the HTML repr or trust the notebook.ProphetverseProphetverse(exogenous_effects=[('yearly_seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[5],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[365.25]),\n                                 None),\n                                ('weekly_seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3],\n                                                          freq='D',\n                                                          prior_scale=0.05,\n                                                          sp_list=[7]),\n                                 None),\n                                ('ad_spend_search',\n                                 LiftExp...\n2002-07-03  1.053316   25537.869750   38025.254409\n2004-09-27  1.010456    8523.749415   11037.466255\n2000-10-25  0.980559   50964.957119   65038.733878\n2002-11-02  1.355164    1195.765141    1603.253926,\n                                                          prior_scale=0.05),\n                                 'ad_spend_social_media')],\n             inference_engine=MAPInferenceEngine(num_steps=5000,\n                                                 optimizer=LBFGSSolver(max_linesearch_steps=300,\n                                                                       memory_size=300)),\n             trend=PiecewiseLinearTrend(changepoint_interval=100))effectsPiecewiseLinearTrendPiecewiseLinearTrend(changepoint_interval=100)LinearFourierSeasonalityLinearFourierSeasonality(effect_mode='multiplicative', fourier_terms_list=[5],\n                         freq='D', prior_scale=0.1, sp_list=[365.25])LinearFourierSeasonalityLinearFourierSeasonality(effect_mode='multiplicative', fourier_terms_list=[3],\n                         freq='D', prior_scale=0.05, sp_list=[7])ad_spend_searchLiftExperimentLikelihood(effect=ChainedEffects(steps=[('adstock',\n                                                       GeometricAdstockEffect()),\n                                                      ('saturation',\n                                                       HillEffect(effect_mode='additive',\n                                                                  half_max_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x33bbe1190 with batch shape () and event shape ()&gt;,\n                                                                  input_scale=1000000.0,\n                                                                  max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x106ca3a...\n2001-02-02  1.212479   67857.642170   88071.282712\n2001-05-21  1.178636   71337.927880   93445.016948\n2003-05-06  0.644814   25609.287364   21843.082411\n2000-07-31  1.012828   92617.293678   85766.681515\n2002-07-03  1.267595   57443.596207   66595.562035\n2004-09-27  1.369890   41546.443755   50176.823146\n2000-10-25  0.985062   72757.343228   70195.986606\n2002-11-02  0.834577   19043.272758   18373.397899,\n                         prior_scale=0.05)effect: ChainedEffectsChainedEffects(steps=[('adstock', GeometricAdstockEffect()),\n                      ('saturation',\n                       HillEffect(effect_mode='additive',\n                                  half_max_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x33bbe1190 with batch shape () and event shape ()&gt;,\n                                  input_scale=1000000.0,\n                                  max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x106ca3a90 with batch shape () and event shape ()&gt;,\n                                  slope_prior=&lt;numpyro.distributions.continuous.InverseGamma object at 0x320cc1850 with batch shape () and event shape ()&gt;))])GeometricAdstockEffectGeometricAdstockEffect()HillEffectHillEffect(effect_mode='additive',\n           half_max_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x33bbe1190 with batch shape () and event shape ()&gt;,\n           input_scale=1000000.0,\n           max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x106ca3a90 with batch shape () and event shape ()&gt;,\n           slope_prior=&lt;numpyro.distributions.continuous.InverseGamma object at 0x320cc1850 with batch shape () and event shape ()&gt;)ad_spend_social_mediaLiftExperimentLikelihood(effect=ChainedEffects(steps=[('adstock',\n                                                       GeometricAdstockEffect()),\n                                                      ('saturation',\n                                                       HillEffect(effect_mode='additive',\n                                                                  half_max_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x33bbe1190 with batch shape () and event shape ()&gt;,\n                                                                  input_scale=1000000.0,\n                                                                  max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x106ca3a...\n2001-02-02  1.085461   38129.336619   47626.107330\n2001-05-21  0.961702   50010.574434   73721.957893\n2003-05-06  1.099837    2893.044038    2702.425237\n2000-07-31  1.028477  105196.357631  125200.174092\n2002-07-03  1.053316   25537.869750   38025.254409\n2004-09-27  1.010456    8523.749415   11037.466255\n2000-10-25  0.980559   50964.957119   65038.733878\n2002-11-02  1.355164    1195.765141    1603.253926,\n                         prior_scale=0.05)effect: ChainedEffectsChainedEffects(steps=[('adstock', GeometricAdstockEffect()),\n                      ('saturation',\n                       HillEffect(effect_mode='additive',\n                                  half_max_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x33bbe1190 with batch shape () and event shape ()&gt;,\n                                  input_scale=1000000.0,\n                                  max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x106ca3a90 with batch shape () and event shape ()&gt;,\n                                  slope_prior=&lt;numpyro.distributions.continuous.InverseGamma object at 0x320cc1850 with batch shape () and event shape ()&gt;))])GeometricAdstockEffectGeometricAdstockEffect()HillEffectHillEffect(effect_mode='additive',\n           half_max_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x33bbe1190 with batch shape () and event shape ()&gt;,\n           input_scale=1000000.0,\n           max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x106ca3a90 with batch shape () and event shape ()&gt;,\n           slope_prior=&lt;numpyro.distributions.continuous.InverseGamma object at 0x320cc1850 with batch shape () and event shape ()&gt;)inference_engineMAPInferenceEngineMAPInferenceEngine(num_steps=5000,\n                   optimizer=LBFGSSolver(max_linesearch_steps=300,\n                                         memory_size=300))\n\n\n\ncomponents_lift = model_lift.predict_components(X=X, fh=X.index)\ncomponents_lift.head()\n\n\n\n\n\n\n\n\nad_spend_search\nad_spend_social_media\nmean\nobs\ntrend\nweekly_seasonality\nyearly_seasonality\n\n\n\n\n2000-01-01\n6.614968e+06\n9.251862e+06\n1.649529e+07\n1.653332e+07\n676808.848392\n28409.989940\n-76757.145418\n\n\n2000-01-02\n7.420463e+06\n9.617179e+06\n1.764898e+07\n1.757692e+07\n686712.039752\n1211.884003\n-76584.079411\n\n\n2000-01-03\n7.560165e+06\n9.706066e+06\n1.789518e+07\n1.795228e+07\n696615.231112\n8548.595991\n-76213.556534\n\n\n2000-01-04\n7.587806e+06\n9.752298e+06\n1.793803e+07\n1.788749e+07\n706518.422472\n-32953.299498\n-75640.951096\n\n\n2000-01-05\n7.562759e+06\n9.753211e+06\n1.795322e+07\n1.794313e+07\n716421.613833\n-4312.267153\n-74862.125627\n\n\n\n\n\n\n\n\nfig, axs = plt.subplots(figsize=(8, 6), ncols=2)\n\nfor ax, channel in zip(axs, [\"ad_spend_search\", \"ad_spend_social_media\"]):\n    ax.scatter(\n        X[channel],\n        y_pred_components[channel],\n        label=\"Baseline\",\n        alpha=0.6,\n        s=50,\n    )\n    ax.scatter(\n        X[channel],\n        components_lift[channel],\n        label=\"With Lift Test\",\n        alpha=0.6,\n        s=50,\n    )\n    ax.plot(\n        X[channel],\n        true_components[channel],\n        label=\"True\",\n        color=\"black\",\n        linewidth=2,\n    )\n    ax.set(\n        title=f\"{channel} Predicted Effects\",\n        xlabel=\"Daily Spend\",\n        ylabel=\"Incremental Effect\",\n    )\n    ax.axhline(0, linestyle=\"--\", color=\"gray\", alpha=0.7)\n    ax.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nMuch better, right? And it was implemented with a really modular and flexible code. You could wrap any effect with LiftExperimentLikelihood to add lift test data to guide its behaviour. Nevertheless, this is not the end of the story.\n\n\n2.3 Add Attribution Signals with ExactLikelihood\nAttribution models can provide daily signals. If available, you can incorporate them by adding another likelihood term via ExactLikelihood.\nWe create a synthetic attribution signal by multiplying the true effect with a random noise factor.\n\nfrom prophetverse.effects import ExactLikelihood\n\nrng = np.random.default_rng(42)\n\n# Generate attribution signals for search and social media channels\nattr_search = true_components[[\"ad_spend_search\"]] * rng.normal(\n    1, 0.1, size=(len(y), 1)\n)\nattr_social = true_components[[\"ad_spend_social_media\"]] * rng.normal(\n    1, 0.1, size=(len(y), 1)\n)\n\n# Display the first few rows of the social media attribution signal\nattr_social.head()\n\n\n\n\n\n\n\n\nad_spend_social_media\n\n\n\n\n2000-01-01\n7.239655e+06\n\n\n2000-01-02\n8.750496e+06\n\n\n2000-01-03\n9.943655e+06\n\n\n2000-01-04\n9.456071e+06\n\n\n2000-01-05\n8.401297e+06\n\n\n\n\n\n\n\n\nmodel_umm = model_lift.clone()\nmodel_umm.set_params(\n    exogenous_effects=model_lift.get_params()[\"exogenous_effects\"]\n    + [\n        (\n            \"attribution_search\",\n            ExactLikelihood(\"ad_spend_search\", attr_search, 0.01),\n            None,\n        ),\n        (\n            \"attribution_social_media\",\n            ExactLikelihood(\"ad_spend_social_media\", attr_social, 0.01),\n            None,\n        ),\n    ]\n)\nmodel_umm.fit(y=y, X=X)\n\nProphetverse(exogenous_effects=[('yearly_seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[5],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[365.25]),\n                                 None),\n                                ('weekly_seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3],\n                                                          freq='D',\n                                                          prior_scale=0.05,\n                                                          sp_list=[7]),\n                                 None),\n                                ('ad_spend_search',\n                                 LiftExp...\n2000-01-04           9.456071e+06\n2000-01-05           8.401297e+06\n...                           ...\n2004-12-28           3.374429e+06\n2004-12-29           3.695968e+06\n2004-12-30           3.569596e+06\n2004-12-31           3.425932e+06\n2005-01-01           3.373882e+06\n\n[1828 rows x 1 columns]),\n                                 None)],\n             inference_engine=MAPInferenceEngine(num_steps=5000,\n                                                 optimizer=LBFGSSolver(max_linesearch_steps=300,\n                                                                       memory_size=300)),\n             trend=PiecewiseLinearTrend(changepoint_interval=100))Please rerun this cell to show the HTML repr or trust the notebook.ProphetverseProphetverse(exogenous_effects=[('yearly_seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[5],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[365.25]),\n                                 None),\n                                ('weekly_seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3],\n                                                          freq='D',\n                                                          prior_scale=0.05,\n                                                          sp_list=[7]),\n                                 None),\n                                ('ad_spend_search',\n                                 LiftExp...\n2000-01-04           9.456071e+06\n2000-01-05           8.401297e+06\n...                           ...\n2004-12-28           3.374429e+06\n2004-12-29           3.695968e+06\n2004-12-30           3.569596e+06\n2004-12-31           3.425932e+06\n2005-01-01           3.373882e+06\n\n[1828 rows x 1 columns]),\n                                 None)],\n             inference_engine=MAPInferenceEngine(num_steps=5000,\n                                                 optimizer=LBFGSSolver(max_linesearch_steps=300,\n                                                                       memory_size=300)),\n             trend=PiecewiseLinearTrend(changepoint_interval=100))effectsPiecewiseLinearTrendPiecewiseLinearTrend(changepoint_interval=100)LinearFourierSeasonalityLinearFourierSeasonality(effect_mode='multiplicative', fourier_terms_list=[5],\n                         freq='D', prior_scale=0.1, sp_list=[365.25])LinearFourierSeasonalityLinearFourierSeasonality(effect_mode='multiplicative', fourier_terms_list=[3],\n                         freq='D', prior_scale=0.05, sp_list=[7])ad_spend_searchLiftExperimentLikelihood(effect=ChainedEffects(steps=[('adstock',\n                                                       GeometricAdstockEffect()),\n                                                      ('saturation',\n                                                       HillEffect(effect_mode='additive',\n                                                                  half_max_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x33bbe1190 with batch shape () and event shape ()&gt;,\n                                                                  input_scale=1000000.0,\n                                                                  max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x106ca3a...\n2001-02-02  1.212479   67857.642170   88071.282712\n2001-05-21  1.178636   71337.927880   93445.016948\n2003-05-06  0.644814   25609.287364   21843.082411\n2000-07-31  1.012828   92617.293678   85766.681515\n2002-07-03  1.267595   57443.596207   66595.562035\n2004-09-27  1.369890   41546.443755   50176.823146\n2000-10-25  0.985062   72757.343228   70195.986606\n2002-11-02  0.834577   19043.272758   18373.397899,\n                         prior_scale=0.05)effect: ChainedEffectsChainedEffects(steps=[('adstock', GeometricAdstockEffect()),\n                      ('saturation',\n                       HillEffect(effect_mode='additive',\n                                  half_max_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x33bbe1190 with batch shape () and event shape ()&gt;,\n                                  input_scale=1000000.0,\n                                  max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x106ca3a90 with batch shape () and event shape ()&gt;,\n                                  slope_prior=&lt;numpyro.distributions.continuous.InverseGamma object at 0x320cc1850 with batch shape () and event shape ()&gt;))])GeometricAdstockEffectGeometricAdstockEffect()HillEffectHillEffect(effect_mode='additive',\n           half_max_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x33bbe1190 with batch shape () and event shape ()&gt;,\n           input_scale=1000000.0,\n           max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x106ca3a90 with batch shape () and event shape ()&gt;,\n           slope_prior=&lt;numpyro.distributions.continuous.InverseGamma object at 0x320cc1850 with batch shape () and event shape ()&gt;)ad_spend_social_mediaLiftExperimentLikelihood(effect=ChainedEffects(steps=[('adstock',\n                                                       GeometricAdstockEffect()),\n                                                      ('saturation',\n                                                       HillEffect(effect_mode='additive',\n                                                                  half_max_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x33bbe1190 with batch shape () and event shape ()&gt;,\n                                                                  input_scale=1000000.0,\n                                                                  max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x106ca3a...\n2001-02-02  1.085461   38129.336619   47626.107330\n2001-05-21  0.961702   50010.574434   73721.957893\n2003-05-06  1.099837    2893.044038    2702.425237\n2000-07-31  1.028477  105196.357631  125200.174092\n2002-07-03  1.053316   25537.869750   38025.254409\n2004-09-27  1.010456    8523.749415   11037.466255\n2000-10-25  0.980559   50964.957119   65038.733878\n2002-11-02  1.355164    1195.765141    1603.253926,\n                         prior_scale=0.05)effect: ChainedEffectsChainedEffects(steps=[('adstock', GeometricAdstockEffect()),\n                      ('saturation',\n                       HillEffect(effect_mode='additive',\n                                  half_max_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x33bbe1190 with batch shape () and event shape ()&gt;,\n                                  input_scale=1000000.0,\n                                  max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x106ca3a90 with batch shape () and event shape ()&gt;,\n                                  slope_prior=&lt;numpyro.distributions.continuous.InverseGamma object at 0x320cc1850 with batch shape () and event shape ()&gt;))])GeometricAdstockEffectGeometricAdstockEffect()HillEffectHillEffect(effect_mode='additive',\n           half_max_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x33bbe1190 with batch shape () and event shape ()&gt;,\n           input_scale=1000000.0,\n           max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x106ca3a90 with batch shape () and event shape ()&gt;,\n           slope_prior=&lt;numpyro.distributions.continuous.InverseGamma object at 0x320cc1850 with batch shape () and event shape ()&gt;)ExactLikelihoodExactLikelihood(effect_name='ad_spend_search', prior_scale=0.01,\n                reference_df=            ad_spend_search\n2000-01-01     8.682649e+06\n2000-01-02     7.542228e+06\n2000-01-03     9.091821e+06\n2000-01-04     9.259557e+06\n2000-01-05     6.785853e+06\n...                     ...\n2004-12-28     3.547301e+06\n2004-12-29     2.771639e+06\n2004-12-30     2.972279e+06\n2004-12-31     3.226681e+06\n2005-01-01     3.605501e+06\n\n[1828 rows x 1 columns])ExactLikelihoodExactLikelihood(effect_name='ad_spend_social_media', prior_scale=0.01,\n                reference_df=            ad_spend_social_media\n2000-01-01           7.239655e+06\n2000-01-02           8.750496e+06\n2000-01-03           9.943655e+06\n2000-01-04           9.456071e+06\n2000-01-05           8.401297e+06\n...                           ...\n2004-12-28           3.374429e+06\n2004-12-29           3.695968e+06\n2004-12-30           3.569596e+06\n2004-12-31           3.425932e+06\n2005-01-01           3.373882e+06\n\n[1828 rows x 1 columns])inference_engineMAPInferenceEngineMAPInferenceEngine(num_steps=5000,\n                   optimizer=LBFGSSolver(max_linesearch_steps=300,\n                                         memory_size=300))\n\n\n\ncomponents_umm = model_umm.predict_components(X=X, fh=X.index)\n\nfig, axs = plt.subplots(2, 1, figsize=(8, 10), sharex=True)\nfor ax, channel in zip(axs, [\"ad_spend_search\", \"ad_spend_social_media\"]):\n    ax.scatter(X[channel], y_pred_components[channel], label=\"Baseline\", alpha=0.4)\n    ax.scatter(X[channel], components_lift[channel], label=\"With Lift Test\", alpha=0.4)\n    ax.scatter(X[channel], components_umm[channel], label=\"With Attribution\", alpha=0.4)\n    ax.plot(X[channel], true_components[channel], label=\"True Effect\", color=\"black\")\n    ax.set_title(channel)\n    ax.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nEven better! And, due to sktime-like interface, wrapping and adding new effects is easy.",
    "crumbs": [
      "Fitting, calibrating and Unified Marketing Measurement"
    ]
  },
  {
    "objectID": "mmm/fitting_and_calibration.html#final-thoughts-toward-unified-marketing-measurement",
    "href": "mmm/fitting_and_calibration.html#final-thoughts-toward-unified-marketing-measurement",
    "title": "Forecasting, Calibration, and Unified Marketing Measurement",
    "section": "Final Thoughts: Toward Unified Marketing Measurement",
    "text": "Final Thoughts: Toward Unified Marketing Measurement\n✅ What we learned:\n1. Adstock + saturation are essential to capture media dynamics.\n2. Good predictions ≠ good attribution.\n3. Causal data like lift tests can correct misattribution.\n4. Attribution signals add further constraints.\n🛠️ Use this when:\n* Channels are correlated → Use lift tests.\n* You have granular model output → Add attribution likelihoods.\n🧪 Model selection tip:\nAlways validate causal logic, not just fit quality.\nWith Prophetverse, you can combine observational, experimental, and model-based signals into one coherent MMM+UMM pipeline.",
    "crumbs": [
      "Fitting, calibrating and Unified Marketing Measurement"
    ]
  },
  {
    "objectID": "mmm/budget_allocation.html",
    "href": "mmm/budget_allocation.html",
    "title": "Budget Optimization",
    "section": "",
    "text": "In this tutorial, you’ll learn how to use Prophetverse’s budget-optimization module to:\nYou’ll also see how to switch between two parametrizations without hassle:\nBy the end, you’ll know how to pick the right setup for your campaign goals and make adjustments in seconds.",
    "crumbs": [
      "Budget Optimization"
    ]
  },
  {
    "objectID": "mmm/budget_allocation.html#setting-up-the-problem",
    "href": "mmm/budget_allocation.html#setting-up-the-problem",
    "title": "Budget Optimization",
    "section": "1. Setting Up the Problem",
    "text": "1. Setting Up the Problem\nIn this example, we will load the same dataset as in the previous tutorial, and use the complete model, including the calibration with lift tests and attribution models.\n\nimport numpyro\nnumpyro.enable_x64()\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n\n1.1 Load synthetic data\n\nfrom prophetverse.datasets._mmm.dataset1 import get_dataset\n\ny, X, lift_tests, true_components, model = get_dataset()\n\n\n\n1.2 Utility plotting functions\n\ndef plot_spend_comparison(\n    X_baseline,\n    X_optimized,\n    channels,\n    indexer,\n    *,\n    baseline_title=\"Baseline Spend\",\n    optimized_title=\"Optimized Spend\",\n    figsize=(8, 6),\n):\n    fig, ax = plt.subplots(1, 2, figsize=figsize)\n    X_baseline.loc[indexer, channels].plot(ax=ax[0], title=baseline_title)\n    X_optimized.loc[indexer, channels].plot(ax=ax[1], title=optimized_title)\n    for a in ax:\n        a.set_ylabel(\"Spend\")\n        a.set_xlabel(\"Date\")\n    plt.tight_layout()\n    return fig, ax",
    "crumbs": [
      "Budget Optimization"
    ]
  },
  {
    "objectID": "mmm/budget_allocation.html#budget-optimization",
    "href": "mmm/budget_allocation.html#budget-optimization",
    "title": "Budget Optimization",
    "section": "2. Budget Optimization",
    "text": "2. Budget Optimization\nOnce we have our model to predict the KPI, we can use it to optimize our budget-allocation strategy.\nYou can use this feature to provide not only a descriptive analysis of the channels’ contributions, but also a prescriptive analysis of the budget allocation.\nThe budget-optimization module is designed to be flexible and extensible. It is composed of three main components:\n\nThe objective function – what you want to optimize.\nThe constraints – e.g. budget caps, channel-share rules, or any custom rule.\nThe parametrization transform – optionally, how you want to parameterize the problem.\n\nFor example, instead of optimizing the daily spend for each channel, you can optimize the share of budget for each channel while keeping the overall spending pattern fixed.\n\n2.1 Maximizing a KPI\nThe BudgetOptimizer class is the main entry point for the budget-optimization module. It takes these three components as input and uses them to optimize the budget allocation.\n\nfrom prophetverse.experimental.budget_optimization import (\n    BudgetOptimizer,\n    SharedBudgetConstraint,\n    MaximizeKPI,\n)\n\nbudget_optimizer = BudgetOptimizer(\n    objective=MaximizeKPI(),\n    constraints=[SharedBudgetConstraint()],\n)\nbudget_optimizer\n\nBudgetOptimizer(constraints=[SharedBudgetConstraint()], objective=MaximizeKPI())Please rerun this cell to show the HTML repr or trust the notebook.BudgetOptimizerBudgetOptimizer(constraints=[SharedBudgetConstraint()], objective=MaximizeKPI())MaximizeKPIMaximizeKPI()\n\n\nThis is our optimization horizon:\n\nhorizon = pd.period_range(\"2004-01-01\", \"2004-12-31\", freq=\"D\")\nhorizon\n\nPeriodIndex(['2004-01-01', '2004-01-02', '2004-01-03', '2004-01-04',\n             '2004-01-05', '2004-01-06', '2004-01-07', '2004-01-08',\n             '2004-01-09', '2004-01-10',\n             ...\n             '2004-12-22', '2004-12-23', '2004-12-24', '2004-12-25',\n             '2004-12-26', '2004-12-27', '2004-12-28', '2004-12-29',\n             '2004-12-30', '2004-12-31'],\n            dtype='period[D]', length=366)\n\n\nBy default, BudgetOptimizer will optimize the daily spend for each channel. Let’s see it in action.\n\nX_opt = budget_optimizer.optimize(\n    model=model,\n    X=X,\n    horizon=horizon,\n    columns=[\"ad_spend_search\", \"ad_spend_social_media\"],\n)\n\n\nBaseline vs. optimized spend\n\nplot_spend_comparison(\n    X,\n    X_opt,\n    [\"ad_spend_search\", \"ad_spend_social_media\"],\n    \"2004\",\n)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nPredictions comparison\n\ny_pred_baseline = model.predict(X=X, fh=horizon)\ny_pred_opt = model.predict(X=X_opt, fh=horizon)\n\nprint(\n    f\"MMM Predictions - Optimization KPI gain: +{y_pred_opt.sum()/y_pred_baseline.sum()-1:.2%}\"\n)\n\nMMM Predictions - Optimization KPI gain: +3.57%\n\n\n\n\n\n2.2. Reparametrization: Optimizing channel share\nInstead of optimizing daily spend, we can optimize the share of budget for each channel. This is useful when you want to keep the spending pattern fixed (e.g. seasonal bursts) and usually converges faster because fewer parameters are free.\n\nfrom prophetverse.experimental.budget_optimization import InvestmentPerChannelTransform\n\nbudget_optimizer = BudgetOptimizer(\n    objective=MaximizeKPI(),\n    constraints=[SharedBudgetConstraint()],\n    parametrization_transform=InvestmentPerChannelTransform(),\n    options={\"disp\": True},\n)\n\n\nX_opt = budget_optimizer.optimize(\n    model=model,\n    X=X,\n    horizon=horizon,\n    columns=[\"ad_spend_search\", \"ad_spend_social_media\"],\n)\n\nOptimization terminated successfully    (Exit mode 0)\n            Current function value: -11675234227.278275\n            Iterations: 12\n            Function evaluations: 12\n            Gradient evaluations: 12\n\n\n\nBaseline vs. optimized spend\n\nplot_spend_comparison(\n    X,\n    X_opt,\n    [\"ad_spend_search\", \"ad_spend_social_media\"],\n    horizon,\n)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nPredictions comparison\n\ny_pred_baseline = model.predict(X=X, fh=horizon)\ny_pred_opt = model.predict(X=X_opt, fh=horizon)\n\nprint(\n    f\"MMM Predictions - Optimization KPI gain: +{y_pred_opt.sum()/y_pred_baseline.sum()-1:.2%}\"\n)\n\nMMM Predictions - Optimization KPI gain: +0.05%\n\n\n\n\n\n2.3. Minimizing budget to reach a target\nHow much should we invest to reach a 30 % increase in 2004?\n\nfrom prophetverse.experimental.budget_optimization import (\n    MinimizeBudget,\n    MinimumTargetResponse,\n)\n\ntarget = y.loc[\"2003\"].sum() * 1.30\n\nbudget_optimizer = BudgetOptimizer(\n    objective=MinimizeBudget(),\n    constraints=[MinimumTargetResponse(target_response=target, constraint_type=\"eq\")],\n    options={\"disp\": True, \"maxiter\" : 300},\n)\n\nX0 = X.copy()\nX_opt = budget_optimizer.optimize(\n    model=model,\n    X=X0,\n    horizon=horizon,\n    columns=[\"ad_spend_search\", \"ad_spend_social_media\"],\n)\n\nIteration limit reached    (Exit mode 9)\n            Current function value: 12676761.108709542\n            Iterations: 300\n            Function evaluations: 306\n            Gradient evaluations: 300\n\n\n\nBudget comparison\n\nplot_spend_comparison(\n    X0,\n    X_opt,\n    [\"ad_spend_search\", \"ad_spend_social_media\"],\n    indexer=horizon,\n)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nPredictions comparison\n\ny_pred_baseline = model.predict(X=X0, fh=horizon)\ny_pred_opt = model.predict(X=X_opt, fh=horizon)\n\n\nprint(\n    f\"MMM Predictions \\n\",\n    f\"Baseline KPI: {y_pred_baseline.sum()/1e9:.2f} B \\n\",\n    f\"Optimized KPI: {y_pred_opt.sum()/1e9:.2f} B \\n\",\n    f\"Target KPI: {target/1e9:.2f} B \\n\",\n    \"Baseline spend: \",\n    X0.loc[horizon, [\"ad_spend_search\", \"ad_spend_social_media\"]].sum().sum(),\n    \"\\n\",\n    \"Optimized spend: \",\n    X_opt.loc[horizon, [\"ad_spend_search\", \"ad_spend_social_media\"]].sum().sum(),\n    \"\\n\",\n)\n\nMMM Predictions \n Baseline KPI: 11.67 B \n Optimized KPI: 9.96 B \n Target KPI: 9.96 B \n Baseline spend:  29349153.831201974 \n Optimized spend:  12676761.108709542 \n\n\n\n\n\n\n2.4. Reparametrization: optimizing channel share\nHere we minimize budget again but let the optimizer search only for each channel’s share of the total investment instead of day-by-day dollar amounts.\n\nbudget_optimizer = BudgetOptimizer(\n    objective=MinimizeBudget(),\n    constraints=[MinimumTargetResponse(target_response=target, constraint_type=\"eq\")],\n    parametrization_transform=InvestmentPerChannelTransform(),\n    bounds={\"ad_spend_search\": (0, 2e9), \"ad_spend_social_media\": (0, 2e9)},\n    options={\"disp\": True, \"maxiter\": 300},\n)\n\nX_opt = budget_optimizer.optimize(\n    model=model,\n    X=X0,\n    horizon=horizon,\n    columns=[\"ad_spend_search\", \"ad_spend_social_media\"],\n)\n\nOptimization terminated successfully    (Exit mode 0)\n            Current function value: 12205937.565087624\n            Iterations: 39\n            Function evaluations: 57\n            Gradient evaluations: 38\n\n\n\nBudget comparison\n\nplot_spend_comparison(\n    X0,\n    X_opt,\n    [\"ad_spend_search\", \"ad_spend_social_media\"],\n    horizon,\n)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nPredictions comparison\n\ny_pred_baseline = model.predict(X=X0, fh=horizon)\ny_pred_opt = model.predict(X=X_opt, fh=horizon)\n\nprint(\n    f\"MMM Predictions \\n\",\n    f\"Baseline KPI: {y_pred_baseline.sum()/1e9:.2f} B \\n\",\n    f\"Optimized KPI: {y_pred_opt.sum()/1e9:.2f} B \\n\",\n    f\"Target KPI: {target/1e9:.2f} B \\n\",\n\n    \"Baseline spend: \",\n    X0.loc[horizon, [\"ad_spend_search\", \"ad_spend_social_media\"]].sum().sum(),\n    \"\\n\",\n    \"Optimized spend: \",\n    X_opt.loc[horizon, [\"ad_spend_search\", \"ad_spend_social_media\"]].sum().sum(),\n    \"\\n\",\n)\nplt.show()\n\nMMM Predictions \n Baseline KPI: 11.67 B \n Optimized KPI: 9.96 B \n Target KPI: 9.96 B \n Baseline spend:  29349153.831201974 \n Optimized spend:  12205937.565087626",
    "crumbs": [
      "Budget Optimization"
    ]
  },
  {
    "objectID": "mmm/budget_allocation.html#conclusion",
    "href": "mmm/budget_allocation.html#conclusion",
    "title": "Budget Optimization",
    "section": "Conclusion",
    "text": "Conclusion\nWe have seen some of the capabilities of the budget-optimization module. There are three key components, besides the optimizer itself:\n\nThe objective function\nThe constraints\nThe parametrization transform\n\nYou can also create your own objective functions and constraints, and use them in the optimizer. Budget optimization lets you and your team bring the company closer to data-driven decisions.",
    "crumbs": [
      "Budget Optimization"
    ]
  },
  {
    "objectID": "tutorial/index.html",
    "href": "tutorial/index.html",
    "title": "Tutorial",
    "section": "",
    "text": "In this section, you can find a collection of tutorials that will help you understand the basics of Prophetverse for forecasting."
  },
  {
    "objectID": "index.html#the-flexible-bayesian-forecasting-and-marketing-mix-modeling-package",
    "href": "index.html#the-flexible-bayesian-forecasting-and-marketing-mix-modeling-package",
    "title": "Prophetverse",
    "section": "The flexible bayesian forecasting and Marketing Mix Modeling package",
    "text": "The flexible bayesian forecasting and Marketing Mix Modeling package\nProphetverse leverages the theory behind the Prophet model for time series forecasting and expands it into a more general framework, enabling custom priors, non-linear effects for exogenous variables and other likelihoods. Built on top of sktime and numpyro, Prophetverse aims to provide a flexible and easy-to-use library for time series forecasting with a focus on interpretability and customizability. It is particularly useful for Marketing Mix Modeling, where understanding the effect of different marketing channels on sales is crucial."
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Prophetverse",
    "section": "Getting started",
    "text": "Getting started\n\nInstallation\nTo install with pip:\npip install prophetverse\nOr with poetry:\npoetry add prophetverse\n\n\nForecasting with default values\nThe Prophetverse model provides an interface compatible with sktime. Here’s an example of how to use it:\nfrom prophetverse.sktime import Prophetverse\n\n# Create the model\nmodel = Prophetverse()\n\n# Fit the model\nmodel.fit(y=y, X=X)\n\n# Forecast in sample\ny_pred = model.predict(X=X, fh=y.index)"
  },
  {
    "objectID": "index.html#features",
    "href": "index.html#features",
    "title": "Prophetverse",
    "section": "Features",
    "text": "Features\nProphetverse is similar to the original Prophet model in many aspects, but it has some differences and new features. The following table summarizes the main features of Prophetverse and compares them with the original Prophet model:\n\n\n\n\n\n\n\n\n\nFeature\nProphetverse\nOriginal Prophet\nMotivation\n\n\n\n\nLogistic trend\nCapacity as a random variable\nCapacity as a hyperparameter, user input required\nThe capacity is usually unknown by the users. Having it as a variable is useful for Total Addressable Market inference\n\n\nCustom trend\nCustomizable trend functions\nNot available\nUsers can create custom trends and leverage their knowledge about the timeseries to enhance long-term accuracy\n\n\nLikelihoods\nGaussian, Gamma and Negative Binomial\nGaussian only\nGaussian likelihood fails to provide good forecasts to positive-only and count data (sales, for example)\n\n\nCustom priors\nSupports custom priors for model parameters and exogenous variables\nNot supported\nForcing positive coefficients, using prior knowledge to model the timeseries\n\n\nCustom exogenous effects\nNon-linear and customizable effects for exogenous variables, shared coefficients between time series\nNot available\nUsers can create any kind of relationship between exogenous variables and the timeseries, which can be useful for Marketing Mix Modeling and other applications.\n\n\nChangepoints\nUses changepoint interval\nUses changepoint number\nThe changepoint number is not stable in the sense that, when the size of timeseries increases, its impact on forecast changes. Think about setting a changepoint number when timeseries has 6 months, and forecasting in future with 2 years of data (4x time original size). Re-tuning would be required. Prophetverse is expected to be more stable\n\n\nScaling\nTime series scaled internally, exogenous variables scaled by the user\nTime series scaled internally\nScaling y is needed to enhance user experience with hyperparameters. On the other hand, not scaling the exogenous variables provide more control to the user and they can leverage sktime’s transformers to handle that.\n\n\nSeasonality\nFourier terms for seasonality passed as exogenous variables\nBuilt-in seasonality handling\nSetting up seasonality requires almost zero effort by using LinearFourierSeasonality in Prophetverse. The idea is to allow the user to create custom seasonalities easily, without hardcoding it in the code.\n\n\nMultivariate model\nHierarchical model with multivariate normal likelihood and LKJ prior, bottom-up forecast\nNot available\nHaving shared coefficients, using global information to enhance individual forecast.\n\n\nImplementation\nNumpyro\nStan"
  },
  {
    "objectID": "howto/index.html",
    "href": "howto/index.html",
    "title": "How-to",
    "section": "",
    "text": "In this documentation section, you will find how you can create more advanced patterns\n\n  \n    \n      \n        **Custom Time Series Component**\n        Learn how to craft a custom time series component and unlock powerful forecasting enhancements.\n        → Custom Effect\n      \n    \n  \n\n  \n    \n      \n        Custom Trend\n        Follow a hands-on example to create a custom trend component and integrate it seamlessly into your forecasts.\n        → Custom Trend\n      \n    \n  \n\n  \n    \n      \n        Composition of Time Series Components\n        Discover how to combine multiple time series components for more nuanced and accurate forecasting models.\n        → Composite Exogenous Effect"
  },
  {
    "objectID": "howto/custom_trend.html",
    "href": "howto/custom_trend.html",
    "title": "Custom Trend in Prophetverse",
    "section": "",
    "text": "Diffusion of innovations is a theory that seeks to explain how, why, and at what rate new ideas and technology spread through cultures. This theory was formulated by E.M. Rogers in 1962 and is often used to understand the adoption or spread of new products and technologies among different groups of people.\nAn innovation is something new or significantly improved. This can include products, ideas, or practices that are perceived as new by an individual or other unit of adoption. Diffusion refers to the process by which an innovation is communicated over time among the participants in a social system.\nThe diffusion of innovations theory applies to a variety of new ideas. Here are a few examples:",
    "crumbs": [
      "Custom trend"
    ]
  },
  {
    "objectID": "howto/custom_trend.html#use-case-forecasting-product-adoption",
    "href": "howto/custom_trend.html#use-case-forecasting-product-adoption",
    "title": "Custom Trend in Prophetverse",
    "section": "",
    "text": "Diffusion of innovations is a theory that seeks to explain how, why, and at what rate new ideas and technology spread through cultures. This theory was formulated by E.M. Rogers in 1962 and is often used to understand the adoption or spread of new products and technologies among different groups of people.\nAn innovation is something new or significantly improved. This can include products, ideas, or practices that are perceived as new by an individual or other unit of adoption. Diffusion refers to the process by which an innovation is communicated over time among the participants in a social system.\nThe diffusion of innovations theory applies to a variety of new ideas. Here are a few examples:",
    "crumbs": [
      "Custom trend"
    ]
  },
  {
    "objectID": "howto/custom_trend.html#examples-of-processes-following-diffusion-of-innovations",
    "href": "howto/custom_trend.html#examples-of-processes-following-diffusion-of-innovations",
    "title": "Custom Trend in Prophetverse",
    "section": "Examples of Processes Following Diffusion of Innovations",
    "text": "Examples of Processes Following Diffusion of Innovations\n\nNumber of new unique users: The number of new unique users of a product or service can be modeled using the diffusion of innovations theory. This can help businesses forecast their growth and plan for future expansion.\nTechnology Adoption: Perhaps the most common application of the theory, technology adoption refers to how new gadgets, software, or platforms spread among users. For instance, the adoption of smartphones followed this diffusion process, starting with innovators and tech enthusiasts before reaching the broader public.\nHealthcare Practices: New medical practices, treatments, or health campaigns spread among medical professionals and the public using the diffusion framework. An example could be the adoption of telemedicine, which has seen increased acceptance over recent years.\nSustainable Practices: The adoption of renewable energy sources like solar panels or wind turbines often follows the diffusion of innovations model. Innovators begin by testing and using these technologies, which gradually become more mainstream as their advantages and efficiencies are recognized.\nAgricultural Techniques: New farming technologies or methods, such as hydroponics or genetically modified crops, also spread through agricultural communities by following the principles of diffusion of innovations.",
    "crumbs": [
      "Custom trend"
    ]
  },
  {
    "objectID": "howto/custom_trend.html#the-bell-shaped-curve",
    "href": "howto/custom_trend.html#the-bell-shaped-curve",
    "title": "Custom Trend in Prophetverse",
    "section": "The Bell-Shaped Curve",
    "text": "The Bell-Shaped Curve\nThe diffusion of innovations can be visualized using a bell-shaped curve, often called the “diffusion curve.” This curve is crucial for understanding the rate at which new ideas and technologies are adopted in a society. Here’s how it aligns with the categories of adopters:\n\nInnovators make up the first small section on the left of the curve. These are the first few who adopt the innovation.\nEarly Adopters follow next and represent a slightly larger segment as the curve starts to ascend.\nEarly Majority forms the first large segment of the curve, where it reaches and crosses the mean. Adoption is becoming more common and widespread here.\nLate Majority comes next, at the point where the curve starts to descend. This group adopts just as the new idea or technology begins to feel outdated.\nLaggards are the last segment, where the curve tails off. Adoption within this group occurs very slowly and often only when necessary.\n\nThe bell-shaped curve reflects the cumulative adoption of innovations over time, demonstrating that the speed of adoption typically starts slow, accelerates until it reaches the majority of the potential market, and then slows down as fewer non-adopters remain.\nThis curve is central to strategic decisions in marketing, product development, and policy-making, helping stakeholders identify when and how to best introduce new ideas or technologies to different segments of society.",
    "crumbs": [
      "Custom trend"
    ]
  },
  {
    "objectID": "howto/custom_trend.html#the-dataset",
    "href": "howto/custom_trend.html#the-dataset",
    "title": "Custom Trend in Prophetverse",
    "section": "The dataset",
    "text": "The dataset\nAs a proxy for diffusion of innovations, we will use the number of stars received by Tensorflow Repository over time. Although this is not a perfect measure of adoption, it can give us an idea of how the popularity of the repository has grown since its inception.\nThis repository had an initial explosion of stars during the first ~10 days, which we will ignore since the daily granularity is not enough to capture the initial growth (hourly might work). After that, the number of starts grew by following a bell-shaped curve, which we will try to model. This curve might be related to the popularity of deep learning itself.\n\n\n\n\n\n\nNote\n\n\n\nThis dataset was obtained from https://github.com/emanuelef/daily-stars-explorer\n\n\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nfrom prophetverse.datasets.loaders import load_tensorflow_github_stars\n\ny = load_tensorflow_github_stars()\n\nfig, ax = plt.subplots()\n# First 30 days\ny.iloc[:30].plot.line(ax=ax)\ny.iloc[:30].cumsum()[\"day-stars\"].rename(\"Cumulative sum\").plot.line(ax=ax, legend=True)\nax.set_title(\"First 30 days\")\nfig.show()\n\nfig, axs = plt.subplots(nrows=2, sharex=True)\ny.iloc[30:].plot.line(ax=axs[0])\ny.iloc[30:].cumsum()[\"day-stars\"].rename(\"Cumulative sum\").plot.line(ax=axs[1])\n# FIgure title\nfig.suptitle(\"After the first 30 days\")\nfig.show()\n\n/var/folders/_2/9y4tsvdd2n3gqjgd2zmlr1km0000gn/T/ipykernel_78608/559078181.py:13: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n  fig.show()\n/var/folders/_2/9y4tsvdd2n3gqjgd2zmlr1km0000gn/T/ipykernel_78608/559078181.py:20: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n  fig.show()",
    "crumbs": [
      "Custom trend"
    ]
  },
  {
    "objectID": "howto/custom_trend.html#modeling-the-diffusion-of-innovations",
    "href": "howto/custom_trend.html#modeling-the-diffusion-of-innovations",
    "title": "Custom Trend in Prophetverse",
    "section": "Modeling the Diffusion of Innovations",
    "text": "Modeling the Diffusion of Innovations\nTo model this behaviour with Prophetverse, we will use the custom trend feature.\nWe will define a trend model class that implements the generalized logistic curve, which accepts assymetric curves. We will also add another premise: a varying capacity, which will allow us to model a linear growth of the total addressable market (TAM) over time. Let \\(G(t)\\) be the logistic curve defining the acumulated number of stars at time \\(t\\):\n\\[\n\\begin{align*}\nG(t) &= \\frac{C_1(t-t_0) + C_2}{\\left(1 + \\exp(-\\alpha v (t - t_0))\\right)^{\\frac{1}{v}}} \\\n\\text{where} & \\\\\nC_2 \\in \\mathbb{R}_+ &= \\text{is the constant capacity term} \\\\\nC_1 \\in \\mathbb{R}_+ &= \\text{is the linear increasing rate of the capacity} \\\\\nt_0 \\in \\mathbb{R} &= \\text{is the time offset term} \\\\\nv \\in \\mathbb{R}_+ &= \\text{determines the shape of the curve} \\\\\n\\alpha \\in \\mathbb{R} &= \\text{is the rate}\n\\end{align*}\n\\]\nIts derivative is:\n\\[\n\\begin{align*}\ng(t) &= \\alpha\\left(1 - \\frac{G(T)}{C_1(t-t_0) + C_2}\\right) G(T)  + \\frac{C_1}{C_1(t-t_0) + C_2}G(T)\n\\end{align*}\n\\]\nThat curve can be used as trend to model a diffusion process. Below, we plot it for a combination of parameters\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef g(t, C1, C2, t0, v, alpha):\n    return (C1 * (t - t0) + C2) / ((1 + np.exp(-alpha * v * (t - t0))) ** (1 / v))\n\n\ndef normalized_generalized_logistic(x, A, v, t0):\n    return 1 / (1 + np.exp(-A * v * (x - t0))) ** (1 / v)\n\n\n# Define the generalized logistic function\ndef generalized_logistic(x, C1, C2, alpha, v, t0):\n    return (C1 * x + C2) * normalized_generalized_logistic(x, alpha, v, t0)\n\n\ndef dgeneralized_logistic(x, C1, C2, alpha, v, t0):\n    return alpha * (\n        1 - (generalized_logistic(x, C1, C2, alpha, v, t0) / (C1 * x + C2)) ** v\n    ) * generalized_logistic(\n        x, C1, C2, alpha, v, t0\n    ) + C1 * normalized_generalized_logistic(\n        x, alpha, v, t0\n    )\n\n\nC1 = 1\nC2 = 50\nt0 = -2\nv = 1.2\nalpha = 0.5\nlabel = (f\"C1={C1:.1f}, C2={C2:.1f}, t0={t0:.1f}, v={v:.1f}, alpha={alpha:.1f}\",)\n\nt = np.linspace(-10, 10, 1000)\ngt = dgeneralized_logistic(t, C1=C1, C2=C2, t0=t0, v=v, alpha=alpha)\n\nfig, axs = plt.subplots(figsize=(12, 6), nrows=2, sharex=True)\naxs[0].plot(\n    t,\n    gt,\n    label=label,\n)\naxs[0].set_title(\"Visualization of g(t)\")\naxs[0].set_xlabel(\"t\")\naxs[0].set_ylabel(\"g(t)\")\n\nGt = generalized_logistic(t, C1=C1, C2=C2, t0=t0, v=v, alpha=alpha)\naxs[1].plot(\n    t,\n    Gt,\n    label=label,\n)\naxs[1].set_title(\"Visualization of G(t)\")\naxs[1].set_xlabel(\"t\")\naxs[1].set_ylabel(\"g(t)\")\n\n# axs[1].grid(True, alpha=0.3)\nfig.show()\n\n/var/folders/_2/9y4tsvdd2n3gqjgd2zmlr1km0000gn/T/ipykernel_78608/2052635467.py:59: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n  fig.show()\n\n\n\n\n\n\n\n\n\nThat curve has the bell-shape and the flexiblity to not be symmetric depending on the parameters. Furthermore, it tends to a constant value (\\(C1\\)) as time goes to infinity, which represent our knowledge that the size of the “market” of tensorflow/neural networks users starts at a value and grows with time.",
    "crumbs": [
      "Custom trend"
    ]
  },
  {
    "objectID": "howto/custom_trend.html#splitting-the-dataset",
    "href": "howto/custom_trend.html#splitting-the-dataset",
    "title": "Custom Trend in Prophetverse",
    "section": "Splitting the dataset",
    "text": "Splitting the dataset\nWe leave 7 years to forecast, and 1.5 year to train. Note that, without the prior information on the nature of the curve, a model could simply forecast a linear growth of the number of stars, which would be a very poor forecast.\n\nsplit_at = -int(365 * 5)\ny = y.iloc[20:]\ny_train, y_test = y.iloc[:split_at], y.iloc[split_at:]\n\n\nfig, axs = plt.subplots(nrows=2, sharex=True, figsize=(12, 6))\nax = axs[0]\nax = y_train[\"day-stars\"].rename(\"Train\").plot.line(legend=True, ax=ax)\ny_test[\"day-stars\"].rename(\"Test\").plot.line(ax=ax, alpha=0.2, legend=True)\nax.axvline(y_train.index[-1], color=\"red\", linestyle=\"--\", alpha=0.5, zorder=-1)\nax.set_title(\"Daily new stars\", loc=\"left\")\nax.set_xlabel(\"\")\nax.spines[\"left\"].set_visible(False)\nax.spines[\"right\"].set_visible(False)\nax.spines[\"top\"].set_visible(False)\nax.set_xlim(y.index.min(), y.index.max())\nax.set_ylim(0, 300)\n\nax = axs[1]\nax = y_train[\"day-stars\"].rename(\"Train\").cumsum().plot.line(legend=True)\n(\n    y_test[\"day-stars\"].rename(\"Test\").cumsum()\n    + y_train[\"day-stars\"].rename(\"Train\").cumsum().max()\n).plot.line(ax=ax, alpha=0.2, legend=True)\nax.axvline(y_train.index[-1], color=\"red\", linestyle=\"--\", alpha=0.5, zorder=-1)\nax.set_title(\"Total stars\", loc=\"left\")\nax.set_xlabel(\"\")\nax.spines[\"left\"].set_visible(False)\nax.spines[\"right\"].set_visible(False)\n# Superior\nax.spines[\"top\"].set_visible(False)\nax.set_xlim(y.index.min(), y.index.max())\n\nfig.suptitle(\"Tensorflow Stars\")\n\nText(0.5, 0.98, 'Tensorflow Stars')",
    "crumbs": [
      "Custom trend"
    ]
  },
  {
    "objectID": "howto/custom_trend.html#creating-the-custom-trend",
    "href": "howto/custom_trend.html#creating-the-custom-trend",
    "title": "Custom Trend in Prophetverse",
    "section": "Creating the custom trend",
    "text": "Creating the custom trend\nTo create a custom trend model for use in the Prophetverse library, users can extend the TrendModel abstract base class and implement the required abstract methods. Here’s a step-by-step guide to create a custom trend model, using the GenLogisticTrend class as an example.\n\nStep 1: Define helper functions\nThe GenLogisticTrend class will use the following helper functions:\n\nimport jax\nimport jax.numpy as jnp\n\n\n@jax.jit\ndef normalized_generalized_logistic(x, A, v, t0):\n    return 1 / (1 + jnp.exp(-A * v * (x - t0))) ** (1 / v)\n\n\n@jax.jit\ndef dnormalized_generalized_logistic(x, A, v, t0):\n    return (\n        A\n        * (1 - normalized_generalized_logistic(x, A, v, t0) ** v)\n        * normalized_generalized_logistic(x, A, v, t0)\n    )\n\n\n# Define the generalized logistic function\n\n\n@jax.jit\ndef dgeneralized_logistic(x, K1, K2, A, v, M):\n    return dnormalized_generalized_logistic(x, A, v, M) * (\n        K1 * x + K2\n    ) + K1 * normalized_generalized_logistic(x, A, v, M)\n\n\n\nStep 2: Define the Custom Trend Model Class\nCreate a new class that extends the TrendModel abstract base class. Implement the abstract methods initialize, prepare_input_data, and compute_trend.\n\nimport numpyro\nfrom numpyro import distributions as dist\nimport pandas as pd\nfrom typing import Dict # Added for type hint Dict[str, jnp.ndarray]\n\nfrom prophetverse.distributions import GammaReparametrized\nfrom prophetverse.effects import BaseEffect\nfrom prophetverse.effects.trend import TrendEffectMixin\nfrom prophetverse.utils.frame_to_array import convert_index_to_days_since_epoch\n\n\nclass GenLogisticTrend(TrendEffectMixin, BaseEffect):\n    \"\"\"\n    Custom trend model based on the Generalized Logistic function.\n    \"\"\"\n\n    def __init__(\n        self,\n        logistic_capacity_dist=dist.HalfNormal(10),\n        logistic_capacity2_dist=dist.HalfNormal(50_000),\n        shape_dist=dist.Gamma(1, 1),\n        logistic_rate_dist=GammaReparametrized(0.01, 0.01),\n        offset_prior=dist.Normal(0, 365 * 2),\n    ):\n\n        self.logistic_capacity_dist = logistic_capacity_dist\n        self.logistic_capacity2_dist = logistic_capacity2_dist\n        self.shape_dist = shape_dist\n        self.logistic_rate_dist = logistic_rate_dist\n        self.offset_prior = offset_prior\n\n        super().__init__()\n\n    def _fit(self, y: pd.DataFrame, X: pd.DataFrame, scale: float = 1):\n        \"\"\"Initialize the effect.\n\n        Set the prior location for the trend.\n\n        Parameters\n        ----------\n        y : pd.DataFrame\n            The timeseries dataframe\n\n        X : pd.DataFrame\n            The DataFrame to initialize the effect.\n\n        scale : float, optional\n            The scale of the timeseries. For multivariate timeseries, this is\n            a dataframe. For univariate, it is a simple float.\n        \"\"\"\n        t = convert_index_to_days_since_epoch(y.index)\n        self.t_min_ = t.min()\n        t = t - self.t_min_\n\n    def _transform(self, X: pd.DataFrame, fh: pd.PeriodIndex) -&gt; dict:\n        \"\"\"\n        Prepare the input data for the piecewise trend model.\n\n        Parameters\n        ----------\n        X: pd.DataFrame\n            The exogenous variables DataFrame.\n        fh: pd.PeriodIndex\n            The forecasting horizon as a pandas Index.\n\n        Returns\n        -------\n        jnp.ndarray\n            An array containing the prepared input data.\n        \"\"\"\n        t = convert_index_to_days_since_epoch(fh)\n        t = t - self.t_min_\n        self.offset_prior_loc = len(fh)\n        return t\n\n    def _predict(self, data, previous_effects: Dict[str, jnp.ndarray], params):\n        \"\"\"\n        Compute the trend based on the Generalized Logistic function.\n\n        Parameters\n        ----------\n        data: jnp.ndarray\n            The changepoint matrix.\n        predicted_effects: Dict[str, jnp.ndarray]\n            Dictionary of previously computed effects. For the trend, it is an empty\n            dict.\n\n        Returns\n        -------\n        jnp.ndarray\n            The computed trend.\n        \"\"\"\n        # Alias for clarity\n        time = data\n\n        logistic_rate = numpyro.sample(\"logistic_rate\", self.logistic_rate_dist)\n\n        logistic_capacity1 = numpyro.sample(\n            \"logistic_capacity\",\n            self.logistic_capacity_dist,\n        )\n\n        logistic_capacity2 = numpyro.sample(\n            \"logistic_capacity2\",\n            self.logistic_capacity2_dist,\n        )\n\n        shape = numpyro.sample(\"logistic_shape\", self.shape_dist)\n\n        offset = numpyro.sample(\"offset\", self.offset_prior)\n\n        trend = dgeneralized_logistic(\n            time,\n            K1=logistic_capacity1,\n            K2=logistic_capacity2,\n            A=logistic_rate,\n            v=shape,\n            M=offset,\n        )\n\n        numpyro.deterministic(\"__trend\", trend)\n\n        numpyro.deterministic(\n            \"capacity\", logistic_capacity1 * (time - offset) + logistic_capacity2\n        )\n\n        return trend.reshape((-1, 1))\n\n/Users/felipeangelim/Workspace/prophetverse/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm",
    "crumbs": [
      "Custom trend"
    ]
  },
  {
    "objectID": "howto/custom_trend.html#fit-the-model-and-make-predictions",
    "href": "howto/custom_trend.html#fit-the-model-and-make-predictions",
    "title": "Custom Trend in Prophetverse",
    "section": "Fit the model and make predictions",
    "text": "Fit the model and make predictions\n\nimport numpyro\nfrom sktime.transformations.series.fourier import FourierFeatures\n\nfrom prophetverse.effects import LinearFourierSeasonality\nfrom prophetverse.effects.linear import LinearEffect\nfrom prophetverse.engine import MCMCInferenceEngine\nfrom prophetverse.sktime import Prophetverse\nfrom prophetverse.utils.regex import no_input_columns, starts_with\n\nnumpyro.enable_x64()\n\nmodel = Prophetverse(\n    likelihood=\"negbinomial\",\n    trend=GenLogisticTrend(),\n    exogenous_effects=[\n        (\n            \"seasonality\",\n            LinearFourierSeasonality(\n                sp_list=[7, 365.25],\n                fourier_terms_list=[3, 8],\n                freq=\"D\",\n                prior_scale=0.1,\n                effect_mode=\"multiplicative\",\n            ),\n            no_input_columns,\n        ),\n    ],\n    inference_engine=MCMCInferenceEngine(\n        num_samples=500,\n        num_warmup=1000,\n    ),\n    # Avoid normalization of the timeseries by setting\n    # scale=1\n    scale=1,\n    noise_scale=10,\n)\n\nnumpyro.enable_x64()\n\nmodel.fit(y_train)\n\n/Users/felipeangelim/Workspace/prophetverse/src/prophetverse/sktime/univariate.py:126: FutureWarning: Warning: 'noise_scale' is deprecated and will be removed in version 0.7.0. Please update your code to avoid issues. Use the noise_scale parameter in the likelihood instead. You can import the likelihood from prophetverse.effects import NormalTargetLikelihood\n  deprecation_warning(\n/Users/felipeangelim/Workspace/prophetverse/src/prophetverse/sktime/univariate.py:126: FutureWarning: Warning: 'noise_scale' is deprecated and will be removed in version 0.7.0. Please update your code to avoid issues. Use the noise_scale parameter in the likelihood instead. You can import the likelihood from prophetverse.effects import NormalTargetLikelihood\n  deprecation_warning(\n/Users/felipeangelim/Workspace/prophetverse/src/prophetverse/sktime/univariate.py:214: UserWarning: No columns match the regex ^$\n  self._fit_effects(X, y)\n  0%|          | 0/1500 [00:00&lt;?, ?it/s]warmup:   0%|          | 1/1500 [00:02&lt;1:00:28,  2.42s/it, 1 steps of size 2.34e+00. acc. prob=0.00]warmup:   2%|▏         | 28/1500 [00:02&lt;01:35, 15.34it/s, 63 steps of size 7.21e-03. acc. prob=0.71]warmup:   3%|▎         | 43/1500 [00:03&lt;01:39, 14.65it/s, 1023 steps of size 8.50e-03. acc. prob=0.73]warmup:   4%|▍         | 58/1500 [00:03&lt;01:12, 19.82it/s, 727 steps of size 9.85e-03. acc. prob=0.75] warmup:   4%|▍         | 65/1500 [00:04&lt;01:02, 23.08it/s, 255 steps of size 5.76e-03. acc. prob=0.75]warmup:   5%|▍         | 73/1500 [00:04&lt;00:54, 26.43it/s, 271 steps of size 8.27e-03. acc. prob=0.75]warmup:   6%|▌         | 83/1500 [00:04&lt;00:55, 25.59it/s, 1023 steps of size 1.35e-02. acc. prob=0.76]warmup:   6%|▋         | 97/1500 [00:04&lt;00:37, 37.06it/s, 23 steps of size 1.17e-02. acc. prob=0.76]  warmup:   7%|▋         | 105/1500 [00:04&lt;00:38, 36.32it/s, 15 steps of size 2.11e-01. acc. prob=0.77]warmup:   8%|▊         | 120/1500 [00:05&lt;00:27, 51.03it/s, 63 steps of size 8.45e-02. acc. prob=0.77]warmup:   9%|▉         | 137/1500 [00:05&lt;00:20, 67.74it/s, 63 steps of size 8.20e-02. acc. prob=0.77]warmup:  10%|▉         | 148/1500 [00:05&lt;00:23, 57.99it/s, 15 steps of size 1.38e-01. acc. prob=0.77]warmup:  10%|█         | 157/1500 [00:06&lt;00:39, 34.41it/s, 255 steps of size 1.73e-02. acc. prob=0.77]warmup:  11%|█         | 164/1500 [00:06&lt;00:52, 25.27it/s, 511 steps of size 9.46e-03. acc. prob=0.77]warmup:  11%|█▏        | 169/1500 [00:07&lt;01:14, 17.77it/s, 511 steps of size 9.36e-03. acc. prob=0.77]warmup:  12%|█▏        | 173/1500 [00:07&lt;01:25, 15.55it/s, 255 steps of size 1.92e-02. acc. prob=0.77]warmup:  12%|█▏        | 176/1500 [00:07&lt;01:23, 15.86it/s, 255 steps of size 2.85e-02. acc. prob=0.77]warmup:  12%|█▏        | 179/1500 [00:08&lt;01:18, 16.87it/s, 255 steps of size 2.29e-02. acc. prob=0.77]warmup:  12%|█▏        | 182/1500 [00:08&lt;01:22, 15.93it/s, 511 steps of size 8.92e-03. acc. prob=0.77]warmup:  12%|█▏        | 185/1500 [00:08&lt;01:28, 14.85it/s, 255 steps of size 3.85e-02. acc. prob=0.77]warmup:  12%|█▏        | 187/1500 [00:08&lt;01:24, 15.45it/s, 255 steps of size 2.23e-02. acc. prob=0.77]warmup:  13%|█▎        | 190/1500 [00:08&lt;01:14, 17.68it/s, 63 steps of size 2.16e-02. acc. prob=0.77] warmup:  13%|█▎        | 193/1500 [00:08&lt;01:17, 16.89it/s, 127 steps of size 1.37e-02. acc. prob=0.77]warmup:  13%|█▎        | 195/1500 [00:09&lt;01:18, 16.63it/s, 127 steps of size 2.51e-02. acc. prob=0.77]warmup:  13%|█▎        | 197/1500 [00:09&lt;01:31, 14.17it/s, 511 steps of size 6.88e-03. acc. prob=0.77]warmup:  13%|█▎        | 199/1500 [00:09&lt;01:49, 11.88it/s, 255 steps of size 1.71e-02. acc. prob=0.77]warmup:  13%|█▎        | 201/1500 [00:09&lt;01:42, 12.73it/s, 127 steps of size 1.88e-02. acc. prob=0.77]warmup:  14%|█▎        | 204/1500 [00:09&lt;01:21, 15.91it/s, 63 steps of size 1.05e-02. acc. prob=0.77] warmup:  14%|█▎        | 206/1500 [00:09&lt;01:28, 14.67it/s, 255 steps of size 2.62e-02. acc. prob=0.77]warmup:  14%|█▍        | 210/1500 [00:10&lt;01:25, 15.15it/s, 511 steps of size 1.41e-02. acc. prob=0.77]warmup:  14%|█▍        | 212/1500 [00:10&lt;01:23, 15.34it/s, 127 steps of size 3.31e-02. acc. prob=0.77]warmup:  14%|█▍        | 216/1500 [00:10&lt;01:04, 19.78it/s, 127 steps of size 3.06e-02. acc. prob=0.77]warmup:  15%|█▍        | 219/1500 [00:10&lt;01:12, 17.76it/s, 255 steps of size 1.98e-02. acc. prob=0.77]warmup:  15%|█▍        | 222/1500 [00:10&lt;01:19, 16.04it/s, 511 steps of size 9.04e-03. acc. prob=0.77]warmup:  15%|█▍        | 224/1500 [00:10&lt;01:25, 14.89it/s, 255 steps of size 1.63e-02. acc. prob=0.77]warmup:  15%|█▌        | 228/1500 [00:11&lt;01:24, 15.04it/s, 511 steps of size 1.26e-02. acc. prob=0.77]warmup:  15%|█▌        | 230/1500 [00:11&lt;01:29, 14.22it/s, 255 steps of size 1.23e-02. acc. prob=0.77]warmup:  15%|█▌        | 232/1500 [00:11&lt;01:33, 13.59it/s, 255 steps of size 2.43e-02. acc. prob=0.77]warmup:  16%|█▌        | 235/1500 [00:11&lt;01:31, 13.80it/s, 511 steps of size 1.16e-02. acc. prob=0.77]warmup:  16%|█▌        | 237/1500 [00:11&lt;01:35, 13.25it/s, 255 steps of size 2.27e-02. acc. prob=0.78]warmup:  16%|█▌        | 240/1500 [00:12&lt;01:21, 15.47it/s, 255 steps of size 1.18e-02. acc. prob=0.77]warmup:  16%|█▌        | 242/1500 [00:12&lt;01:20, 15.54it/s, 127 steps of size 1.51e-02. acc. prob=0.77]warmup:  16%|█▋        | 244/1500 [00:12&lt;01:27, 14.43it/s, 255 steps of size 1.67e-02. acc. prob=0.77]warmup:  16%|█▋        | 246/1500 [00:12&lt;01:24, 14.83it/s, 127 steps of size 1.89e-02. acc. prob=0.78]warmup:  17%|█▋        | 249/1500 [00:12&lt;01:12, 17.21it/s, 127 steps of size 3.76e-02. acc. prob=0.78]warmup:  17%|█▋        | 251/1500 [00:12&lt;01:21, 15.35it/s, 511 steps of size 1.26e-01. acc. prob=0.77]warmup:  17%|█▋        | 255/1500 [00:12&lt;01:03, 19.59it/s, 127 steps of size 2.14e-02. acc. prob=0.77]warmup:  17%|█▋        | 258/1500 [00:13&lt;01:02, 19.81it/s, 63 steps of size 8.22e-03. acc. prob=0.77] warmup:  17%|█▋        | 261/1500 [00:13&lt;01:20, 15.33it/s, 127 steps of size 4.24e-02. acc. prob=0.77]warmup:  18%|█▊        | 263/1500 [00:13&lt;01:26, 14.33it/s, 511 steps of size 1.38e-02. acc. prob=0.77]warmup:  18%|█▊        | 265/1500 [00:13&lt;01:24, 14.68it/s, 127 steps of size 4.77e-02. acc. prob=0.77]warmup:  18%|█▊        | 268/1500 [00:13&lt;01:19, 15.51it/s, 255 steps of size 2.36e-02. acc. prob=0.77]warmup:  18%|█▊        | 271/1500 [00:14&lt;01:21, 15.04it/s, 511 steps of size 1.29e-02. acc. prob=0.77]warmup:  18%|█▊        | 273/1500 [00:14&lt;01:20, 15.23it/s, 127 steps of size 4.24e-02. acc. prob=0.78]warmup:  18%|█▊        | 276/1500 [00:14&lt;01:07, 18.12it/s, 63 steps of size 5.60e-03. acc. prob=0.77] warmup:  19%|█▊        | 278/1500 [00:14&lt;01:40, 12.11it/s, 511 steps of size 1.78e-02. acc. prob=0.77]warmup:  19%|█▊        | 280/1500 [00:14&lt;01:34, 12.90it/s, 127 steps of size 4.40e-02. acc. prob=0.78]warmup:  19%|█▉        | 283/1500 [00:14&lt;01:15, 16.01it/s, 127 steps of size 3.54e-02. acc. prob=0.78]warmup:  19%|█▉        | 286/1500 [00:14&lt;01:07, 18.00it/s, 127 steps of size 3.65e-02. acc. prob=0.78]warmup:  19%|█▉        | 289/1500 [00:15&lt;01:01, 19.55it/s, 127 steps of size 1.14e-02. acc. prob=0.77]warmup:  19%|█▉        | 292/1500 [00:15&lt;01:08, 17.56it/s, 127 steps of size 4.43e-02. acc. prob=0.78]warmup:  20%|█▉        | 295/1500 [00:15&lt;01:04, 18.80it/s, 127 steps of size 4.39e-02. acc. prob=0.78]warmup:  20%|█▉        | 298/1500 [00:15&lt;00:57, 20.97it/s, 255 steps of size 2.14e-02. acc. prob=0.78]warmup:  20%|██        | 301/1500 [00:15&lt;00:55, 21.79it/s, 127 steps of size 4.33e-02. acc. prob=0.78]warmup:  20%|██        | 305/1500 [00:15&lt;00:48, 24.64it/s, 127 steps of size 2.31e-02. acc. prob=0.78]warmup:  21%|██        | 308/1500 [00:15&lt;00:48, 24.43it/s, 127 steps of size 4.47e-02. acc. prob=0.78]warmup:  21%|██        | 311/1500 [00:16&lt;00:51, 23.15it/s, 127 steps of size 4.34e-02. acc. prob=0.78]warmup:  21%|██        | 315/1500 [00:16&lt;00:48, 24.48it/s, 255 steps of size 1.81e-02. acc. prob=0.78]warmup:  21%|██        | 318/1500 [00:16&lt;00:49, 23.70it/s, 63 steps of size 4.47e-02. acc. prob=0.78] warmup:  21%|██▏       | 322/1500 [00:16&lt;00:56, 20.74it/s, 511 steps of size 1.37e-02. acc. prob=0.78]warmup:  22%|██▏       | 325/1500 [00:16&lt;01:03, 18.50it/s, 127 steps of size 4.34e-02. acc. prob=0.78]warmup:  22%|██▏       | 329/1500 [00:16&lt;00:54, 21.49it/s, 255 steps of size 2.39e-02. acc. prob=0.78]warmup:  22%|██▏       | 332/1500 [00:17&lt;00:52, 22.07it/s, 127 steps of size 3.55e-02. acc. prob=0.78]warmup:  22%|██▏       | 335/1500 [00:17&lt;00:50, 23.23it/s, 127 steps of size 4.05e-02. acc. prob=0.78]warmup:  23%|██▎       | 338/1500 [00:17&lt;00:49, 23.34it/s, 127 steps of size 3.21e-02. acc. prob=0.78]warmup:  23%|██▎       | 341/1500 [00:17&lt;00:49, 23.30it/s, 255 steps of size 1.94e-02. acc. prob=0.78]warmup:  23%|██▎       | 344/1500 [00:17&lt;00:49, 23.41it/s, 127 steps of size 5.17e-02. acc. prob=0.78]warmup:  23%|██▎       | 347/1500 [00:17&lt;00:54, 21.33it/s, 255 steps of size 2.58e-02. acc. prob=0.78]warmup:  23%|██▎       | 350/1500 [00:17&lt;00:49, 23.05it/s, 127 steps of size 2.44e-02. acc. prob=0.78]warmup:  24%|██▎       | 353/1500 [00:17&lt;00:46, 24.43it/s, 63 steps of size 4.00e-02. acc. prob=0.78] warmup:  24%|██▎       | 356/1500 [00:18&lt;00:54, 21.10it/s, 255 steps of size 1.98e-02. acc. prob=0.78]warmup:  24%|██▍       | 359/1500 [00:18&lt;00:57, 19.99it/s, 127 steps of size 1.44e-02. acc. prob=0.78]warmup:  24%|██▍       | 362/1500 [00:18&lt;01:03, 17.82it/s, 127 steps of size 3.10e-02. acc. prob=0.78]warmup:  24%|██▍       | 365/1500 [00:18&lt;01:03, 17.86it/s, 255 steps of size 1.69e-02. acc. prob=0.78]warmup:  24%|██▍       | 367/1500 [00:18&lt;01:05, 17.37it/s, 127 steps of size 3.35e-02. acc. prob=0.78]warmup:  25%|██▍       | 370/1500 [00:18&lt;00:56, 19.90it/s, 63 steps of size 2.18e-02. acc. prob=0.78] warmup:  25%|██▍       | 373/1500 [00:19&lt;00:51, 22.07it/s, 63 steps of size 2.71e-02. acc. prob=0.78]warmup:  25%|██▌       | 376/1500 [00:19&lt;00:47, 23.74it/s, 63 steps of size 4.38e-02. acc. prob=0.78]warmup:  25%|██▌       | 379/1500 [00:19&lt;00:46, 24.18it/s, 255 steps of size 1.92e-02. acc. prob=0.78]warmup:  25%|██▌       | 382/1500 [00:19&lt;00:51, 21.81it/s, 127 steps of size 9.59e-03. acc. prob=0.78]warmup:  26%|██▌       | 385/1500 [00:19&lt;01:13, 15.19it/s, 255 steps of size 1.94e-02. acc. prob=0.78]warmup:  26%|██▌       | 387/1500 [00:19&lt;01:12, 15.34it/s, 127 steps of size 3.67e-02. acc. prob=0.78]warmup:  26%|██▌       | 390/1500 [00:19&lt;01:02, 17.80it/s, 63 steps of size 6.02e-02. acc. prob=0.78] warmup:  26%|██▌       | 393/1500 [00:20&lt;01:00, 18.30it/s, 255 steps of size 2.49e-02. acc. prob=0.78]warmup:  26%|██▋       | 396/1500 [00:20&lt;00:56, 19.68it/s, 127 steps of size 3.68e-02. acc. prob=0.78]warmup:  27%|██▋       | 399/1500 [00:20&lt;00:50, 21.71it/s, 127 steps of size 2.74e-02. acc. prob=0.78]warmup:  27%|██▋       | 402/1500 [00:20&lt;00:49, 22.32it/s, 127 steps of size 2.79e-02. acc. prob=0.78]warmup:  27%|██▋       | 405/1500 [00:20&lt;00:48, 22.75it/s, 127 steps of size 3.62e-02. acc. prob=0.78]warmup:  27%|██▋       | 408/1500 [00:20&lt;00:49, 21.98it/s, 255 steps of size 2.02e-02. acc. prob=0.78]warmup:  27%|██▋       | 411/1500 [00:20&lt;00:52, 20.56it/s, 255 steps of size 1.98e-02. acc. prob=0.78]warmup:  28%|██▊       | 414/1500 [00:21&lt;00:55, 19.70it/s, 127 steps of size 3.22e-02. acc. prob=0.78]warmup:  28%|██▊       | 417/1500 [00:21&lt;00:52, 20.80it/s, 127 steps of size 4.05e-02. acc. prob=0.78]warmup:  28%|██▊       | 421/1500 [00:21&lt;00:49, 21.76it/s, 255 steps of size 2.24e-02. acc. prob=0.78]warmup:  28%|██▊       | 424/1500 [00:21&lt;00:48, 22.28it/s, 127 steps of size 2.28e-02. acc. prob=0.78]warmup:  28%|██▊       | 427/1500 [00:21&lt;00:45, 23.76it/s, 127 steps of size 2.82e-02. acc. prob=0.78]warmup:  29%|██▊       | 430/1500 [00:21&lt;00:44, 23.79it/s, 127 steps of size 3.39e-02. acc. prob=0.78]warmup:  29%|██▉       | 433/1500 [00:21&lt;00:44, 23.77it/s, 127 steps of size 1.29e-02. acc. prob=0.78]warmup:  29%|██▉       | 436/1500 [00:22&lt;00:53, 19.81it/s, 127 steps of size 2.45e-02. acc. prob=0.78]warmup:  29%|██▉       | 439/1500 [00:22&lt;00:50, 20.83it/s, 127 steps of size 4.45e-02. acc. prob=0.78]warmup:  29%|██▉       | 442/1500 [00:22&lt;00:49, 21.34it/s, 127 steps of size 2.47e-02. acc. prob=0.78]warmup:  30%|██▉       | 445/1500 [00:22&lt;00:52, 20.16it/s, 127 steps of size 2.80e-02. acc. prob=0.78]warmup:  30%|██▉       | 448/1500 [00:22&lt;00:54, 19.45it/s, 127 steps of size 3.83e-02. acc. prob=0.78]warmup:  30%|███       | 453/1500 [00:22&lt;00:41, 25.25it/s, 50 steps of size 2.78e-02. acc. prob=0.78] warmup:  30%|███       | 457/1500 [00:22&lt;00:42, 24.42it/s, 255 steps of size 2.39e-02. acc. prob=0.78]warmup:  31%|███       | 461/1500 [00:23&lt;00:50, 20.72it/s, 511 steps of size 1.50e-02. acc. prob=0.78]warmup:  31%|███       | 464/1500 [00:23&lt;00:50, 20.63it/s, 63 steps of size 9.12e-02. acc. prob=0.78] warmup:  31%|███       | 467/1500 [00:23&lt;00:59, 17.25it/s, 255 steps of size 3.01e-02. acc. prob=0.78]warmup:  31%|███▏      | 470/1500 [00:23&lt;00:55, 18.66it/s, 127 steps of size 5.30e-02. acc. prob=0.78]warmup:  32%|███▏      | 473/1500 [00:23&lt;00:53, 19.15it/s, 127 steps of size 5.71e-02. acc. prob=0.78]warmup:  32%|███▏      | 477/1500 [00:24&lt;00:49, 20.54it/s, 255 steps of size 2.85e-02. acc. prob=0.78]warmup:  32%|███▏      | 480/1500 [00:24&lt;00:47, 21.32it/s, 127 steps of size 8.72e-03. acc. prob=0.78]warmup:  32%|███▏      | 483/1500 [00:24&lt;01:02, 16.37it/s, 127 steps of size 3.89e-02. acc. prob=0.78]warmup:  32%|███▏      | 486/1500 [00:24&lt;00:56, 17.90it/s, 255 steps of size 2.08e-02. acc. prob=0.78]warmup:  33%|███▎      | 488/1500 [00:24&lt;00:58, 17.40it/s, 127 steps of size 3.97e-02. acc. prob=0.78]warmup:  33%|███▎      | 491/1500 [00:24&lt;00:53, 18.92it/s, 255 steps of size 3.02e-02. acc. prob=0.78]warmup:  33%|███▎      | 494/1500 [00:25&lt;00:49, 20.18it/s, 127 steps of size 6.63e-02. acc. prob=0.78]warmup:  33%|███▎      | 497/1500 [00:25&lt;00:55, 18.20it/s, 255 steps of size 2.92e-02. acc. prob=0.78]warmup:  33%|███▎      | 500/1500 [00:25&lt;00:48, 20.45it/s, 63 steps of size 6.58e-02. acc. prob=0.78] warmup:  34%|███▎      | 503/1500 [00:25&lt;00:46, 21.24it/s, 127 steps of size 4.13e-02. acc. prob=0.78]warmup:  34%|███▍      | 509/1500 [00:25&lt;00:34, 28.37it/s, 127 steps of size 2.90e-02. acc. prob=0.78]warmup:  34%|███▍      | 512/1500 [00:25&lt;00:34, 28.43it/s, 63 steps of size 3.52e-02. acc. prob=0.78] warmup:  34%|███▍      | 515/1500 [00:25&lt;00:39, 24.63it/s, 127 steps of size 3.70e-02. acc. prob=0.78]warmup:  35%|███▍      | 518/1500 [00:25&lt;00:38, 25.61it/s, 127 steps of size 3.45e-02. acc. prob=0.78]warmup:  35%|███▍      | 522/1500 [00:26&lt;00:33, 28.82it/s, 63 steps of size 7.14e-02. acc. prob=0.78] warmup:  35%|███▌      | 526/1500 [00:26&lt;00:31, 31.08it/s, 127 steps of size 5.04e-02. acc. prob=0.78]warmup:  35%|███▌      | 531/1500 [00:26&lt;00:28, 33.60it/s, 127 steps of size 3.23e-02. acc. prob=0.78]warmup:  36%|███▌      | 535/1500 [00:26&lt;00:29, 33.09it/s, 63 steps of size 3.60e-02. acc. prob=0.78] warmup:  36%|███▌      | 539/1500 [00:26&lt;00:28, 34.16it/s, 127 steps of size 3.10e-02. acc. prob=0.78]warmup:  36%|███▌      | 543/1500 [00:26&lt;00:31, 30.21it/s, 127 steps of size 3.96e-02. acc. prob=0.78]warmup:  36%|███▋      | 547/1500 [00:26&lt;00:34, 27.39it/s, 255 steps of size 2.40e-02. acc. prob=0.78]warmup:  37%|███▋      | 550/1500 [00:27&lt;00:35, 26.46it/s, 63 steps of size 6.77e-02. acc. prob=0.79] warmup:  37%|███▋      | 554/1500 [00:27&lt;00:33, 27.86it/s, 127 steps of size 4.35e-02. acc. prob=0.78]warmup:  37%|███▋      | 559/1500 [00:27&lt;00:33, 28.33it/s, 255 steps of size 2.41e-02. acc. prob=0.78]warmup:  37%|███▋      | 562/1500 [00:27&lt;00:34, 27.11it/s, 127 steps of size 2.88e-02. acc. prob=0.78]warmup:  38%|███▊      | 566/1500 [00:27&lt;00:36, 25.79it/s, 255 steps of size 2.13e-02. acc. prob=0.78]warmup:  38%|███▊      | 569/1500 [00:27&lt;00:40, 23.19it/s, 127 steps of size 4.59e-02. acc. prob=0.79]warmup:  38%|███▊      | 572/1500 [00:27&lt;00:39, 23.33it/s, 127 steps of size 4.79e-02. acc. prob=0.79]warmup:  38%|███▊      | 575/1500 [00:27&lt;00:37, 24.55it/s, 127 steps of size 3.86e-02. acc. prob=0.79]warmup:  39%|███▊      | 579/1500 [00:28&lt;00:33, 27.75it/s, 127 steps of size 3.03e-02. acc. prob=0.78]warmup:  39%|███▉      | 582/1500 [00:28&lt;00:32, 27.92it/s, 63 steps of size 2.91e-02. acc. prob=0.78] warmup:  39%|███▉      | 585/1500 [00:28&lt;00:34, 26.62it/s, 127 steps of size 5.31e-02. acc. prob=0.79]warmup:  39%|███▉      | 589/1500 [00:28&lt;00:30, 29.68it/s, 127 steps of size 3.59e-02. acc. prob=0.79]warmup:  40%|███▉      | 593/1500 [00:28&lt;00:30, 30.13it/s, 127 steps of size 3.03e-02. acc. prob=0.79]warmup:  40%|███▉      | 597/1500 [00:28&lt;00:31, 29.12it/s, 63 steps of size 2.83e-02. acc. prob=0.79] warmup:  40%|████      | 600/1500 [00:28&lt;00:32, 27.44it/s, 127 steps of size 3.33e-02. acc. prob=0.79]warmup:  40%|████      | 603/1500 [00:28&lt;00:34, 26.32it/s, 127 steps of size 3.43e-02. acc. prob=0.79]warmup:  40%|████      | 606/1500 [00:29&lt;00:35, 25.54it/s, 127 steps of size 5.31e-02. acc. prob=0.79]warmup:  41%|████      | 609/1500 [00:29&lt;00:37, 23.79it/s, 127 steps of size 3.06e-02. acc. prob=0.79]warmup:  41%|████      | 612/1500 [00:29&lt;00:35, 24.95it/s, 63 steps of size 4.68e-02. acc. prob=0.79] warmup:  41%|████      | 616/1500 [00:29&lt;00:32, 27.00it/s, 127 steps of size 5.19e-02. acc. prob=0.79]warmup:  41%|████▏     | 620/1500 [00:29&lt;00:29, 29.87it/s, 63 steps of size 5.34e-02. acc. prob=0.79] warmup:  42%|████▏     | 624/1500 [00:29&lt;00:32, 27.24it/s, 127 steps of size 3.81e-02. acc. prob=0.79]warmup:  42%|████▏     | 627/1500 [00:29&lt;00:31, 27.54it/s, 127 steps of size 4.86e-02. acc. prob=0.79]warmup:  42%|████▏     | 630/1500 [00:29&lt;00:31, 27.78it/s, 127 steps of size 3.62e-02. acc. prob=0.79]warmup:  42%|████▏     | 634/1500 [00:30&lt;00:28, 30.64it/s, 63 steps of size 2.45e-02. acc. prob=0.79] warmup:  43%|████▎     | 638/1500 [00:30&lt;00:29, 29.37it/s, 63 steps of size 5.15e-02. acc. prob=0.79]warmup:  43%|████▎     | 642/1500 [00:30&lt;00:29, 28.82it/s, 127 steps of size 2.54e-02. acc. prob=0.79]warmup:  43%|████▎     | 645/1500 [00:30&lt;00:31, 27.33it/s, 127 steps of size 3.99e-02. acc. prob=0.79]warmup:  43%|████▎     | 648/1500 [00:30&lt;00:32, 26.29it/s, 127 steps of size 3.23e-02. acc. prob=0.79]warmup:  43%|████▎     | 651/1500 [00:30&lt;00:33, 25.46it/s, 127 steps of size 3.14e-02. acc. prob=0.79]warmup:  44%|████▎     | 654/1500 [00:30&lt;00:32, 26.21it/s, 63 steps of size 6.23e-02. acc. prob=0.79] warmup:  44%|████▍     | 658/1500 [00:30&lt;00:30, 27.53it/s, 127 steps of size 3.17e-02. acc. prob=0.79]warmup:  44%|████▍     | 661/1500 [00:31&lt;00:30, 27.75it/s, 127 steps of size 3.59e-02. acc. prob=0.79]warmup:  44%|████▍     | 664/1500 [00:31&lt;00:38, 21.92it/s, 255 steps of size 2.40e-02. acc. prob=0.79]warmup:  44%|████▍     | 667/1500 [00:31&lt;00:35, 23.46it/s, 63 steps of size 4.51e-02. acc. prob=0.79] warmup:  45%|████▍     | 670/1500 [00:31&lt;00:33, 24.75it/s, 127 steps of size 3.13e-02. acc. prob=0.79]warmup:  45%|████▍     | 673/1500 [00:31&lt;00:33, 24.50it/s, 127 steps of size 3.21e-02. acc. prob=0.79]warmup:  45%|████▌     | 676/1500 [00:31&lt;00:32, 25.58it/s, 63 steps of size 4.87e-02. acc. prob=0.79] warmup:  45%|████▌     | 680/1500 [00:31&lt;00:28, 29.04it/s, 63 steps of size 5.96e-02. acc. prob=0.79]warmup:  46%|████▌     | 683/1500 [00:31&lt;00:29, 27.24it/s, 127 steps of size 3.30e-02. acc. prob=0.79]warmup:  46%|████▌     | 686/1500 [00:32&lt;00:31, 26.22it/s, 127 steps of size 3.37e-02. acc. prob=0.79]warmup:  46%|████▌     | 690/1500 [00:32&lt;00:29, 27.86it/s, 127 steps of size 3.30e-02. acc. prob=0.79]warmup:  46%|████▋     | 694/1500 [00:32&lt;00:26, 30.00it/s, 127 steps of size 4.54e-02. acc. prob=0.79]warmup:  47%|████▋     | 698/1500 [00:32&lt;00:24, 32.12it/s, 6 steps of size 2.12e-02. acc. prob=0.79]  warmup:  47%|████▋     | 702/1500 [00:32&lt;00:28, 27.68it/s, 63 steps of size 5.07e-02. acc. prob=0.79]warmup:  47%|████▋     | 705/1500 [00:32&lt;00:31, 25.43it/s, 127 steps of size 3.08e-02. acc. prob=0.79]warmup:  47%|████▋     | 708/1500 [00:32&lt;00:31, 24.97it/s, 127 steps of size 5.23e-02. acc. prob=0.79]warmup:  47%|████▋     | 712/1500 [00:33&lt;00:28, 27.59it/s, 127 steps of size 2.80e-02. acc. prob=0.79]warmup:  48%|████▊     | 715/1500 [00:33&lt;00:29, 26.49it/s, 127 steps of size 4.59e-02. acc. prob=0.79]warmup:  48%|████▊     | 718/1500 [00:33&lt;00:28, 27.02it/s, 127 steps of size 4.73e-02. acc. prob=0.79]warmup:  48%|████▊     | 723/1500 [00:33&lt;00:24, 31.59it/s, 63 steps of size 4.06e-02. acc. prob=0.79] warmup:  48%|████▊     | 727/1500 [00:33&lt;00:24, 31.66it/s, 63 steps of size 3.57e-02. acc. prob=0.79]warmup:  49%|████▊     | 731/1500 [00:33&lt;00:24, 31.72it/s, 63 steps of size 4.23e-02. acc. prob=0.79]warmup:  49%|████▉     | 735/1500 [00:33&lt;00:25, 30.29it/s, 127 steps of size 3.61e-02. acc. prob=0.79]warmup:  49%|████▉     | 739/1500 [00:33&lt;00:23, 32.18it/s, 5 steps of size 2.67e-02. acc. prob=0.79]  warmup:  50%|████▉     | 743/1500 [00:34&lt;00:24, 30.53it/s, 63 steps of size 4.98e-02. acc. prob=0.79]warmup:  50%|████▉     | 747/1500 [00:34&lt;00:23, 31.71it/s, 127 steps of size 3.99e-02. acc. prob=0.79]warmup:  50%|█████     | 752/1500 [00:34&lt;00:22, 33.20it/s, 127 steps of size 3.42e-02. acc. prob=0.79]warmup:  50%|█████     | 756/1500 [00:34&lt;00:23, 31.29it/s, 127 steps of size 3.23e-02. acc. prob=0.79]warmup:  51%|█████     | 760/1500 [00:34&lt;00:25, 28.68it/s, 127 steps of size 4.41e-02. acc. prob=0.79]warmup:  51%|█████     | 763/1500 [00:34&lt;00:25, 28.65it/s, 127 steps of size 3.37e-02. acc. prob=0.79]warmup:  51%|█████     | 766/1500 [00:34&lt;00:26, 27.21it/s, 127 steps of size 3.49e-02. acc. prob=0.79]warmup:  51%|█████▏    | 769/1500 [00:34&lt;00:26, 27.50it/s, 63 steps of size 5.39e-02. acc. prob=0.79] warmup:  51%|█████▏    | 772/1500 [00:35&lt;00:26, 27.77it/s, 127 steps of size 3.46e-02. acc. prob=0.79]warmup:  52%|█████▏    | 775/1500 [00:35&lt;00:25, 27.96it/s, 127 steps of size 4.63e-02. acc. prob=0.79]warmup:  52%|█████▏    | 778/1500 [00:35&lt;00:30, 23.58it/s, 127 steps of size 3.47e-02. acc. prob=0.79]warmup:  52%|█████▏    | 781/1500 [00:35&lt;00:28, 24.82it/s, 63 steps of size 3.35e-02. acc. prob=0.79] warmup:  52%|█████▏    | 786/1500 [00:35&lt;00:24, 29.26it/s, 127 steps of size 3.28e-02. acc. prob=0.79]warmup:  53%|█████▎    | 789/1500 [00:35&lt;00:24, 29.07it/s, 63 steps of size 3.15e-02. acc. prob=0.79] warmup:  53%|█████▎    | 792/1500 [00:35&lt;00:24, 28.92it/s, 63 steps of size 5.47e-02. acc. prob=0.79]warmup:  53%|█████▎    | 796/1500 [00:35&lt;00:22, 31.87it/s, 63 steps of size 4.42e-02. acc. prob=0.79]warmup:  53%|█████▎    | 800/1500 [00:35&lt;00:22, 31.81it/s, 63 steps of size 5.99e-02. acc. prob=0.79]warmup:  54%|█████▎    | 804/1500 [00:36&lt;00:22, 31.51it/s, 74 steps of size 4.95e-02. acc. prob=0.79]warmup:  54%|█████▍    | 808/1500 [00:36&lt;00:21, 31.56it/s, 127 steps of size 3.72e-02. acc. prob=0.79]warmup:  54%|█████▍    | 812/1500 [00:36&lt;00:22, 30.07it/s, 63 steps of size 5.75e-02. acc. prob=0.79] warmup:  54%|█████▍    | 816/1500 [00:36&lt;00:22, 30.57it/s, 127 steps of size 3.74e-02. acc. prob=0.79]warmup:  55%|█████▍    | 820/1500 [00:36&lt;00:23, 29.48it/s, 127 steps of size 3.65e-02. acc. prob=0.79]warmup:  55%|█████▍    | 823/1500 [00:36&lt;00:24, 27.88it/s, 127 steps of size 3.97e-02. acc. prob=0.79]warmup:  55%|█████▌    | 827/1500 [00:36&lt;00:23, 28.99it/s, 127 steps of size 4.25e-02. acc. prob=0.79]warmup:  55%|█████▌    | 830/1500 [00:37&lt;00:23, 28.88it/s, 127 steps of size 2.33e-02. acc. prob=0.79]warmup:  56%|█████▌    | 833/1500 [00:37&lt;00:24, 27.37it/s, 127 steps of size 3.78e-02. acc. prob=0.79]warmup:  56%|█████▌    | 836/1500 [00:37&lt;00:25, 26.28it/s, 127 steps of size 3.08e-02. acc. prob=0.79]warmup:  56%|█████▌    | 839/1500 [00:37&lt;00:25, 25.53it/s, 127 steps of size 3.23e-02. acc. prob=0.79]warmup:  56%|█████▌    | 842/1500 [00:37&lt;00:24, 26.38it/s, 63 steps of size 4.34e-02. acc. prob=0.79] warmup:  56%|█████▋    | 846/1500 [00:37&lt;00:23, 28.05it/s, 127 steps of size 3.02e-02. acc. prob=0.79]warmup:  57%|█████▋    | 849/1500 [00:37&lt;00:23, 28.13it/s, 63 steps of size 4.92e-02. acc. prob=0.79] warmup:  57%|█████▋    | 853/1500 [00:37&lt;00:20, 30.82it/s, 63 steps of size 5.98e-02. acc. prob=0.79]warmup:  57%|█████▋    | 857/1500 [00:37&lt;00:20, 30.90it/s, 127 steps of size 4.65e-02. acc. prob=0.79]warmup:  57%|█████▋    | 862/1500 [00:38&lt;00:17, 35.46it/s, 63 steps of size 5.07e-02. acc. prob=0.79] warmup:  58%|█████▊    | 866/1500 [00:38&lt;00:17, 35.89it/s, 127 steps of size 3.42e-02. acc. prob=0.79]warmup:  58%|█████▊    | 870/1500 [00:38&lt;00:20, 31.12it/s, 127 steps of size 3.98e-02. acc. prob=0.79]warmup:  58%|█████▊    | 874/1500 [00:38&lt;00:20, 29.84it/s, 127 steps of size 4.76e-02. acc. prob=0.79]warmup:  59%|█████▊    | 879/1500 [00:38&lt;00:18, 33.90it/s, 63 steps of size 4.16e-02. acc. prob=0.79] warmup:  59%|█████▉    | 884/1500 [00:38&lt;00:16, 37.44it/s, 63 steps of size 2.25e-02. acc. prob=0.79]warmup:  59%|█████▉    | 888/1500 [00:38&lt;00:19, 31.89it/s, 127 steps of size 4.04e-02. acc. prob=0.79]warmup:  59%|█████▉    | 892/1500 [00:39&lt;00:19, 30.40it/s, 127 steps of size 3.28e-02. acc. prob=0.79]warmup:  60%|█████▉    | 896/1500 [00:39&lt;00:18, 31.83it/s, 16 steps of size 3.14e-02. acc. prob=0.79] warmup:  60%|██████    | 900/1500 [00:39&lt;00:21, 27.32it/s, 127 steps of size 3.52e-02. acc. prob=0.79]warmup:  60%|██████    | 903/1500 [00:39&lt;00:21, 27.54it/s, 63 steps of size 5.40e-02. acc. prob=0.79] warmup:  60%|██████    | 907/1500 [00:39&lt;00:19, 29.88it/s, 127 steps of size 3.35e-02. acc. prob=0.79]warmup:  61%|██████    | 911/1500 [00:39&lt;00:20, 28.96it/s, 63 steps of size 4.59e-02. acc. prob=0.79] warmup:  61%|██████    | 915/1500 [00:39&lt;00:20, 28.43it/s, 127 steps of size 3.38e-02. acc. prob=0.79]warmup:  61%|██████▏   | 919/1500 [00:39&lt;00:19, 29.31it/s, 127 steps of size 3.92e-02. acc. prob=0.79]warmup:  61%|██████▏   | 922/1500 [00:40&lt;00:20, 27.72it/s, 127 steps of size 3.48e-02. acc. prob=0.79]warmup:  62%|██████▏   | 925/1500 [00:40&lt;00:21, 26.55it/s, 127 steps of size 4.54e-02. acc. prob=0.79]warmup:  62%|██████▏   | 930/1500 [00:40&lt;00:17, 31.93it/s, 63 steps of size 3.10e-02. acc. prob=0.79] warmup:  62%|██████▏   | 934/1500 [00:40&lt;00:18, 30.31it/s, 63 steps of size 2.68e-02. acc. prob=0.79]warmup:  63%|██████▎   | 938/1500 [00:40&lt;00:20, 28.02it/s, 127 steps of size 4.61e-02. acc. prob=0.79]warmup:  63%|██████▎   | 941/1500 [00:40&lt;00:19, 28.11it/s, 127 steps of size 3.54e-02. acc. prob=0.79]warmup:  63%|██████▎   | 946/1500 [00:40&lt;00:18, 30.51it/s, 127 steps of size 3.01e-02. acc. prob=0.79]warmup:  63%|██████▎   | 950/1500 [00:41&lt;00:19, 28.12it/s, 127 steps of size 4.39e-02. acc. prob=0.79]warmup:  64%|██████▎   | 954/1500 [00:41&lt;00:21, 25.47it/s, 511 steps of size 1.29e-02. acc. prob=0.79]warmup:  64%|██████▍   | 957/1500 [00:41&lt;00:28, 18.73it/s, 127 steps of size 5.66e-02. acc. prob=0.79]warmup:  64%|██████▍   | 962/1500 [00:41&lt;00:25, 21.45it/s, 255 steps of size 1.96e-02. acc. prob=0.79]warmup:  64%|██████▍   | 965/1500 [00:41&lt;00:25, 21.18it/s, 63 steps of size 9.39e-02. acc. prob=0.79] warmup:  65%|██████▍   | 968/1500 [00:42&lt;00:30, 17.62it/s, 255 steps of size 2.77e-02. acc. prob=0.79]warmup:  65%|██████▍   | 972/1500 [00:42&lt;00:30, 17.27it/s, 511 steps of size 1.26e-02. acc. prob=0.79]warmup:  65%|██████▍   | 974/1500 [00:42&lt;00:33, 15.89it/s, 255 steps of size 3.46e-02. acc. prob=0.79]warmup:  65%|██████▌   | 976/1500 [00:42&lt;00:32, 16.13it/s, 127 steps of size 5.94e-02. acc. prob=0.79]warmup:  65%|██████▌   | 979/1500 [00:42&lt;00:33, 15.71it/s, 511 steps of size 1.70e-02. acc. prob=0.79]warmup:  65%|██████▌   | 981/1500 [00:42&lt;00:32, 15.74it/s, 127 steps of size 4.83e-02. acc. prob=0.79]warmup:  66%|██████▌   | 984/1500 [00:43&lt;00:32, 15.71it/s, 511 steps of size 1.84e-02. acc. prob=0.79]warmup:  66%|██████▌   | 986/1500 [00:43&lt;00:32, 15.76it/s, 127 steps of size 4.51e-02. acc. prob=0.79]warmup:  66%|██████▌   | 990/1500 [00:43&lt;00:25, 20.18it/s, 63 steps of size 2.74e-02. acc. prob=0.79] warmup:  66%|██████▌   | 993/1500 [00:43&lt;00:22, 22.20it/s, 127 steps of size 3.76e-02. acc. prob=0.79]warmup:  66%|██████▋   | 997/1500 [00:43&lt;00:20, 24.97it/s, 127 steps of size 5.75e-02. acc. prob=0.79]sample:  67%|██████▋   | 1001/1500 [00:43&lt;00:18, 26.30it/s, 127 steps of size 3.42e-02. acc. prob=0.99]sample:  67%|██████▋   | 1004/1500 [00:43&lt;00:19, 25.61it/s, 127 steps of size 3.42e-02. acc. prob=0.91]sample:  67%|██████▋   | 1007/1500 [00:44&lt;00:19, 25.11it/s, 127 steps of size 3.42e-02. acc. prob=0.92]sample:  67%|██████▋   | 1010/1500 [00:44&lt;00:19, 24.76it/s, 127 steps of size 3.42e-02. acc. prob=0.91]sample:  68%|██████▊   | 1013/1500 [00:44&lt;00:18, 25.77it/s, 127 steps of size 3.42e-02. acc. prob=0.92]sample:  68%|██████▊   | 1016/1500 [00:44&lt;00:19, 25.22it/s, 127 steps of size 3.42e-02. acc. prob=0.92]sample:  68%|██████▊   | 1019/1500 [00:44&lt;00:19, 24.87it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  68%|██████▊   | 1022/1500 [00:44&lt;00:19, 24.56it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  68%|██████▊   | 1025/1500 [00:44&lt;00:19, 24.37it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  69%|██████▊   | 1028/1500 [00:44&lt;00:19, 24.23it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  69%|██████▊   | 1031/1500 [00:45&lt;00:19, 24.11it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  69%|██████▉   | 1034/1500 [00:45&lt;00:19, 24.00it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  69%|██████▉   | 1037/1500 [00:45&lt;00:19, 23.92it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  69%|██████▉   | 1040/1500 [00:45&lt;00:19, 23.86it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  70%|██████▉   | 1043/1500 [00:45&lt;00:19, 23.79it/s, 127 steps of size 3.42e-02. acc. prob=0.92]sample:  70%|██████▉   | 1046/1500 [00:45&lt;00:19, 23.78it/s, 127 steps of size 3.42e-02. acc. prob=0.92]sample:  70%|██████▉   | 1049/1500 [00:45&lt;00:18, 23.75it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  70%|███████   | 1052/1500 [00:45&lt;00:18, 23.79it/s, 127 steps of size 3.42e-02. acc. prob=0.92]sample:  70%|███████   | 1055/1500 [00:46&lt;00:18, 23.79it/s, 127 steps of size 3.42e-02. acc. prob=0.92]sample:  71%|███████   | 1058/1500 [00:46&lt;00:18, 23.82it/s, 127 steps of size 3.42e-02. acc. prob=0.92]sample:  71%|███████   | 1061/1500 [00:46&lt;00:18, 23.81it/s, 127 steps of size 3.42e-02. acc. prob=0.92]sample:  71%|███████   | 1064/1500 [00:46&lt;00:17, 25.04it/s, 63 steps of size 3.42e-02. acc. prob=0.92] sample:  71%|███████   | 1067/1500 [00:46&lt;00:17, 24.71it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  71%|███████▏  | 1070/1500 [00:46&lt;00:17, 24.45it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  72%|███████▏  | 1073/1500 [00:46&lt;00:17, 24.30it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  72%|███████▏  | 1076/1500 [00:46&lt;00:17, 24.18it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  72%|███████▏  | 1079/1500 [00:47&lt;00:17, 24.07it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  72%|███████▏  | 1082/1500 [00:47&lt;00:17, 24.03it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  72%|███████▏  | 1085/1500 [00:47&lt;00:17, 24.01it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  73%|███████▎  | 1088/1500 [00:47&lt;00:17, 23.93it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  73%|███████▎  | 1091/1500 [00:47&lt;00:17, 23.89it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  73%|███████▎  | 1094/1500 [00:47&lt;00:17, 23.85it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  73%|███████▎  | 1097/1500 [00:47&lt;00:16, 23.86it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  73%|███████▎  | 1100/1500 [00:47&lt;00:16, 23.86it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  74%|███████▎  | 1103/1500 [00:48&lt;00:16, 23.84it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  74%|███████▎  | 1106/1500 [00:48&lt;00:16, 23.85it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  74%|███████▍  | 1109/1500 [00:48&lt;00:16, 23.81it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  74%|███████▍  | 1112/1500 [00:48&lt;00:16, 23.78it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  74%|███████▍  | 1115/1500 [00:48&lt;00:16, 23.81it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  75%|███████▍  | 1118/1500 [00:48&lt;00:16, 23.81it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  75%|███████▍  | 1121/1500 [00:48&lt;00:15, 23.77it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  75%|███████▍  | 1124/1500 [00:48&lt;00:15, 23.87it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  75%|███████▌  | 1127/1500 [00:49&lt;00:15, 23.85it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  75%|███████▌  | 1130/1500 [00:49&lt;00:15, 23.76it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  76%|███████▌  | 1133/1500 [00:49&lt;00:15, 23.72it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  76%|███████▌  | 1136/1500 [00:49&lt;00:15, 23.74it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  76%|███████▌  | 1139/1500 [00:49&lt;00:15, 23.74it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  76%|███████▌  | 1142/1500 [00:49&lt;00:15, 23.72it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  76%|███████▋  | 1145/1500 [00:49&lt;00:14, 23.73it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  77%|███████▋  | 1148/1500 [00:49&lt;00:14, 23.69it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  77%|███████▋  | 1151/1500 [00:50&lt;00:14, 23.58it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  77%|███████▋  | 1154/1500 [00:50&lt;00:14, 23.52it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  77%|███████▋  | 1157/1500 [00:50&lt;00:14, 23.52it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  77%|███████▋  | 1160/1500 [00:50&lt;00:14, 23.58it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  78%|███████▊  | 1163/1500 [00:50&lt;00:13, 24.84it/s, 63 steps of size 3.42e-02. acc. prob=0.93] sample:  78%|███████▊  | 1166/1500 [00:50&lt;00:13, 24.35it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  78%|███████▊  | 1169/1500 [00:50&lt;00:13, 24.17it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  78%|███████▊  | 1172/1500 [00:50&lt;00:13, 24.00it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  78%|███████▊  | 1175/1500 [00:51&lt;00:13, 23.88it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  79%|███████▊  | 1178/1500 [00:51&lt;00:13, 23.86it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  79%|███████▊  | 1181/1500 [00:51&lt;00:13, 23.84it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  79%|███████▉  | 1184/1500 [00:51&lt;00:13, 23.82it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  79%|███████▉  | 1187/1500 [00:51&lt;00:13, 23.85it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  79%|███████▉  | 1190/1500 [00:51&lt;00:12, 23.87it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  80%|███████▉  | 1193/1500 [00:51&lt;00:12, 23.86it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  80%|███████▉  | 1196/1500 [00:51&lt;00:12, 23.87it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  80%|███████▉  | 1199/1500 [00:52&lt;00:12, 23.89it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  80%|████████  | 1202/1500 [00:52&lt;00:12, 23.88it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  80%|████████  | 1205/1500 [00:52&lt;00:12, 23.87it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  81%|████████  | 1208/1500 [00:52&lt;00:12, 23.85it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  81%|████████  | 1211/1500 [00:52&lt;00:13, 22.15it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  81%|████████  | 1214/1500 [00:52&lt;00:12, 22.58it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  81%|████████  | 1217/1500 [00:52&lt;00:11, 23.96it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  81%|████████▏ | 1220/1500 [00:52&lt;00:11, 23.81it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  82%|████████▏ | 1223/1500 [00:53&lt;00:11, 24.99it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  82%|████████▏ | 1226/1500 [00:53&lt;00:11, 24.61it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  82%|████████▏ | 1229/1500 [00:53&lt;00:11, 24.23it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  82%|████████▏ | 1232/1500 [00:53&lt;00:11, 24.03it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  82%|████████▏ | 1235/1500 [00:53&lt;00:11, 23.95it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  83%|████████▎ | 1238/1500 [00:53&lt;00:10, 23.87it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  83%|████████▎ | 1241/1500 [00:53&lt;00:10, 23.78it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  83%|████████▎ | 1244/1500 [00:53&lt;00:10, 23.78it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  83%|████████▎ | 1247/1500 [00:54&lt;00:10, 23.76it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  83%|████████▎ | 1250/1500 [00:54&lt;00:10, 23.74it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  84%|████████▎ | 1253/1500 [00:54&lt;00:10, 23.76it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  84%|████████▎ | 1256/1500 [00:54&lt;00:10, 23.76it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  84%|████████▍ | 1259/1500 [00:54&lt;00:10, 23.72it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  84%|████████▍ | 1262/1500 [00:54&lt;00:10, 23.67it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  84%|████████▍ | 1265/1500 [00:54&lt;00:09, 23.65it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  85%|████████▍ | 1268/1500 [00:54&lt;00:09, 23.64it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  85%|████████▍ | 1271/1500 [00:55&lt;00:09, 23.62it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  85%|████████▍ | 1274/1500 [00:55&lt;00:09, 23.65it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  85%|████████▌ | 1277/1500 [00:55&lt;00:09, 23.67it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  85%|████████▌ | 1280/1500 [00:55&lt;00:09, 23.74it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  86%|████████▌ | 1283/1500 [00:55&lt;00:09, 23.71it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  86%|████████▌ | 1286/1500 [00:55&lt;00:09, 23.74it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  86%|████████▌ | 1289/1500 [00:55&lt;00:08, 23.74it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  86%|████████▌ | 1292/1500 [00:55&lt;00:08, 23.76it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  86%|████████▋ | 1295/1500 [00:56&lt;00:08, 23.83it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  87%|████████▋ | 1298/1500 [00:56&lt;00:08, 23.90it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  87%|████████▋ | 1301/1500 [00:56&lt;00:08, 23.89it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  87%|████████▋ | 1304/1500 [00:56&lt;00:08, 23.89it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  87%|████████▋ | 1307/1500 [00:56&lt;00:08, 23.90it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  87%|████████▋ | 1310/1500 [00:56&lt;00:07, 23.95it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  88%|████████▊ | 1313/1500 [00:56&lt;00:07, 23.92it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  88%|████████▊ | 1316/1500 [00:56&lt;00:07, 23.92it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  88%|████████▊ | 1319/1500 [00:57&lt;00:07, 23.94it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  88%|████████▊ | 1322/1500 [00:57&lt;00:07, 23.91it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  88%|████████▊ | 1325/1500 [00:57&lt;00:07, 23.86it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  89%|████████▊ | 1328/1500 [00:57&lt;00:07, 23.88it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  89%|████████▊ | 1331/1500 [00:57&lt;00:07, 23.90it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  89%|████████▉ | 1334/1500 [00:57&lt;00:06, 23.89it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  89%|████████▉ | 1337/1500 [00:57&lt;00:06, 23.92it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  89%|████████▉ | 1340/1500 [00:57&lt;00:06, 23.94it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  90%|████████▉ | 1343/1500 [00:58&lt;00:06, 23.89it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  90%|████████▉ | 1346/1500 [00:58&lt;00:06, 23.84it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  90%|████████▉ | 1349/1500 [00:58&lt;00:06, 23.80it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  90%|█████████ | 1352/1500 [00:58&lt;00:06, 23.83it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  90%|█████████ | 1355/1500 [00:58&lt;00:06, 23.85it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  91%|█████████ | 1358/1500 [00:58&lt;00:05, 23.77it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  91%|█████████ | 1361/1500 [00:58&lt;00:05, 25.00it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  91%|█████████ | 1364/1500 [00:58&lt;00:05, 25.90it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  91%|█████████ | 1367/1500 [00:59&lt;00:05, 25.19it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  91%|█████████▏| 1370/1500 [00:59&lt;00:05, 24.71it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  92%|█████████▏| 1373/1500 [00:59&lt;00:05, 24.37it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  92%|█████████▏| 1376/1500 [00:59&lt;00:05, 24.18it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  92%|█████████▏| 1379/1500 [00:59&lt;00:05, 23.98it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  92%|█████████▏| 1382/1500 [00:59&lt;00:04, 23.87it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  92%|█████████▏| 1385/1500 [00:59&lt;00:04, 23.83it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  93%|█████████▎| 1388/1500 [00:59&lt;00:04, 25.00it/s, 63 steps of size 3.42e-02. acc. prob=0.93] sample:  93%|█████████▎| 1391/1500 [01:00&lt;00:04, 24.56it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  93%|█████████▎| 1394/1500 [01:00&lt;00:04, 24.26it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  93%|█████████▎| 1397/1500 [01:00&lt;00:04, 24.16it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  93%|█████████▎| 1400/1500 [01:00&lt;00:04, 24.12it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  94%|█████████▎| 1403/1500 [01:00&lt;00:03, 25.32it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  94%|█████████▎| 1406/1500 [01:00&lt;00:03, 26.20it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  94%|█████████▍| 1409/1500 [01:00&lt;00:03, 25.49it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  94%|█████████▍| 1412/1500 [01:00&lt;00:03, 24.98it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  94%|█████████▍| 1415/1500 [01:01&lt;00:03, 22.66it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  95%|█████████▍| 1418/1500 [01:01&lt;00:03, 22.95it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  95%|█████████▍| 1421/1500 [01:01&lt;00:03, 23.15it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  95%|█████████▍| 1424/1500 [01:01&lt;00:03, 23.09it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  95%|█████████▌| 1427/1500 [01:01&lt;00:03, 23.22it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  95%|█████████▌| 1430/1500 [01:01&lt;00:02, 23.36it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  96%|█████████▌| 1433/1500 [01:01&lt;00:02, 23.45it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  96%|█████████▌| 1436/1500 [01:01&lt;00:02, 23.55it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  96%|█████████▌| 1439/1500 [01:02&lt;00:02, 23.59it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  96%|█████████▌| 1442/1500 [01:02&lt;00:02, 23.53it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  96%|█████████▋| 1445/1500 [01:02&lt;00:02, 23.60it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  97%|█████████▋| 1448/1500 [01:02&lt;00:02, 23.69it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  97%|█████████▋| 1451/1500 [01:02&lt;00:02, 23.81it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  97%|█████████▋| 1454/1500 [01:02&lt;00:01, 23.86it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  97%|█████████▋| 1457/1500 [01:02&lt;00:01, 23.91it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  97%|█████████▋| 1460/1500 [01:02&lt;00:01, 23.96it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  98%|█████████▊| 1463/1500 [01:03&lt;00:01, 23.99it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  98%|█████████▊| 1466/1500 [01:03&lt;00:01, 23.97it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  98%|█████████▊| 1469/1500 [01:03&lt;00:01, 23.96it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  98%|█████████▊| 1472/1500 [01:03&lt;00:01, 25.22it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  98%|█████████▊| 1475/1500 [01:03&lt;00:01, 24.83it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  99%|█████████▊| 1478/1500 [01:03&lt;00:00, 25.89it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  99%|█████████▊| 1481/1500 [01:03&lt;00:00, 25.28it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  99%|█████████▉| 1484/1500 [01:03&lt;00:00, 24.82it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  99%|█████████▉| 1487/1500 [01:04&lt;00:00, 24.57it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample:  99%|█████████▉| 1490/1500 [01:04&lt;00:00, 24.38it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample: 100%|█████████▉| 1493/1500 [01:04&lt;00:00, 24.09it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample: 100%|█████████▉| 1496/1500 [01:04&lt;00:00, 23.95it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample: 100%|█████████▉| 1499/1500 [01:04&lt;00:00, 23.88it/s, 127 steps of size 3.42e-02. acc. prob=0.93]sample: 100%|██████████| 1500/1500 [01:04&lt;00:00, 23.23it/s, 127 steps of size 3.42e-02. acc. prob=0.93]\n\n\nProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3,\n                                                                              8],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[7, 365.25]),\n                                 '^$')],\n             inference_engine=MCMCInferenceEngine(num_samples=500,\n                                                  num_warmup=1000),\n             likelihood='negbinomial', noise_scale=10, scale=1,\n             trend=GenLogisticTrend())Please rerun this cell to show the HTML repr or trust the notebook.ProphetverseProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3,\n                                                                              8],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[7, 365.25]),\n                                 '^$')],\n             inference_engine=MCMCInferenceEngine(num_samples=500,\n                                                  num_warmup=1000),\n             likelihood='negbinomial', noise_scale=10, scale=1,\n             trend=GenLogisticTrend())effectsGenLogisticTrendGenLogisticTrend()LinearFourierSeasonalityLinearFourierSeasonality(effect_mode='multiplicative',\n                         fourier_terms_list=[3, 8], freq='D', prior_scale=0.1,\n                         sp_list=[7, 365.25])inference_engineMCMCInferenceEngineMCMCInferenceEngine(num_samples=500, num_warmup=1000)\n\n\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nfh = pd.period_range(y.index.min(), \"2026-01-01\")\npreds = model.predict(fh=fh)\ninterval = model.predict_interval(\n    fh=fh,\n    coverage=0.9,\n)\ndisplay(preds.head())\n\n/Users/felipeangelim/Workspace/prophetverse/src/prophetverse/sktime/univariate.py:126: FutureWarning: Warning: 'noise_scale' is deprecated and will be removed in version 0.7.0. Please update your code to avoid issues. Use the noise_scale parameter in the likelihood instead. You can import the likelihood from prophetverse.effects import NormalTargetLikelihood\n  deprecation_warning(\n/Users/felipeangelim/Workspace/prophetverse/src/prophetverse/sktime/univariate.py:126: FutureWarning: Warning: 'noise_scale' is deprecated and will be removed in version 0.7.0. Please update your code to avoid issues. Use the noise_scale parameter in the likelihood instead. You can import the likelihood from prophetverse.effects import NormalTargetLikelihood\n  deprecation_warning(\n\n\n\n\n\n\n\n\n\nday-stars\n\n\n\n\n2015-11-27\n53.167550\n\n\n2015-11-28\n38.729958\n\n\n2015-11-29\n38.421034\n\n\n2015-11-30\n52.242276\n\n\n2015-12-01\n54.248416\n\n\n\n\n\n\n\n\ninterval.columns = interval.columns.droplevel([0, 1])\ninterval.head()\n\n\n\n\n\n\n\n\nlower\nupper\n\n\n\n\n2015-11-27\n29.95\n79.00\n\n\n2015-11-28\n19.95\n60.00\n\n\n2015-11-29\n21.00\n59.05\n\n\n2015-11-30\n31.00\n82.00\n\n\n2015-12-01\n31.00\n82.00",
    "crumbs": [
      "Custom trend"
    ]
  },
  {
    "objectID": "howto/custom_trend.html#plotting-the-results",
    "href": "howto/custom_trend.html#plotting-the-results",
    "title": "Custom Trend in Prophetverse",
    "section": "Plotting the results",
    "text": "Plotting the results\n\n# Just the scatter of y, without lines\nfig, ax = plt.subplots(figsize=(12, 5))\nax = (\n    y[\"day-stars\"]\n    .rename(\"Observed\")\n    .plot.line(\n        marker=\"o\", linestyle=\"None\", legend=False, markersize=1, color=\"black\", ax=ax\n    )\n)\nax.axvline(y_train.index.max(), color=\"black\", zorder=-1, alpha=0.4, linewidth=1)\nax.spines[\"left\"].set_visible(False)\nax.spines[\"right\"].set_visible(False)\n# Superior\nax.spines[\"top\"].set_visible(False)\n\npreds[\"day-stars\"].rename(\"Forecast\").plot.line(\n    ax=ax, alpha=1, linewidth=0.5, legend=False\n)\n\nax.fill_between(\n    fh.to_timestamp(),\n    interval[\"lower\"],\n    interval[\"upper\"],\n    color=\"blue\",\n    alpha=0.2,\n    zorder=-1,\n    label=\"90% Credible Interval\",\n)\nfig.legend()\nfig.tight_layout()\nfig.show()\n\n/var/folders/_2/9y4tsvdd2n3gqjgd2zmlr1km0000gn/T/ipykernel_78608/3422318019.py:31: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n  fig.show()\n\n\n\n\n\n\n\n\n\n\n# Forecast samples\nyhat_samples = model.predict_samples(fh=fh)\n# Samples of all sites (capacity, for example, that we had set as deterministic with numpyro.deterministic)\nsite_samples = model.predict_component_samples(fh=fh)\n\n/Users/felipeangelim/Workspace/prophetverse/src/prophetverse/sktime/univariate.py:126: FutureWarning: Warning: 'noise_scale' is deprecated and will be removed in version 0.7.0. Please update your code to avoid issues. Use the noise_scale parameter in the likelihood instead. You can import the likelihood from prophetverse.effects import NormalTargetLikelihood\n  deprecation_warning(\n/Users/felipeangelim/Workspace/prophetverse/src/prophetverse/sktime/univariate.py:126: FutureWarning: Warning: 'noise_scale' is deprecated and will be removed in version 0.7.0. Please update your code to avoid issues. Use the noise_scale parameter in the likelihood instead. You can import the likelihood from prophetverse.effects import NormalTargetLikelihood\n  deprecation_warning(\n\n\n\nimport pandas as pd\n# Set number of columns to display to 4 temporarily\npd.set_option(\"display.max_columns\", 4)\nyhat_samples.head()\n\n\n\n\n\n\n\n\n0\n1\n...\n498\n499\n\n\n\n\n2015-11-27\n56\n68\n...\n94\n58\n\n\n2015-11-28\n12\n63\n...\n52\n23\n\n\n2015-11-29\n28\n33\n...\n34\n49\n\n\n2015-11-30\n54\n59\n...\n56\n49\n\n\n2015-12-01\n42\n41\n...\n29\n68\n\n\n\n\n5 rows × 500 columns\n\n\n\n\n# Get quantile 0.25, 0.75 and mean\nimport functools\nimport numpy as np\n\n\ndef q25(x):\n    return np.quantile(x, q=0.25)\n\n\ndef q75(x):\n    return np.quantile(x, q=0.75)\n\n\nsite_quantiles = site_samples.groupby(level=[-1]).agg(\n    [\n        np.mean,\n        q25,\n        q75,\n    ]\n)\nsite_quantiles.head()\n\n/var/folders/_2/9y4tsvdd2n3gqjgd2zmlr1km0000gn/T/ipykernel_78608/2904010725.py:14: FutureWarning: The provided callable &lt;function mean at 0x110266340&gt; is currently using SeriesGroupBy.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n  site_quantiles = site_samples.groupby(level=[-1]).agg(\n\n\n\n\n\n\n\n\n\nmean\n...\ntrend/capacity\n\n\n\nmean\nq25\n...\nq25\nq75\n\n\n\n\n2015-11-27\n53.167550\n51.643131\n...\n42214.150393\n79390.725753\n\n\n2015-11-28\n38.729958\n37.471232\n...\n42257.753786\n79417.386110\n\n\n2015-11-29\n38.421034\n37.205271\n...\n42301.357178\n79444.046467\n\n\n2015-11-30\n52.242276\n50.809756\n...\n42344.960570\n79470.706825\n\n\n2015-12-01\n54.248416\n52.783770\n...\n42388.563962\n79497.367182\n\n\n\n\n5 rows × 18 columns\n\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfig, ax = plt.subplots(figsize=(12, 5))\n\n# Plot true value\nax.plot(y.index.to_timestamp(), y.cumsum(), label=\"Observed\")\n\n# Train test split\nax.axvline(y_train.index.max(), color=\"black\", alpha=0.8, zorder=-1, linewidth=1)\n\n\n# Capacity asymptotic\nax.fill_between(\n    fh.to_timestamp(),\n    site_quantiles.loc[:, (\"trend/capacity\", \"q25\")],\n    site_quantiles.loc[:, (\"trend/capacity\", \"q75\")],\n    color=\"red\",\n    alpha=0.1,\n    zorder=-1,\n    label=\"Asymptotic capacity\",\n)\nax.plot(\n    fh.to_timestamp(),\n    site_quantiles.loc[:, (\"trend/capacity\", \"mean\")],\n    color=\"red\",\n    alpha=0.2,\n    linestyle=\"--\",\n    zorder=-1,\n    linewidth=0.9,\n)\n\n\n# Plot some random samples\nidxs = np.random.choice(yhat_samples.columns, 10)\n\nfor i, idx in enumerate(idxs):\n    kwargs = {}\n    if i == 0:\n        kwargs[\"label\"] = \"MCMC Samples\"\n    ax.plot(\n        fh.to_timestamp(),\n        yhat_samples.cumsum().loc[:, idx],\n        color=\"black\",\n        alpha=0.1,\n        linewidth=1,\n        **kwargs,\n    )\n\nalpha = 0.1\nupper_and_lower_cumsum = (\n    yhat_samples.cumsum().quantile([alpha / 2, 1 - alpha / 2], axis=1).T\n)\n\n\nax.fill_between(\n    upper_and_lower_cumsum.index.to_timestamp(),\n    upper_and_lower_cumsum.iloc[:, 0],\n    upper_and_lower_cumsum.iloc[:, 1],\n    alpha=0.5,\n)\nax.grid(alpha=0.2)\nax.spines[\"left\"].set_visible(False)\nax.spines[\"right\"].set_visible(False)\n# Superior\nax.spines[\"top\"].set_visible(False)\n\nax.set_xlim(fh.to_timestamp().min(), fh.to_timestamp().max())\n\n# Add samples to legend\n\nfig.legend()\nax.set_title(\"Total number of stars (forecast)\")\nfig.show()\n\n/var/folders/_2/9y4tsvdd2n3gqjgd2zmlr1km0000gn/T/ipykernel_78608/4137021836.py:74: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n  fig.show()",
    "crumbs": [
      "Custom trend"
    ]
  },
  {
    "objectID": "tutorial/tuning.html",
    "href": "tutorial/tuning.html",
    "title": "Tuning Prophetverse with sktime",
    "section": "",
    "text": "This guide explains how to optimize Prophetverse model hyperparameters using sktime’s tuning classes (e.g., GridSearchCV).\n\n\nProphetverse is compatible with sktime’s tuning framework. You can define a parameter grid for components (such as trend and seasonality) and then use cross-validation tools (e.g., GridSearchCV) to search for the best parameters.\n\n\n\n\nImport necessary modules and load your dataset.\nDefine the hyperparameter grid for components (e.g., changepoint_interval and changepoint_prior_scale in the trend).\nCreate a Prophetverse instance with initial settings.\nWrap the model with sktime’s GridSearchCV and run the tuning process.\n\n\nimport pandas as pd\nfrom sktime.forecasting.model_selection import ForecastingGridSearchCV\nfrom prophetverse.sktime import Prophetverse\nfrom prophetverse.effects.trend import PiecewiseLinearTrend\nfrom prophetverse.effects.fourier import LinearFourierSeasonality\nfrom prophetverse.engine import MAPInferenceEngine\nfrom prophetverse.utils import no_input_columns\n\n# Load example dataset (replace with your own data as needed)\nfrom prophetverse.datasets.loaders import load_peyton_manning\n\ny = load_peyton_manning()\n\n# Define the hyperparameter grid for the trend component.\nparam_grid = {\n    \"trend__changepoint_interval\": [300, 700],\n    \"trend__changepoint_prior_scale\": [0.0001, 0.00001],\n    \"seasonality__prior_scale\": [0.1],\n}\n\n# Create the initial Prophetverse model.\nmodel = Prophetverse(\n    trend=PiecewiseLinearTrend(\n        changepoint_interval=500,\n        changepoint_prior_scale=0.00001,\n        changepoint_range=-250,\n    ),\n    exogenous_effects=[\n        (\n            \"seasonality\",\n            LinearFourierSeasonality(\n                freq=\"D\",\n                sp_list=[7, 365.25],\n                fourier_terms_list=[3, 10],\n                prior_scale=0.1,\n                effect_mode=\"multiplicative\",\n            ),\n            no_input_columns,\n        ),\n    ],\n    inference_engine=MAPInferenceEngine(),\n)\n\n# Set up cv strategy\nfrom sktime.split import ExpandingWindowSplitter\n\ncv = ExpandingWindowSplitter(fh=[1, 2, 3], step_length=1000, initial_window=1000)\n\n# Set up GridSearchCV with 3-fold cross-validation.\ngrid_search = ForecastingGridSearchCV(model, param_grid=param_grid, cv=cv)\n\n# Run the grid search.\ngrid_search.fit(y=y, X=None)\n\n# Display the best parameters found.\nprint(\"Best parameters:\", grid_search.best_params_)\n\n/Users/felipeangelim/Workspace/prophetverse/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n/Users/felipeangelim/Workspace/prophetverse/.venv/lib/python3.11/site-packages/joblib/externals/loky/backend/fork_exec.py:38: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid = os.fork()\n/Users/felipeangelim/Workspace/prophetverse/src/prophetverse/sktime/univariate.py:214: UserWarning: No columns match the regex ^$\n  self._fit_effects(X, y)\n/Users/felipeangelim/Workspace/prophetverse/src/prophetverse/sktime/univariate.py:214: UserWarning: No columns match the regex ^$\n  self._fit_effects(X, y)\n/Users/felipeangelim/Workspace/prophetverse/src/prophetverse/sktime/univariate.py:214: UserWarning: No columns match the regex ^$\n  self._fit_effects(X, y)\n/Users/felipeangelim/Workspace/prophetverse/src/prophetverse/sktime/univariate.py:214: UserWarning: No columns match the regex ^$\n  self._fit_effects(X, y)\n/Users/felipeangelim/Workspace/prophetverse/src/prophetverse/sktime/univariate.py:214: UserWarning: No columns match the regex ^$\n  self._fit_effects(X, y)\n/Users/felipeangelim/Workspace/prophetverse/src/prophetverse/sktime/univariate.py:214: UserWarning: No columns match the regex ^$\n  self._fit_effects(X, y)\n/Users/felipeangelim/Workspace/prophetverse/src/prophetverse/sktime/univariate.py:214: UserWarning: No columns match the regex ^$\n  self._fit_effects(X, y)\n/Users/felipeangelim/Workspace/prophetverse/src/prophetverse/sktime/univariate.py:214: UserWarning: No columns match the regex ^$\n  self._fit_effects(X, y)\n/Users/felipeangelim/Workspace/prophetverse/src/prophetverse/sktime/univariate.py:214: UserWarning: No columns match the regex ^$\n  self._fit_effects(X, y)\n/Users/felipeangelim/Workspace/prophetverse/src/prophetverse/sktime/univariate.py:214: UserWarning: No columns match the regex ^$\n  self._fit_effects(X, y)\n\n\nBest parameters: {'seasonality__prior_scale': 0.1, 'trend__changepoint_interval': 700, 'trend__changepoint_prior_scale': 1e-05}\n\n\n\nbest_model = grid_search.best_forecaster_\nbest_model\n\nProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3,\n                                                                              10],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[7, 365.25]),\n                                 '^$')],\n             inference_engine=MAPInferenceEngine(),\n             trend=PiecewiseLinearTrend(changepoint_interval=700,\n                                        changepoint_prior_scale=1e-05,\n                                        changepoint_range=-250))Please rerun this cell to show the HTML repr or trust the notebook.ProphetverseProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3,\n                                                                              10],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[7, 365.25]),\n                                 '^$')],\n             inference_engine=MAPInferenceEngine(),\n             trend=PiecewiseLinearTrend(changepoint_interval=700,\n                                        changepoint_prior_scale=1e-05,\n                                        changepoint_range=-250))effectsPiecewiseLinearTrendPiecewiseLinearTrend(changepoint_interval=700, changepoint_prior_scale=1e-05,\n                     changepoint_range=-250)LinearFourierSeasonalityLinearFourierSeasonality(effect_mode='multiplicative',\n                         fourier_terms_list=[3, 10], freq='D', prior_scale=0.1,\n                         sp_list=[7, 365.25])inference_engineMAPInferenceEngineMAPInferenceEngine()",
    "crumbs": [
      "Home",
      "Tutorial",
      "Tuning Prophetverse with sktime"
    ]
  },
  {
    "objectID": "tutorial/tuning.html#overview",
    "href": "tutorial/tuning.html#overview",
    "title": "Tuning Prophetverse with sktime",
    "section": "",
    "text": "Prophetverse is compatible with sktime’s tuning framework. You can define a parameter grid for components (such as trend and seasonality) and then use cross-validation tools (e.g., GridSearchCV) to search for the best parameters.",
    "crumbs": [
      "Home",
      "Tutorial",
      "Tuning Prophetverse with sktime"
    ]
  },
  {
    "objectID": "tutorial/tuning.html#example-using-gridsearchcv-with-prophetverse",
    "href": "tutorial/tuning.html#example-using-gridsearchcv-with-prophetverse",
    "title": "Tuning Prophetverse with sktime",
    "section": "",
    "text": "Import necessary modules and load your dataset.\nDefine the hyperparameter grid for components (e.g., changepoint_interval and changepoint_prior_scale in the trend).\nCreate a Prophetverse instance with initial settings.\nWrap the model with sktime’s GridSearchCV and run the tuning process.\n\n\nimport pandas as pd\nfrom sktime.forecasting.model_selection import ForecastingGridSearchCV\nfrom prophetverse.sktime import Prophetverse\nfrom prophetverse.effects.trend import PiecewiseLinearTrend\nfrom prophetverse.effects.fourier import LinearFourierSeasonality\nfrom prophetverse.engine import MAPInferenceEngine\nfrom prophetverse.utils import no_input_columns\n\n# Load example dataset (replace with your own data as needed)\nfrom prophetverse.datasets.loaders import load_peyton_manning\n\ny = load_peyton_manning()\n\n# Define the hyperparameter grid for the trend component.\nparam_grid = {\n    \"trend__changepoint_interval\": [300, 700],\n    \"trend__changepoint_prior_scale\": [0.0001, 0.00001],\n    \"seasonality__prior_scale\": [0.1],\n}\n\n# Create the initial Prophetverse model.\nmodel = Prophetverse(\n    trend=PiecewiseLinearTrend(\n        changepoint_interval=500,\n        changepoint_prior_scale=0.00001,\n        changepoint_range=-250,\n    ),\n    exogenous_effects=[\n        (\n            \"seasonality\",\n            LinearFourierSeasonality(\n                freq=\"D\",\n                sp_list=[7, 365.25],\n                fourier_terms_list=[3, 10],\n                prior_scale=0.1,\n                effect_mode=\"multiplicative\",\n            ),\n            no_input_columns,\n        ),\n    ],\n    inference_engine=MAPInferenceEngine(),\n)\n\n# Set up cv strategy\nfrom sktime.split import ExpandingWindowSplitter\n\ncv = ExpandingWindowSplitter(fh=[1, 2, 3], step_length=1000, initial_window=1000)\n\n# Set up GridSearchCV with 3-fold cross-validation.\ngrid_search = ForecastingGridSearchCV(model, param_grid=param_grid, cv=cv)\n\n# Run the grid search.\ngrid_search.fit(y=y, X=None)\n\n# Display the best parameters found.\nprint(\"Best parameters:\", grid_search.best_params_)\n\n/Users/felipeangelim/Workspace/prophetverse/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n/Users/felipeangelim/Workspace/prophetverse/.venv/lib/python3.11/site-packages/joblib/externals/loky/backend/fork_exec.py:38: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid = os.fork()\n/Users/felipeangelim/Workspace/prophetverse/src/prophetverse/sktime/univariate.py:214: UserWarning: No columns match the regex ^$\n  self._fit_effects(X, y)\n/Users/felipeangelim/Workspace/prophetverse/src/prophetverse/sktime/univariate.py:214: UserWarning: No columns match the regex ^$\n  self._fit_effects(X, y)\n/Users/felipeangelim/Workspace/prophetverse/src/prophetverse/sktime/univariate.py:214: UserWarning: No columns match the regex ^$\n  self._fit_effects(X, y)\n/Users/felipeangelim/Workspace/prophetverse/src/prophetverse/sktime/univariate.py:214: UserWarning: No columns match the regex ^$\n  self._fit_effects(X, y)\n/Users/felipeangelim/Workspace/prophetverse/src/prophetverse/sktime/univariate.py:214: UserWarning: No columns match the regex ^$\n  self._fit_effects(X, y)\n/Users/felipeangelim/Workspace/prophetverse/src/prophetverse/sktime/univariate.py:214: UserWarning: No columns match the regex ^$\n  self._fit_effects(X, y)\n/Users/felipeangelim/Workspace/prophetverse/src/prophetverse/sktime/univariate.py:214: UserWarning: No columns match the regex ^$\n  self._fit_effects(X, y)\n/Users/felipeangelim/Workspace/prophetverse/src/prophetverse/sktime/univariate.py:214: UserWarning: No columns match the regex ^$\n  self._fit_effects(X, y)\n/Users/felipeangelim/Workspace/prophetverse/src/prophetverse/sktime/univariate.py:214: UserWarning: No columns match the regex ^$\n  self._fit_effects(X, y)\n/Users/felipeangelim/Workspace/prophetverse/src/prophetverse/sktime/univariate.py:214: UserWarning: No columns match the regex ^$\n  self._fit_effects(X, y)\n\n\nBest parameters: {'seasonality__prior_scale': 0.1, 'trend__changepoint_interval': 700, 'trend__changepoint_prior_scale': 1e-05}\n\n\n\nbest_model = grid_search.best_forecaster_\nbest_model\n\nProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3,\n                                                                              10],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[7, 365.25]),\n                                 '^$')],\n             inference_engine=MAPInferenceEngine(),\n             trend=PiecewiseLinearTrend(changepoint_interval=700,\n                                        changepoint_prior_scale=1e-05,\n                                        changepoint_range=-250))Please rerun this cell to show the HTML repr or trust the notebook.ProphetverseProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3,\n                                                                              10],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[7, 365.25]),\n                                 '^$')],\n             inference_engine=MAPInferenceEngine(),\n             trend=PiecewiseLinearTrend(changepoint_interval=700,\n                                        changepoint_prior_scale=1e-05,\n                                        changepoint_range=-250))effectsPiecewiseLinearTrendPiecewiseLinearTrend(changepoint_interval=700, changepoint_prior_scale=1e-05,\n                     changepoint_range=-250)LinearFourierSeasonalityLinearFourierSeasonality(effect_mode='multiplicative',\n                         fourier_terms_list=[3, 10], freq='D', prior_scale=0.1,\n                         sp_list=[7, 365.25])inference_engineMAPInferenceEngineMAPInferenceEngine()",
    "crumbs": [
      "Home",
      "Tutorial",
      "Tuning Prophetverse with sktime"
    ]
  },
  {
    "objectID": "tutorials/tuning.html",
    "href": "tutorials/tuning.html",
    "title": "Tuning Prophetverse with sktime",
    "section": "",
    "text": "Prophetverse is compatible with sktime’s tuning framework. You can define a parameter grid for components (such as trend and seasonality) and then use cross-validation tools (e.g., GridSearchCV) to search for the best parameters.",
    "crumbs": [
      "Hyperparameter tuning"
    ]
  },
  {
    "objectID": "tutorials/tuning.html#overview",
    "href": "tutorials/tuning.html#overview",
    "title": "Tuning Prophetverse with sktime",
    "section": "",
    "text": "Prophetverse is compatible with sktime’s tuning framework. You can define a parameter grid for components (such as trend and seasonality) and then use cross-validation tools (e.g., GridSearchCV) to search for the best parameters.",
    "crumbs": [
      "Hyperparameter tuning"
    ]
  },
  {
    "objectID": "tutorials/tuning.html#example-using-gridsearchcv-with-prophetverse",
    "href": "tutorials/tuning.html#example-using-gridsearchcv-with-prophetverse",
    "title": "Tuning Prophetverse with sktime",
    "section": "Example: Using GridSearchCV with Prophetverse",
    "text": "Example: Using GridSearchCV with Prophetverse\n\nImport necessary modules and load your dataset.\nDefine the hyperparameter grid for components (e.g., changepoint_interval and changepoint_prior_scale in the trend).\nCreate a Prophetverse instance with initial settings.\nWrap the model with sktime’s GridSearchCV and run the tuning process.\n\n\nLoading the data\n\nimport pandas as pd\nfrom sktime.forecasting.model_selection import ForecastingGridSearchCV\nfrom prophetverse.sktime import Prophetverse\nfrom prophetverse.effects.trend import PiecewiseLinearTrend\nfrom prophetverse.effects.fourier import LinearFourierSeasonality\nfrom prophetverse.engine import MAPInferenceEngine\nfrom prophetverse.utils import no_input_columns\n\n# Load example dataset (replace with your own data as needed)\nfrom prophetverse.datasets.loaders import load_peyton_manning\n\ny = load_peyton_manning()\ny.head()\n\n\n\n\n\n\n\n\ny\n\n\nds\n\n\n\n\n\n2007-12-10\n9.590761\n\n\n2007-12-11\n8.519590\n\n\n2007-12-12\n8.183677\n\n\n2007-12-13\n8.072467\n\n\n2007-12-14\n7.893572\n\n\n\n\n\n\n\n\n\nSetting the model\nWe create our model instance, before passing it to tuning.\n\n# Create the initial Prophetverse model.\nmodel = Prophetverse(\n    trend=PiecewiseLinearTrend(\n        changepoint_interval=500,\n        changepoint_prior_scale=0.00001,\n        changepoint_range=-250,\n    ),\n    exogenous_effects=[\n        (\n            \"seasonality\",\n            LinearFourierSeasonality(\n                freq=\"D\",\n                sp_list=[7, 365.25],\n                fourier_terms_list=[3, 10],\n                prior_scale=0.1,\n                effect_mode=\"multiplicative\",\n            ),\n            no_input_columns,\n        ),\n    ],\n    inference_engine=MAPInferenceEngine(),\n)\nmodel\n\nProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3,\n                                                                              10],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[7, 365.25]),\n                                 '^$')],\n             inference_engine=MAPInferenceEngine(),\n             trend=PiecewiseLinearTrend(changepoint_interval=500,\n                                        changepoint_prior_scale=1e-05,\n                                        changepoint_range=-250))Please rerun this cell to show the HTML repr or trust the notebook.ProphetverseProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3,\n                                                                              10],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[7, 365.25]),\n                                 '^$')],\n             inference_engine=MAPInferenceEngine(),\n             trend=PiecewiseLinearTrend(changepoint_interval=500,\n                                        changepoint_prior_scale=1e-05,\n                                        changepoint_range=-250))effectsPiecewiseLinearTrendPiecewiseLinearTrend(changepoint_interval=500, changepoint_prior_scale=1e-05,\n                     changepoint_range=-250)LinearFourierSeasonalityLinearFourierSeasonality(effect_mode='multiplicative',\n                         fourier_terms_list=[3, 10], freq='D', prior_scale=0.1,\n                         sp_list=[7, 365.25])inference_engineMAPInferenceEngineMAPInferenceEngine()\n\n\n\n\nDefine the searcher\nIn sktime, the tuner is also an estimator/forecaster, so we can use the same interface as for any other sktime forecaster. We can use GridSearchCV to search for the best parameters in a given parameter grid.\n\n# Set up cv strategy\nfrom sktime.split import ExpandingWindowSplitter\n\ncv = ExpandingWindowSplitter(fh=[1, 2, 3], step_length=1000, initial_window=1000)\n\nparam_grid = {\n    \"trend__changepoint_interval\": [300, 700],\n    \"trend__changepoint_prior_scale\": [0.0001, 0.00001],\n    \"seasonality__prior_scale\": [0.1],\n}\n\n\n# Set up GridSearchCV with 3-fold cross-validation.\ngrid_search = ForecastingGridSearchCV(\n                model,\n                param_grid=param_grid,\n                cv=cv\n            )\ngrid_search\n\nForecastingGridSearchCV(cv=ExpandingWindowSplitter(fh=[1, 2, 3],\n                                                   initial_window=1000,\n                                                   step_length=1000),\n                        forecaster=Prophetverse(exogenous_effects=[('seasonality',\n                                                                    LinearFourierSeasonality(effect_mode='multiplicative',\n                                                                                             fourier_terms_list=[3,\n                                                                                                                 10],\n                                                                                             freq='D',\n                                                                                             prior_scale=0.1,\n                                                                                             sp_list=[7,\n                                                                                                      365.25]),\n                                                                    '^$')],\n                                                inference_engine=MAPInferenceEngine(),\n                                                trend=PiecewiseLinearTrend(changepoint_interval=500,\n                                                                           changepoint_prior_scale=1e-05,\n                                                                           changepoint_range=-250)),\n                        param_grid={'seasonality__prior_scale': [0.1],\n                                    'trend__changepoint_interval': [300, 700],\n                                    'trend__changepoint_prior_scale': [0.0001,\n                                                                       1e-05]})Please rerun this cell to show the HTML repr or trust the notebook.ForecastingGridSearchCV?Documentation for ForecastingGridSearchCVForecastingGridSearchCV(cv=ExpandingWindowSplitter(fh=[1, 2, 3],\n                                                   initial_window=1000,\n                                                   step_length=1000),\n                        forecaster=Prophetverse(exogenous_effects=[('seasonality',\n                                                                    LinearFourierSeasonality(effect_mode='multiplicative',\n                                                                                             fourier_terms_list=[3,\n                                                                                                                 10],\n                                                                                             freq='D',\n                                                                                             prior_scale=0.1,\n                                                                                             sp_list=[7,\n                                                                                                      365.25]),\n                                                                    '^$')],\n                                                inference_engine=MAPInferenceEngine(),\n                                                trend=PiecewiseLinearTrend(changepoint_interval=500,\n                                                                           changepoint_prior_scale=1e-05,\n                                                                           changepoint_range=-250)),\n                        param_grid={'seasonality__prior_scale': [0.1],\n                                    'trend__changepoint_interval': [300, 700],\n                                    'trend__changepoint_prior_scale': [0.0001,\n                                                                       1e-05]})forecaster: ProphetverseProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3,\n                                                                              10],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[7, 365.25]),\n                                 '^$')],\n             inference_engine=MAPInferenceEngine(),\n             trend=PiecewiseLinearTrend(changepoint_interval=500,\n                                        changepoint_prior_scale=1e-05,\n                                        changepoint_range=-250))effectsPiecewiseLinearTrendPiecewiseLinearTrend(changepoint_interval=500, changepoint_prior_scale=1e-05,\n                     changepoint_range=-250)LinearFourierSeasonalityLinearFourierSeasonality(effect_mode='multiplicative',\n                         fourier_terms_list=[3, 10], freq='D', prior_scale=0.1,\n                         sp_list=[7, 365.25])inference_engineMAPInferenceEngineMAPInferenceEngine()\n\n\nNow, we can call fit.\n\n# Run the grid search.\ngrid_search.fit(y=y, X=None)\n\n# Display the best parameters found.\nprint(\"Best parameters:\", grid_search.best_params_)\n\n/Users/felipeangelim/Workspace/prophetverse/src/prophetverse/sktime/univariate.py:214: UserWarning: No columns match the regex ^$\n  self._fit_effects(X, y)\n/Users/felipeangelim/Workspace/prophetverse/src/prophetverse/sktime/univariate.py:214: UserWarning: No columns match the regex ^$\n  self._fit_effects(X, y)\n/Users/felipeangelim/Workspace/prophetverse/src/prophetverse/sktime/univariate.py:214: UserWarning: No columns match the regex ^$\n  self._fit_effects(X, y)\n/Users/felipeangelim/Workspace/prophetverse/src/prophetverse/sktime/univariate.py:214: UserWarning: No columns match the regex ^$\n  self._fit_effects(X, y)\n/Users/felipeangelim/Workspace/prophetverse/src/prophetverse/sktime/univariate.py:214: UserWarning: No columns match the regex ^$\n  self._fit_effects(X, y)\n/Users/felipeangelim/Workspace/prophetverse/src/prophetverse/sktime/univariate.py:214: UserWarning: No columns match the regex ^$\n  self._fit_effects(X, y)\n/Users/felipeangelim/Workspace/prophetverse/src/prophetverse/sktime/univariate.py:214: UserWarning: No columns match the regex ^$\n  self._fit_effects(X, y)\n/Users/felipeangelim/Workspace/prophetverse/src/prophetverse/sktime/univariate.py:214: UserWarning: No columns match the regex ^$\n  self._fit_effects(X, y)\n\n\nBest parameters: {'seasonality__prior_scale': 0.1, 'trend__changepoint_interval': 700, 'trend__changepoint_prior_scale': 1e-05}\n\n\nWe can also see the performance of each parameter combination:\n\ngrid_search.cv_results_\n\n\n\n\n\n\n\n\nmean_test_MeanAbsolutePercentageError\nmean_fit_time\nmean_pred_time\nparams\nrank_test_MeanAbsolutePercentageError\n\n\n\n\n0\n0.053895\n3.284044\n0.209378\n{'seasonality__prior_scale': 0.1, 'trend__chan...\n4.0\n\n\n1\n0.049950\n3.293722\n0.208157\n{'seasonality__prior_scale': 0.1, 'trend__chan...\n3.0\n\n\n2\n0.036170\n3.184667\n0.208334\n{'seasonality__prior_scale': 0.1, 'trend__chan...\n2.0\n\n\n3\n0.033396\n5.177941\n0.210713\n{'seasonality__prior_scale': 0.1, 'trend__chan...\n1.0\n\n\n\n\n\n\n\nOptionally, extract the best model from the grid search results.\n\nbest_model = grid_search.best_forecaster_\nbest_model\n\nProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3,\n                                                                              10],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[7, 365.25]),\n                                 '^$')],\n             inference_engine=MAPInferenceEngine(),\n             trend=PiecewiseLinearTrend(changepoint_interval=700,\n                                        changepoint_prior_scale=1e-05,\n                                        changepoint_range=-250))Please rerun this cell to show the HTML repr or trust the notebook.ProphetverseProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3,\n                                                                              10],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[7, 365.25]),\n                                 '^$')],\n             inference_engine=MAPInferenceEngine(),\n             trend=PiecewiseLinearTrend(changepoint_interval=700,\n                                        changepoint_prior_scale=1e-05,\n                                        changepoint_range=-250))effectsPiecewiseLinearTrendPiecewiseLinearTrend(changepoint_interval=700, changepoint_prior_scale=1e-05,\n                     changepoint_range=-250)LinearFourierSeasonalityLinearFourierSeasonality(effect_mode='multiplicative',\n                         fourier_terms_list=[3, 10], freq='D', prior_scale=0.1,\n                         sp_list=[7, 365.25])inference_engineMAPInferenceEngineMAPInferenceEngine()",
    "crumbs": [
      "Hyperparameter tuning"
    ]
  },
  {
    "objectID": "tutorials/univariate.html",
    "href": "tutorials/univariate.html",
    "title": "Basic Univariate Forecasting with Prophetverse",
    "section": "",
    "text": "This example shows how to use Prophetverse to perform univariate forecasting with a time series dataset, using sktime-style interface.\nBecause of this compatibility, you can benefit from all the features of sktime, such as hierarchical reconciliation, ensemble models, pipelines, etc. There are two main methods to use Prophetverse with sktime:\nLater in this example, we will also show additional methods to make predictions, such as predict_quantiles and predict_components.\nimport warnings\nwarnings.simplefilter(action=\"ignore\")\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom numpyro import distributions as dist\nimport numpyro\n\nnumpyro.enable_x64()  # To avoid computational issues",
    "crumbs": [
      "Univariate forecasting"
    ]
  },
  {
    "objectID": "tutorials/univariate.html#import-dataset",
    "href": "tutorials/univariate.html#import-dataset",
    "title": "Basic Univariate Forecasting with Prophetverse",
    "section": "Import dataset",
    "text": "Import dataset\nWe import a dataset from Prophet’s original repository. We then put it into sktime-friendly format, where the index is a pd.PeriodIndex and the colums are the time series.\n\nfrom prophetverse.datasets.loaders import load_peyton_manning\n\ny = load_peyton_manning()\ndisplay(y.head())\n\n\n\n\n\n\n\n\ny\n\n\nds\n\n\n\n\n\n2007-12-10\n9.590761\n\n\n2007-12-11\n8.519590\n\n\n2007-12-12\n8.183677\n\n\n2007-12-13\n8.072467\n\n\n2007-12-14\n7.893572\n\n\n\n\n\n\n\nThe full dataset looks like this:\n\ny.plot.line(figsize=(8, 6))\nplt.show()",
    "crumbs": [
      "Univariate forecasting"
    ]
  },
  {
    "objectID": "tutorials/univariate.html#fit-model",
    "href": "tutorials/univariate.html#fit-model",
    "title": "Basic Univariate Forecasting with Prophetverse",
    "section": "Fit model",
    "text": "Fit model\nHere, we will show how you can fit a simple model with Prophetverse. We first fit a model without seasonal components, and then fit a full model. We also show how easy it is to switch between Maximum A Posteriori (MAP) inference and Markov Chain Monte Carlo (MCMC).\n\nNo seasonality\n\nfrom prophetverse.effects.trend import PiecewiseLinearTrend\nfrom prophetverse.engine import MAPInferenceEngine\nfrom prophetverse.sktime import Prophetverse\nfrom prophetverse.utils import no_input_columns\n\nmodel = Prophetverse(\n    trend=PiecewiseLinearTrend(\n        changepoint_interval=500,\n        changepoint_prior_scale=0.00001,\n        changepoint_range=-250,\n    ),\n    inference_engine=MAPInferenceEngine(),\n)\nmodel.fit(y=y)\n\nProphetverse(inference_engine=MAPInferenceEngine(),\n             trend=PiecewiseLinearTrend(changepoint_interval=500,\n                                        changepoint_prior_scale=1e-05,\n                                        changepoint_range=-250))Please rerun this cell to show the HTML repr or trust the notebook.ProphetverseProphetverse(inference_engine=MAPInferenceEngine(),\n             trend=PiecewiseLinearTrend(changepoint_interval=500,\n                                        changepoint_prior_scale=1e-05,\n                                        changepoint_range=-250))effectsPiecewiseLinearTrendPiecewiseLinearTrend(changepoint_interval=500, changepoint_prior_scale=1e-05,\n                     changepoint_range=-250)inference_engineMAPInferenceEngineMAPInferenceEngine()\n\n\n\nforecast_horizon = pd.period_range(\"2007-01-01\", \"2018-01-01\", freq=\"D\")\nfig, ax = plt.subplots(figsize=(8, 5))\npreds = model.predict(fh=forecast_horizon)\npreds.plot.line(ax=ax)\nax.scatter(y.index, y, marker=\"o\", color=\"k\", s=2, alpha=0.5)\n\n\n\n\n\n\n\n\n\n\nWith seasonality\nHere, we fit the univariate Prophet and pass an exogenous effect as hyperparameter. The exogenous_effects parameter let us add new components to the model and control the relationship between exogenous variables and the target variable.\nIn this case, the LinearFourierSeasonality effect creates sinusoidal and cosine terms to model the seasonality of the time series, which are then multiplied by linear coefficients and added to the model.\nThis argument is a list of tuples of the form (effect_name, effect, regex_to_filter_relevant_columns), where effect_name is a string and effect is an instance of a subclass of prophetverse.effects.BaseEffect. The regex is used to filter the columns of X that are relevant for the effect, but can also be None (or its alias prophetverse.utils.no_input_columns) if no input in X is needed for the effect.\n\nfrom prophetverse.effects.fourier import LinearFourierSeasonality\nfrom prophetverse.utils import no_input_columns\n\nmodel = Prophetverse(\n    trend=PiecewiseLinearTrend(\n        changepoint_interval=500,\n        changepoint_prior_scale=0.00001,\n        changepoint_range=-500,\n    ),\n    exogenous_effects=[\n        (\n            \"seasonality\",\n            LinearFourierSeasonality(\n                freq=\"D\",\n                sp_list=[7, 365.25],\n                fourier_terms_list=[3, 10],\n                prior_scale=0.1,\n                effect_mode=\"multiplicative\",\n            ),\n            no_input_columns,\n        ),\n    ],\n    inference_engine=MAPInferenceEngine(),\n)\nmodel.fit(y=y)\n\nProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3,\n                                                                              10],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[7, 365.25]),\n                                 '^$')],\n             inference_engine=MAPInferenceEngine(),\n             trend=PiecewiseLinearTrend(changepoint_interval=500,\n                                        changepoint_prior_scale=1e-05,\n                                        changepoint_range=-500))Please rerun this cell to show the HTML repr or trust the notebook.ProphetverseProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3,\n                                                                              10],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[7, 365.25]),\n                                 '^$')],\n             inference_engine=MAPInferenceEngine(),\n             trend=PiecewiseLinearTrend(changepoint_interval=500,\n                                        changepoint_prior_scale=1e-05,\n                                        changepoint_range=-500))effectsPiecewiseLinearTrendPiecewiseLinearTrend(changepoint_interval=500, changepoint_prior_scale=1e-05,\n                     changepoint_range=-500)LinearFourierSeasonalityLinearFourierSeasonality(effect_mode='multiplicative',\n                         fourier_terms_list=[3, 10], freq='D', prior_scale=0.1,\n                         sp_list=[7, 365.25])inference_engineMAPInferenceEngineMAPInferenceEngine()\n\n\n\nforecast_horizon = pd.period_range(\"2007-01-01\", \"2018-01-01\", freq=\"D\")\nfig, ax = plt.subplots(figsize=(8, 5))\npreds = model.predict(fh=forecast_horizon)\npreds.plot.line(ax=ax)\nax.scatter(y.index, y, marker=\"o\", color=\"k\", s=2, alpha=0.5)",
    "crumbs": [
      "Univariate forecasting"
    ]
  },
  {
    "objectID": "tutorials/univariate.html#probabilistic-forecasting",
    "href": "tutorials/univariate.html#probabilistic-forecasting",
    "title": "Basic Univariate Forecasting with Prophetverse",
    "section": "Probabilistic forecasting",
    "text": "Probabilistic forecasting\nWe can also make probabilistic forecasts with Prophetverse, in sktime fashion. The predict_quantiles method returns the quantiles of the predictive distribution in a pd.DataFrame\n\nquantiles = model.predict_quantiles(fh=forecast_horizon, alpha=[0.1, 0.9])\nquantiles.head()\n\n\n\n\n\n\n\n\ny\n\n\n\n0.1\n0.9\n\n\n\n\n2007-01-01\n8.007790\n9.344122\n\n\n2007-01-02\n7.872501\n9.168749\n\n\n2007-01-03\n7.729043\n9.019361\n\n\n2007-01-04\n7.724011\n9.026764\n\n\n2007-01-05\n7.777980\n9.042285\n\n\n\n\n\n\n\nThe plot below shows the (0.1, 0.9) quantiles of the predictive distribution\n\nfig, ax = plt.subplots(figsize=(8, 5))\n# Plot area between quantiles\nax.fill_between(\n    quantiles.index.to_timestamp(),\n    quantiles.iloc[:, 0],\n    quantiles.iloc[:, -1],\n    alpha=0.5,\n)\nax.scatter(y.index, y, marker=\"o\", color=\"k\", s=2, alpha=1)",
    "crumbs": [
      "Univariate forecasting"
    ]
  },
  {
    "objectID": "tutorials/univariate.html#timeseries-decomposition",
    "href": "tutorials/univariate.html#timeseries-decomposition",
    "title": "Basic Univariate Forecasting with Prophetverse",
    "section": "Timeseries decomposition",
    "text": "Timeseries decomposition\nWe can easily extract the components of the time series with the predict_components method. This method, in particular, is not implemented in sktime’s BaseForecaster, but it is a method of prophetverse.Prophetverse class.\n\nsites = model.predict_components(fh=forecast_horizon)\nsites.head()\n\n\n\n\n\n\n\n\nmean\nobs\nseasonality\ntrend\n\n\n\n\n2007-01-01\n8.716897\n8.675321\n0.916926\n7.799971\n\n\n2007-01-02\n8.520571\n8.515426\n0.720600\n7.799971\n\n\n2007-01-03\n8.366814\n8.384708\n0.566843\n7.799971\n\n\n2007-01-04\n8.387162\n8.385032\n0.587191\n7.799971\n\n\n2007-01-05\n8.415625\n8.406689\n0.615654\n7.799971\n\n\n\n\n\n\n\n\nfor column in sites.columns:\n    fig, ax = plt.subplots(figsize=(8, 2))\n    ax.plot(sites.index.to_timestamp(), sites[column], label=column)\n    ax.set_title(column)\n    fig.show()",
    "crumbs": [
      "Univariate forecasting"
    ]
  },
  {
    "objectID": "tutorials/univariate.html#fitting-with-mcmc",
    "href": "tutorials/univariate.html#fitting-with-mcmc",
    "title": "Basic Univariate Forecasting with Prophetverse",
    "section": "Fitting with MCMC",
    "text": "Fitting with MCMC\nIn the previous examples, we used MAP inference to fit the model. However, we can also use Markov Chain Monte Carlo (MCMC) to fit the model. To do this, we just need to change the inference_engine parameter to MCMCInferenceEngine. The rest of the code remains the same.\nThe set_params method is used to set the parameters of the model, in sklearn fashion.\n\nfrom prophetverse.engine import MCMCInferenceEngine\n\nmodel.set_params(inference_engine=MCMCInferenceEngine(num_warmup=1000))\n\n\nmodel.fit(y=y)\n\n  0%|          | 0/2000 [00:00&lt;?, ?it/s]warmup:   0%|          | 1/2000 [00:01&lt;47:27,  1.42s/it, 1 steps of size 2.34e+00. acc. prob=0.00]warmup:   2%|▏         | 42/2000 [00:01&lt;00:51, 37.92it/s, 511 steps of size 2.03e-07. acc. prob=0.63]warmup:   3%|▎         | 66/2000 [00:01&lt;00:35, 53.77it/s, 255 steps of size 2.26e-07. acc. prob=0.68]warmup:   4%|▍         | 84/2000 [00:01&lt;00:29, 64.55it/s, 127 steps of size 1.85e-07. acc. prob=0.69]warmup:   5%|▌         | 100/2000 [00:02&lt;00:26, 70.51it/s, 255 steps of size 8.18e-08. acc. prob=0.70]warmup:   6%|▌         | 113/2000 [00:02&lt;00:30, 61.66it/s, 511 steps of size 1.25e-05. acc. prob=0.72]warmup:   6%|▌         | 124/2000 [00:02&lt;00:29, 63.06it/s, 255 steps of size 4.60e-06. acc. prob=0.72]warmup:   7%|▋         | 134/2000 [00:02&lt;00:30, 61.08it/s, 1023 steps of size 4.99e-06. acc. prob=0.73]warmup:   7%|▋         | 143/2000 [00:02&lt;00:29, 63.82it/s, 1023 steps of size 1.89e-05. acc. prob=0.74]warmup:   8%|▊         | 154/2000 [00:02&lt;00:27, 67.96it/s, 1023 steps of size 1.17e-05. acc. prob=0.74]warmup:   8%|▊         | 166/2000 [00:03&lt;00:24, 73.87it/s, 1023 steps of size 4.71e-06. acc. prob=0.74]warmup:   9%|▉         | 175/2000 [00:03&lt;00:24, 74.81it/s, 6 steps of size 2.86e-06. acc. prob=0.74]   warmup:   9%|▉         | 184/2000 [00:03&lt;00:28, 64.58it/s, 1023 steps of size 8.76e-06. acc. prob=0.75]warmup:  10%|▉         | 195/2000 [00:03&lt;00:25, 71.73it/s, 511 steps of size 1.16e-05. acc. prob=0.75] warmup:  10%|█         | 203/2000 [00:03&lt;00:25, 71.02it/s, 127 steps of size 2.50e-05. acc. prob=0.75]warmup:  11%|█         | 211/2000 [00:03&lt;00:25, 69.03it/s, 1023 steps of size 1.95e-05. acc. prob=0.75]warmup:  11%|█         | 222/2000 [00:03&lt;00:24, 73.67it/s, 1023 steps of size 8.65e-06. acc. prob=0.75]warmup:  12%|█▏        | 234/2000 [00:04&lt;00:20, 84.51it/s, 127 steps of size 2.52e-05. acc. prob=0.76] warmup:  12%|█▏        | 245/2000 [00:04&lt;00:19, 90.42it/s, 255 steps of size 1.30e-05. acc. prob=0.76]warmup:  13%|█▎        | 255/2000 [00:04&lt;00:19, 88.16it/s, 127 steps of size 4.70e-05. acc. prob=0.76]warmup:  13%|█▎        | 265/2000 [00:04&lt;00:20, 82.64it/s, 255 steps of size 3.59e-05. acc. prob=0.76]warmup:  14%|█▍        | 277/2000 [00:04&lt;00:18, 91.50it/s, 511 steps of size 1.59e-05. acc. prob=0.76]warmup:  14%|█▍        | 287/2000 [00:04&lt;00:19, 89.85it/s, 255 steps of size 2.17e-05. acc. prob=0.76]warmup:  15%|█▍        | 298/2000 [00:04&lt;00:18, 93.43it/s, 255 steps of size 2.28e-05. acc. prob=0.76]warmup:  15%|█▌        | 308/2000 [00:04&lt;00:18, 92.49it/s, 511 steps of size 1.76e-05. acc. prob=0.76]warmup:  16%|█▌        | 318/2000 [00:04&lt;00:18, 89.52it/s, 127 steps of size 3.26e-05. acc. prob=0.77]warmup:  16%|█▋        | 328/2000 [00:05&lt;00:18, 90.50it/s, 7 steps of size 6.61e-06. acc. prob=0.76]  warmup:  17%|█▋        | 338/2000 [00:05&lt;00:19, 84.10it/s, 511 steps of size 2.04e-05. acc. prob=0.77]warmup:  17%|█▋        | 347/2000 [00:05&lt;00:21, 77.93it/s, 1023 steps of size 1.87e-05. acc. prob=0.77]warmup:  18%|█▊        | 360/2000 [00:05&lt;00:18, 89.63it/s, 255 steps of size 2.26e-05. acc. prob=0.77] warmup:  19%|█▊        | 371/2000 [00:05&lt;00:17, 94.17it/s, 255 steps of size 2.47e-05. acc. prob=0.77]warmup:  19%|█▉        | 382/2000 [00:05&lt;00:16, 96.43it/s, 511 steps of size 1.25e-05. acc. prob=0.77]warmup:  20%|█▉        | 392/2000 [00:05&lt;00:17, 93.34it/s, 127 steps of size 2.49e-05. acc. prob=0.77]warmup:  20%|██        | 405/2000 [00:05&lt;00:15, 102.56it/s, 255 steps of size 2.04e-05. acc. prob=0.77]warmup:  21%|██        | 416/2000 [00:05&lt;00:15, 100.77it/s, 255 steps of size 3.38e-05. acc. prob=0.77]warmup:  21%|██▏       | 427/2000 [00:06&lt;00:16, 96.52it/s, 511 steps of size 1.58e-05. acc. prob=0.77] warmup:  22%|██▏       | 439/2000 [00:06&lt;00:15, 100.86it/s, 511 steps of size 1.69e-05. acc. prob=0.77]warmup:  23%|██▎       | 453/2000 [00:06&lt;00:13, 110.71it/s, 127 steps of size 2.39e-05. acc. prob=0.77]warmup:  23%|██▎       | 465/2000 [00:06&lt;00:15, 97.65it/s, 7 steps of size 6.17e-06. acc. prob=0.77]   warmup:  24%|██▍       | 476/2000 [00:06&lt;00:16, 91.63it/s, 127 steps of size 2.71e-05. acc. prob=0.77]warmup:  24%|██▍       | 486/2000 [00:06&lt;00:16, 92.24it/s, 255 steps of size 2.73e-05. acc. prob=0.77]warmup:  25%|██▍       | 496/2000 [00:06&lt;00:16, 89.48it/s, 255 steps of size 1.14e-05. acc. prob=0.77]warmup:  25%|██▌       | 506/2000 [00:06&lt;00:18, 78.67it/s, 511 steps of size 1.17e-05. acc. prob=0.77]warmup:  26%|██▌       | 515/2000 [00:07&lt;00:18, 79.62it/s, 127 steps of size 4.33e-05. acc. prob=0.78]warmup:  26%|██▋       | 528/2000 [00:07&lt;00:16, 90.64it/s, 255 steps of size 2.02e-05. acc. prob=0.78]warmup:  27%|██▋       | 542/2000 [00:07&lt;00:15, 95.01it/s, 1023 steps of size 2.35e-05. acc. prob=0.78]warmup:  28%|██▊       | 555/2000 [00:07&lt;00:13, 103.29it/s, 127 steps of size 3.59e-05. acc. prob=0.78]warmup:  28%|██▊       | 570/2000 [00:07&lt;00:12, 114.81it/s, 127 steps of size 1.37e-05. acc. prob=0.78]warmup:  29%|██▉       | 582/2000 [00:07&lt;00:13, 103.72it/s, 255 steps of size 3.03e-05. acc. prob=0.78]warmup:  30%|██▉       | 593/2000 [00:07&lt;00:13, 104.36it/s, 255 steps of size 4.40e-05. acc. prob=0.78]warmup:  30%|███       | 604/2000 [00:07&lt;00:13, 102.60it/s, 255 steps of size 2.26e-05. acc. prob=0.78]warmup:  31%|███       | 620/2000 [00:08&lt;00:11, 115.67it/s, 255 steps of size 1.77e-05. acc. prob=0.78]warmup:  32%|███▏      | 632/2000 [00:08&lt;00:12, 110.33it/s, 255 steps of size 1.99e-05. acc. prob=0.78]warmup:  32%|███▏      | 645/2000 [00:08&lt;00:12, 111.30it/s, 511 steps of size 1.66e-05. acc. prob=0.78]warmup:  33%|███▎      | 657/2000 [00:08&lt;00:13, 99.36it/s, 255 steps of size 2.35e-05. acc. prob=0.78] warmup:  33%|███▎      | 668/2000 [00:08&lt;00:15, 87.44it/s, 255 steps of size 2.93e-05. acc. prob=0.78]warmup:  34%|███▍      | 680/2000 [00:08&lt;00:14, 93.55it/s, 255 steps of size 3.91e-05. acc. prob=0.78]warmup:  35%|███▍      | 691/2000 [00:08&lt;00:14, 89.89it/s, 1023 steps of size 2.26e-05. acc. prob=0.78]warmup:  35%|███▌      | 701/2000 [00:08&lt;00:15, 84.42it/s, 511 steps of size 2.33e-05. acc. prob=0.78] warmup:  36%|███▌      | 710/2000 [00:09&lt;00:16, 80.00it/s, 255 steps of size 4.09e-05. acc. prob=0.78]warmup:  36%|███▌      | 723/2000 [00:09&lt;00:14, 90.54it/s, 255 steps of size 3.68e-05. acc. prob=0.78]warmup:  37%|███▋      | 735/2000 [00:09&lt;00:12, 97.44it/s, 255 steps of size 4.16e-05. acc. prob=0.78]warmup:  37%|███▋      | 746/2000 [00:09&lt;00:14, 86.62it/s, 511 steps of size 3.20e-05. acc. prob=0.78]warmup:  38%|███▊      | 756/2000 [00:09&lt;00:14, 84.73it/s, 255 steps of size 3.02e-05. acc. prob=0.78]warmup:  38%|███▊      | 768/2000 [00:09&lt;00:13, 92.85it/s, 255 steps of size 5.24e-05. acc. prob=0.78]warmup:  39%|███▉      | 778/2000 [00:09&lt;00:14, 86.97it/s, 511 steps of size 2.80e-05. acc. prob=0.78]warmup:  39%|███▉      | 787/2000 [00:09&lt;00:14, 84.44it/s, 255 steps of size 2.78e-05. acc. prob=0.78]warmup:  40%|███▉      | 796/2000 [00:10&lt;00:14, 80.65it/s, 511 steps of size 2.94e-05. acc. prob=0.78]warmup:  40%|████      | 806/2000 [00:10&lt;00:14, 85.22it/s, 511 steps of size 4.32e-05. acc. prob=0.78]warmup:  41%|████      | 815/2000 [00:10&lt;00:14, 82.04it/s, 511 steps of size 2.73e-05. acc. prob=0.78]warmup:  41%|████▏     | 825/2000 [00:10&lt;00:13, 85.27it/s, 511 steps of size 2.89e-05. acc. prob=0.78]warmup:  42%|████▏     | 835/2000 [00:10&lt;00:13, 88.48it/s, 255 steps of size 3.94e-05. acc. prob=0.78]warmup:  42%|████▏     | 844/2000 [00:10&lt;00:13, 85.08it/s, 255 steps of size 4.71e-05. acc. prob=0.78]warmup:  43%|████▎     | 854/2000 [00:10&lt;00:12, 88.63it/s, 255 steps of size 2.68e-05. acc. prob=0.78]warmup:  43%|████▎     | 863/2000 [00:10&lt;00:14, 78.54it/s, 255 steps of size 3.64e-05. acc. prob=0.78]warmup:  44%|████▎     | 872/2000 [00:10&lt;00:15, 74.15it/s, 255 steps of size 2.07e-05. acc. prob=0.78]warmup:  44%|████▍     | 880/2000 [00:11&lt;00:15, 71.80it/s, 255 steps of size 3.25e-05. acc. prob=0.78]warmup:  44%|████▍     | 889/2000 [00:11&lt;00:14, 74.37it/s, 511 steps of size 3.75e-05. acc. prob=0.79]warmup:  45%|████▍     | 897/2000 [00:11&lt;00:15, 72.10it/s, 255 steps of size 2.72e-05. acc. prob=0.78]warmup:  45%|████▌     | 905/2000 [00:11&lt;00:15, 70.31it/s, 255 steps of size 3.77e-05. acc. prob=0.79]warmup:  46%|████▌     | 913/2000 [00:11&lt;00:16, 66.04it/s, 511 steps of size 3.43e-05. acc. prob=0.79]warmup:  46%|████▌     | 920/2000 [00:11&lt;00:16, 66.81it/s, 255 steps of size 4.41e-05. acc. prob=0.79]warmup:  46%|████▋     | 928/2000 [00:11&lt;00:15, 68.23it/s, 511 steps of size 3.67e-05. acc. prob=0.79]warmup:  47%|████▋     | 935/2000 [00:11&lt;00:17, 59.61it/s, 511 steps of size 3.14e-05. acc. prob=0.79]warmup:  47%|████▋     | 942/2000 [00:12&lt;00:18, 57.97it/s, 511 steps of size 3.51e-05. acc. prob=0.79]warmup:  47%|████▋     | 948/2000 [00:12&lt;00:19, 54.52it/s, 511 steps of size 4.53e-05. acc. prob=0.79]warmup:  48%|████▊     | 960/2000 [00:12&lt;00:15, 68.86it/s, 1023 steps of size 2.78e-05. acc. prob=0.79]warmup:  48%|████▊     | 970/2000 [00:12&lt;00:13, 75.61it/s, 1023 steps of size 1.98e-05. acc. prob=0.79]warmup:  49%|████▉     | 982/2000 [00:12&lt;00:11, 85.87it/s, 255 steps of size 7.50e-05. acc. prob=0.79] warmup:  50%|████▉     | 991/2000 [00:12&lt;00:11, 85.42it/s, 511 steps of size 3.40e-05. acc. prob=0.79]warmup:  50%|█████     | 1000/2000 [00:12&lt;00:12, 83.08it/s, 255 steps of size 3.99e-05. acc. prob=0.79]sample:  50%|█████     | 1009/2000 [00:12&lt;00:13, 72.05it/s, 255 steps of size 3.99e-05. acc. prob=0.96]sample:  51%|█████     | 1017/2000 [00:13&lt;00:15, 64.84it/s, 511 steps of size 3.99e-05. acc. prob=0.94]sample:  51%|█████     | 1024/2000 [00:13&lt;00:16, 58.53it/s, 511 steps of size 3.99e-05. acc. prob=0.95]sample:  52%|█████▏    | 1031/2000 [00:13&lt;00:16, 57.30it/s, 511 steps of size 3.99e-05. acc. prob=0.95]sample:  52%|█████▏    | 1037/2000 [00:13&lt;00:19, 50.19it/s, 1023 steps of size 3.99e-05. acc. prob=0.95]sample:  52%|█████▏    | 1043/2000 [00:13&lt;00:22, 42.59it/s, 1023 steps of size 3.99e-05. acc. prob=0.95]sample:  52%|█████▏    | 1048/2000 [00:13&lt;00:23, 40.25it/s, 1023 steps of size 3.99e-05. acc. prob=0.96]sample:  53%|█████▎    | 1053/2000 [00:14&lt;00:26, 35.98it/s, 1023 steps of size 3.99e-05. acc. prob=0.96]sample:  53%|█████▎    | 1057/2000 [00:14&lt;00:28, 33.63it/s, 1023 steps of size 3.99e-05. acc. prob=0.96]sample:  53%|█████▎    | 1061/2000 [00:14&lt;00:29, 31.79it/s, 1023 steps of size 3.99e-05. acc. prob=0.96]sample:  53%|█████▎    | 1065/2000 [00:14&lt;00:30, 30.51it/s, 1023 steps of size 3.99e-05. acc. prob=0.96]sample:  53%|█████▎    | 1069/2000 [00:14&lt;00:31, 29.56it/s, 1023 steps of size 3.99e-05. acc. prob=0.96]sample:  54%|█████▎    | 1072/2000 [00:14&lt;00:32, 28.99it/s, 1023 steps of size 3.99e-05. acc. prob=0.96]sample:  54%|█████▍    | 1075/2000 [00:14&lt;00:32, 28.55it/s, 1023 steps of size 3.99e-05. acc. prob=0.96]sample:  54%|█████▍    | 1078/2000 [00:14&lt;00:32, 28.20it/s, 1023 steps of size 3.99e-05. acc. prob=0.96]sample:  54%|█████▍    | 1081/2000 [00:15&lt;00:32, 27.94it/s, 1023 steps of size 3.99e-05. acc. prob=0.96]sample:  54%|█████▍    | 1084/2000 [00:15&lt;00:33, 27.76it/s, 1023 steps of size 3.99e-05. acc. prob=0.96]sample:  54%|█████▍    | 1087/2000 [00:15&lt;00:33, 27.61it/s, 1023 steps of size 3.99e-05. acc. prob=0.96]sample:  55%|█████▍    | 1090/2000 [00:15&lt;00:33, 27.51it/s, 1023 steps of size 3.99e-05. acc. prob=0.96]sample:  55%|█████▍    | 1093/2000 [00:15&lt;00:33, 27.34it/s, 1023 steps of size 3.99e-05. acc. prob=0.96]sample:  55%|█████▍    | 1096/2000 [00:15&lt;00:33, 27.28it/s, 1023 steps of size 3.99e-05. acc. prob=0.96]sample:  55%|█████▍    | 1099/2000 [00:15&lt;00:33, 27.29it/s, 1023 steps of size 3.99e-05. acc. prob=0.96]sample:  55%|█████▌    | 1102/2000 [00:15&lt;00:32, 27.28it/s, 1023 steps of size 3.99e-05. acc. prob=0.96]sample:  55%|█████▌    | 1105/2000 [00:15&lt;00:32, 27.29it/s, 1023 steps of size 3.99e-05. acc. prob=0.96]sample:  55%|█████▌    | 1108/2000 [00:16&lt;00:32, 27.27it/s, 1023 steps of size 3.99e-05. acc. prob=0.96]sample:  56%|█████▌    | 1111/2000 [00:16&lt;00:32, 27.31it/s, 1023 steps of size 3.99e-05. acc. prob=0.96]sample:  56%|█████▌    | 1114/2000 [00:16&lt;00:32, 27.29it/s, 1023 steps of size 3.99e-05. acc. prob=0.96]sample:  56%|█████▌    | 1117/2000 [00:16&lt;00:32, 27.26it/s, 1023 steps of size 3.99e-05. acc. prob=0.96]sample:  56%|█████▌    | 1120/2000 [00:16&lt;00:32, 27.27it/s, 1023 steps of size 3.99e-05. acc. prob=0.96]sample:  56%|█████▌    | 1123/2000 [00:16&lt;00:32, 27.23it/s, 1023 steps of size 3.99e-05. acc. prob=0.96]sample:  56%|█████▋    | 1126/2000 [00:16&lt;00:32, 27.26it/s, 1023 steps of size 3.99e-05. acc. prob=0.96]sample:  56%|█████▋    | 1129/2000 [00:16&lt;00:31, 27.28it/s, 1023 steps of size 3.99e-05. acc. prob=0.96]sample:  57%|█████▋    | 1132/2000 [00:16&lt;00:31, 27.27it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  57%|█████▋    | 1135/2000 [00:17&lt;00:31, 27.24it/s, 1023 steps of size 3.99e-05. acc. prob=0.96]sample:  57%|█████▋    | 1138/2000 [00:17&lt;00:31, 27.26it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  57%|█████▋    | 1141/2000 [00:17&lt;00:31, 27.28it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  57%|█████▋    | 1144/2000 [00:17&lt;00:31, 27.25it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  57%|█████▋    | 1147/2000 [00:17&lt;00:31, 27.27it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  57%|█████▊    | 1150/2000 [00:17&lt;00:31, 27.27it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  58%|█████▊    | 1153/2000 [00:17&lt;00:31, 27.30it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  58%|█████▊    | 1156/2000 [00:17&lt;00:30, 27.33it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  58%|█████▊    | 1159/2000 [00:17&lt;00:30, 27.34it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  58%|█████▊    | 1162/2000 [00:18&lt;00:30, 27.31it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  58%|█████▊    | 1165/2000 [00:18&lt;00:30, 27.30it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  58%|█████▊    | 1168/2000 [00:18&lt;00:30, 27.35it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  59%|█████▊    | 1171/2000 [00:18&lt;00:30, 27.33it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  59%|█████▊    | 1174/2000 [00:18&lt;00:30, 27.33it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  59%|█████▉    | 1177/2000 [00:18&lt;00:30, 27.28it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  59%|█████▉    | 1180/2000 [00:18&lt;00:30, 27.31it/s, 1023 steps of size 3.99e-05. acc. prob=0.96]sample:  59%|█████▉    | 1183/2000 [00:18&lt;00:29, 27.30it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  59%|█████▉    | 1186/2000 [00:18&lt;00:29, 27.28it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  59%|█████▉    | 1189/2000 [00:19&lt;00:29, 27.30it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  60%|█████▉    | 1192/2000 [00:19&lt;00:29, 27.30it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  60%|█████▉    | 1195/2000 [00:19&lt;00:29, 27.30it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  60%|█████▉    | 1198/2000 [00:19&lt;00:29, 27.30it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  60%|██████    | 1201/2000 [00:19&lt;00:29, 27.31it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  60%|██████    | 1204/2000 [00:19&lt;00:29, 27.29it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  60%|██████    | 1207/2000 [00:19&lt;00:29, 27.32it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  60%|██████    | 1210/2000 [00:19&lt;00:28, 27.30it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  61%|██████    | 1213/2000 [00:19&lt;00:28, 27.28it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  61%|██████    | 1216/2000 [00:20&lt;00:28, 27.27it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  61%|██████    | 1219/2000 [00:20&lt;00:28, 27.27it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  61%|██████    | 1222/2000 [00:20&lt;00:28, 27.29it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  61%|██████▏   | 1225/2000 [00:20&lt;00:28, 27.30it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  61%|██████▏   | 1228/2000 [00:20&lt;00:28, 27.32it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  62%|██████▏   | 1231/2000 [00:20&lt;00:28, 27.27it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  62%|██████▏   | 1234/2000 [00:20&lt;00:28, 27.23it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  62%|██████▏   | 1237/2000 [00:20&lt;00:28, 27.23it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  62%|██████▏   | 1240/2000 [00:20&lt;00:27, 27.27it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  62%|██████▏   | 1243/2000 [00:21&lt;00:27, 27.26it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  62%|██████▏   | 1246/2000 [00:21&lt;00:27, 27.26it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  62%|██████▏   | 1249/2000 [00:21&lt;00:27, 27.27it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  63%|██████▎   | 1252/2000 [00:21&lt;00:27, 27.26it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  63%|██████▎   | 1255/2000 [00:21&lt;00:27, 27.25it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  63%|██████▎   | 1258/2000 [00:21&lt;00:27, 27.23it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  63%|██████▎   | 1261/2000 [00:21&lt;00:27, 27.26it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  63%|██████▎   | 1264/2000 [00:21&lt;00:26, 27.29it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  63%|██████▎   | 1267/2000 [00:21&lt;00:26, 27.29it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  64%|██████▎   | 1270/2000 [00:22&lt;00:26, 27.26it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  64%|██████▎   | 1273/2000 [00:22&lt;00:26, 27.25it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  64%|██████▍   | 1276/2000 [00:22&lt;00:26, 27.23it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  64%|██████▍   | 1279/2000 [00:22&lt;00:26, 27.26it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  64%|██████▍   | 1282/2000 [00:22&lt;00:26, 27.25it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  64%|██████▍   | 1285/2000 [00:22&lt;00:26, 27.27it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  64%|██████▍   | 1288/2000 [00:22&lt;00:26, 27.28it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  65%|██████▍   | 1291/2000 [00:22&lt;00:26, 27.22it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  65%|██████▍   | 1294/2000 [00:22&lt;00:25, 27.27it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  65%|██████▍   | 1297/2000 [00:23&lt;00:25, 27.25it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  65%|██████▌   | 1300/2000 [00:23&lt;00:25, 27.23it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  65%|██████▌   | 1303/2000 [00:23&lt;00:25, 27.25it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  65%|██████▌   | 1306/2000 [00:23&lt;00:25, 27.28it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  65%|██████▌   | 1309/2000 [00:23&lt;00:25, 27.29it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  66%|██████▌   | 1312/2000 [00:23&lt;00:25, 27.26it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  66%|██████▌   | 1315/2000 [00:23&lt;00:25, 27.24it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  66%|██████▌   | 1318/2000 [00:23&lt;00:25, 27.25it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  66%|██████▌   | 1321/2000 [00:23&lt;00:24, 27.29it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  66%|██████▌   | 1324/2000 [00:24&lt;00:24, 27.22it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  66%|██████▋   | 1327/2000 [00:24&lt;00:24, 27.17it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  66%|██████▋   | 1330/2000 [00:24&lt;00:24, 27.18it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  67%|██████▋   | 1333/2000 [00:24&lt;00:24, 27.21it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  67%|██████▋   | 1336/2000 [00:24&lt;00:24, 27.26it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  67%|██████▋   | 1339/2000 [00:24&lt;00:24, 27.25it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  67%|██████▋   | 1342/2000 [00:24&lt;00:24, 27.22it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  67%|██████▋   | 1345/2000 [00:24&lt;00:24, 27.22it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  67%|██████▋   | 1348/2000 [00:24&lt;00:23, 27.26it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  68%|██████▊   | 1351/2000 [00:25&lt;00:23, 27.28it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  68%|██████▊   | 1354/2000 [00:25&lt;00:23, 27.23it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  68%|██████▊   | 1357/2000 [00:25&lt;00:23, 27.18it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  68%|██████▊   | 1360/2000 [00:25&lt;00:23, 27.18it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  68%|██████▊   | 1363/2000 [00:25&lt;00:23, 27.18it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  68%|██████▊   | 1366/2000 [00:25&lt;00:23, 27.17it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  68%|██████▊   | 1369/2000 [00:25&lt;00:23, 27.15it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  69%|██████▊   | 1372/2000 [00:25&lt;00:23, 27.17it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  69%|██████▉   | 1375/2000 [00:25&lt;00:22, 27.21it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  69%|██████▉   | 1378/2000 [00:25&lt;00:22, 27.23it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  69%|██████▉   | 1381/2000 [00:26&lt;00:22, 27.18it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  69%|██████▉   | 1384/2000 [00:26&lt;00:22, 27.20it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  69%|██████▉   | 1387/2000 [00:26&lt;00:22, 27.21it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  70%|██████▉   | 1390/2000 [00:26&lt;00:22, 27.21it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  70%|██████▉   | 1393/2000 [00:26&lt;00:22, 27.24it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  70%|██████▉   | 1396/2000 [00:26&lt;00:22, 27.23it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  70%|██████▉   | 1399/2000 [00:26&lt;00:22, 27.16it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  70%|███████   | 1402/2000 [00:26&lt;00:21, 27.18it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  70%|███████   | 1405/2000 [00:26&lt;00:21, 27.23it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  70%|███████   | 1408/2000 [00:27&lt;00:21, 27.22it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  71%|███████   | 1411/2000 [00:27&lt;00:21, 27.21it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  71%|███████   | 1414/2000 [00:27&lt;00:21, 27.16it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  71%|███████   | 1417/2000 [00:27&lt;00:21, 27.09it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  71%|███████   | 1420/2000 [00:27&lt;00:21, 27.19it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  71%|███████   | 1423/2000 [00:27&lt;00:21, 27.18it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  71%|███████▏  | 1426/2000 [00:27&lt;00:21, 27.19it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  71%|███████▏  | 1429/2000 [00:27&lt;00:20, 27.19it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  72%|███████▏  | 1432/2000 [00:27&lt;00:20, 27.21it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  72%|███████▏  | 1435/2000 [00:28&lt;00:20, 27.22it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  72%|███████▏  | 1438/2000 [00:28&lt;00:20, 27.23it/s, 1023 steps of size 3.99e-05. acc. prob=0.97]sample:  72%|███████▏  | 1441/2000 [00:28&lt;00:20, 27.25it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  72%|███████▏  | 1444/2000 [00:28&lt;00:20, 27.27it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  72%|███████▏  | 1447/2000 [00:28&lt;00:20, 27.26it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  72%|███████▎  | 1450/2000 [00:28&lt;00:20, 27.26it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  73%|███████▎  | 1453/2000 [00:28&lt;00:20, 27.27it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  73%|███████▎  | 1456/2000 [00:28&lt;00:19, 27.28it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  73%|███████▎  | 1459/2000 [00:28&lt;00:19, 27.25it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  73%|███████▎  | 1462/2000 [00:29&lt;00:19, 27.27it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  73%|███████▎  | 1465/2000 [00:29&lt;00:19, 27.26it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  73%|███████▎  | 1468/2000 [00:29&lt;00:19, 27.26it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  74%|███████▎  | 1471/2000 [00:29&lt;00:19, 27.27it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  74%|███████▎  | 1474/2000 [00:29&lt;00:19, 27.27it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  74%|███████▍  | 1477/2000 [00:29&lt;00:19, 27.24it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  74%|███████▍  | 1480/2000 [00:29&lt;00:19, 27.27it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  74%|███████▍  | 1483/2000 [00:29&lt;00:18, 27.22it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  74%|███████▍  | 1486/2000 [00:29&lt;00:18, 27.23it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  74%|███████▍  | 1489/2000 [00:30&lt;00:18, 27.24it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  75%|███████▍  | 1492/2000 [00:30&lt;00:18, 27.27it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  75%|███████▍  | 1495/2000 [00:30&lt;00:18, 27.28it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  75%|███████▍  | 1498/2000 [00:30&lt;00:18, 27.26it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  75%|███████▌  | 1501/2000 [00:30&lt;00:18, 27.26it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  75%|███████▌  | 1504/2000 [00:30&lt;00:18, 27.23it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  75%|███████▌  | 1507/2000 [00:30&lt;00:18, 27.22it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  76%|███████▌  | 1510/2000 [00:30&lt;00:18, 27.21it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  76%|███████▌  | 1513/2000 [00:30&lt;00:17, 27.23it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  76%|███████▌  | 1516/2000 [00:31&lt;00:17, 27.22it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  76%|███████▌  | 1519/2000 [00:31&lt;00:17, 27.18it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  76%|███████▌  | 1522/2000 [00:31&lt;00:17, 27.22it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  76%|███████▋  | 1525/2000 [00:31&lt;00:17, 27.24it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  76%|███████▋  | 1528/2000 [00:31&lt;00:17, 27.26it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  77%|███████▋  | 1531/2000 [00:31&lt;00:17, 27.25it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  77%|███████▋  | 1534/2000 [00:31&lt;00:17, 27.29it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  77%|███████▋  | 1537/2000 [00:31&lt;00:16, 27.25it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  77%|███████▋  | 1540/2000 [00:31&lt;00:16, 27.23it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  77%|███████▋  | 1543/2000 [00:32&lt;00:16, 27.21it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  77%|███████▋  | 1546/2000 [00:32&lt;00:16, 27.21it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  77%|███████▋  | 1549/2000 [00:32&lt;00:16, 27.22it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  78%|███████▊  | 1552/2000 [00:32&lt;00:16, 27.21it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  78%|███████▊  | 1555/2000 [00:32&lt;00:16, 27.22it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  78%|███████▊  | 1558/2000 [00:32&lt;00:16, 27.22it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  78%|███████▊  | 1561/2000 [00:32&lt;00:16, 27.19it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  78%|███████▊  | 1564/2000 [00:32&lt;00:16, 27.22it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  78%|███████▊  | 1567/2000 [00:32&lt;00:15, 27.24it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  78%|███████▊  | 1570/2000 [00:33&lt;00:15, 27.24it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  79%|███████▊  | 1573/2000 [00:33&lt;00:15, 27.23it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  79%|███████▉  | 1576/2000 [00:33&lt;00:15, 27.25it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  79%|███████▉  | 1579/2000 [00:33&lt;00:15, 27.27it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  79%|███████▉  | 1582/2000 [00:33&lt;00:15, 27.28it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  79%|███████▉  | 1585/2000 [00:33&lt;00:15, 27.24it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  79%|███████▉  | 1588/2000 [00:33&lt;00:15, 27.27it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  80%|███████▉  | 1591/2000 [00:33&lt;00:15, 27.25it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  80%|███████▉  | 1594/2000 [00:33&lt;00:14, 27.27it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  80%|███████▉  | 1597/2000 [00:34&lt;00:14, 27.24it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  80%|████████  | 1600/2000 [00:34&lt;00:14, 27.16it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  80%|████████  | 1603/2000 [00:34&lt;00:14, 27.24it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  80%|████████  | 1606/2000 [00:34&lt;00:14, 27.23it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  80%|████████  | 1609/2000 [00:34&lt;00:14, 27.23it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  81%|████████  | 1612/2000 [00:34&lt;00:14, 27.22it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  81%|████████  | 1615/2000 [00:34&lt;00:14, 27.25it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  81%|████████  | 1618/2000 [00:34&lt;00:14, 27.24it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  81%|████████  | 1621/2000 [00:34&lt;00:13, 27.26it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  81%|████████  | 1624/2000 [00:35&lt;00:13, 27.27it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  81%|████████▏ | 1627/2000 [00:35&lt;00:13, 27.25it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  82%|████████▏ | 1630/2000 [00:35&lt;00:13, 27.25it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  82%|████████▏ | 1633/2000 [00:35&lt;00:13, 27.29it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  82%|████████▏ | 1636/2000 [00:35&lt;00:13, 27.32it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  82%|████████▏ | 1639/2000 [00:35&lt;00:13, 27.28it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  82%|████████▏ | 1642/2000 [00:35&lt;00:13, 27.30it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  82%|████████▏ | 1645/2000 [00:35&lt;00:13, 27.26it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  82%|████████▏ | 1648/2000 [00:35&lt;00:12, 27.30it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  83%|████████▎ | 1651/2000 [00:36&lt;00:12, 27.29it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  83%|████████▎ | 1654/2000 [00:36&lt;00:12, 27.25it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  83%|████████▎ | 1657/2000 [00:36&lt;00:12, 27.28it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  83%|████████▎ | 1660/2000 [00:36&lt;00:12, 27.30it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  83%|████████▎ | 1663/2000 [00:36&lt;00:12, 27.34it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  83%|████████▎ | 1666/2000 [00:36&lt;00:12, 27.27it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  83%|████████▎ | 1669/2000 [00:36&lt;00:12, 27.28it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  84%|████████▎ | 1672/2000 [00:36&lt;00:12, 27.26it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  84%|████████▍ | 1675/2000 [00:36&lt;00:11, 27.28it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  84%|████████▍ | 1678/2000 [00:37&lt;00:11, 27.28it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  84%|████████▍ | 1681/2000 [00:37&lt;00:11, 27.27it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  84%|████████▍ | 1684/2000 [00:37&lt;00:11, 27.26it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  84%|████████▍ | 1687/2000 [00:37&lt;00:11, 27.31it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  84%|████████▍ | 1690/2000 [00:37&lt;00:11, 27.28it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  85%|████████▍ | 1693/2000 [00:37&lt;00:11, 27.28it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  85%|████████▍ | 1696/2000 [00:37&lt;00:11, 27.25it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  85%|████████▍ | 1699/2000 [00:37&lt;00:11, 27.28it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  85%|████████▌ | 1702/2000 [00:37&lt;00:10, 27.32it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  85%|████████▌ | 1705/2000 [00:37&lt;00:10, 27.33it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  85%|████████▌ | 1708/2000 [00:38&lt;00:10, 27.31it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  86%|████████▌ | 1711/2000 [00:38&lt;00:10, 27.29it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  86%|████████▌ | 1714/2000 [00:38&lt;00:10, 27.34it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  86%|████████▌ | 1717/2000 [00:38&lt;00:10, 27.32it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  86%|████████▌ | 1720/2000 [00:38&lt;00:10, 27.30it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  86%|████████▌ | 1723/2000 [00:38&lt;00:10, 27.24it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  86%|████████▋ | 1726/2000 [00:38&lt;00:10, 27.22it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  86%|████████▋ | 1729/2000 [00:38&lt;00:09, 27.26it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  87%|████████▋ | 1732/2000 [00:38&lt;00:09, 27.28it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  87%|████████▋ | 1735/2000 [00:39&lt;00:09, 27.28it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  87%|████████▋ | 1738/2000 [00:39&lt;00:09, 27.21it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  87%|████████▋ | 1741/2000 [00:39&lt;00:09, 27.10it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  87%|████████▋ | 1744/2000 [00:39&lt;00:09, 27.14it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  87%|████████▋ | 1747/2000 [00:39&lt;00:09, 27.19it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  88%|████████▊ | 1750/2000 [00:39&lt;00:09, 27.15it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  88%|████████▊ | 1753/2000 [00:39&lt;00:09, 27.17it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  88%|████████▊ | 1756/2000 [00:39&lt;00:09, 27.11it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  88%|████████▊ | 1759/2000 [00:39&lt;00:08, 27.14it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  88%|████████▊ | 1762/2000 [00:40&lt;00:08, 27.14it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  88%|████████▊ | 1765/2000 [00:40&lt;00:08, 27.13it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  88%|████████▊ | 1768/2000 [00:40&lt;00:08, 27.04it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  89%|████████▊ | 1771/2000 [00:40&lt;00:08, 27.07it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  89%|████████▊ | 1774/2000 [00:40&lt;00:08, 27.17it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  89%|████████▉ | 1777/2000 [00:40&lt;00:08, 27.19it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  89%|████████▉ | 1780/2000 [00:40&lt;00:08, 27.20it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  89%|████████▉ | 1783/2000 [00:40&lt;00:07, 27.26it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  89%|████████▉ | 1786/2000 [00:40&lt;00:07, 27.28it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  89%|████████▉ | 1789/2000 [00:41&lt;00:07, 27.28it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  90%|████████▉ | 1792/2000 [00:41&lt;00:07, 27.29it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  90%|████████▉ | 1795/2000 [00:41&lt;00:07, 27.33it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  90%|████████▉ | 1798/2000 [00:41&lt;00:07, 27.31it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  90%|█████████ | 1801/2000 [00:41&lt;00:07, 27.30it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  90%|█████████ | 1804/2000 [00:41&lt;00:07, 27.23it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  90%|█████████ | 1807/2000 [00:41&lt;00:07, 27.22it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  90%|█████████ | 1810/2000 [00:41&lt;00:06, 27.24it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  91%|█████████ | 1813/2000 [00:41&lt;00:06, 27.26it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  91%|█████████ | 1816/2000 [00:42&lt;00:06, 27.26it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  91%|█████████ | 1819/2000 [00:42&lt;00:06, 27.25it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  91%|█████████ | 1822/2000 [00:42&lt;00:06, 27.29it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  91%|█████████▏| 1825/2000 [00:42&lt;00:06, 27.21it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  91%|█████████▏| 1828/2000 [00:42&lt;00:06, 27.23it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  92%|█████████▏| 1831/2000 [00:42&lt;00:06, 27.20it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  92%|█████████▏| 1834/2000 [00:42&lt;00:06, 27.24it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  92%|█████████▏| 1837/2000 [00:42&lt;00:05, 27.21it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  92%|█████████▏| 1840/2000 [00:42&lt;00:05, 27.22it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  92%|█████████▏| 1843/2000 [00:43&lt;00:05, 27.23it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  92%|█████████▏| 1846/2000 [00:43&lt;00:05, 27.23it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  92%|█████████▏| 1849/2000 [00:43&lt;00:05, 27.26it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  93%|█████████▎| 1852/2000 [00:43&lt;00:05, 27.25it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  93%|█████████▎| 1855/2000 [00:43&lt;00:05, 27.24it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  93%|█████████▎| 1858/2000 [00:43&lt;00:05, 27.22it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  93%|█████████▎| 1861/2000 [00:43&lt;00:05, 27.16it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  93%|█████████▎| 1864/2000 [00:43&lt;00:05, 27.20it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  93%|█████████▎| 1867/2000 [00:43&lt;00:04, 27.19it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  94%|█████████▎| 1870/2000 [00:44&lt;00:04, 27.22it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  94%|█████████▎| 1873/2000 [00:44&lt;00:04, 27.23it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  94%|█████████▍| 1876/2000 [00:44&lt;00:04, 27.22it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  94%|█████████▍| 1879/2000 [00:44&lt;00:04, 27.22it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  94%|█████████▍| 1882/2000 [00:44&lt;00:04, 27.23it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  94%|█████████▍| 1885/2000 [00:44&lt;00:04, 27.24it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  94%|█████████▍| 1888/2000 [00:44&lt;00:04, 27.24it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  95%|█████████▍| 1891/2000 [00:44&lt;00:04, 27.22it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  95%|█████████▍| 1894/2000 [00:44&lt;00:03, 27.23it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  95%|█████████▍| 1897/2000 [00:45&lt;00:03, 27.25it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  95%|█████████▌| 1900/2000 [00:45&lt;00:03, 27.13it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  95%|█████████▌| 1903/2000 [00:45&lt;00:03, 27.18it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  95%|█████████▌| 1906/2000 [00:45&lt;00:03, 27.24it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  95%|█████████▌| 1909/2000 [00:45&lt;00:03, 27.28it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  96%|█████████▌| 1912/2000 [00:45&lt;00:03, 27.29it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  96%|█████████▌| 1915/2000 [00:45&lt;00:03, 27.29it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  96%|█████████▌| 1918/2000 [00:45&lt;00:03, 27.26it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  96%|█████████▌| 1921/2000 [00:45&lt;00:02, 27.28it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  96%|█████████▌| 1924/2000 [00:46&lt;00:02, 27.30it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  96%|█████████▋| 1927/2000 [00:46&lt;00:02, 27.29it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  96%|█████████▋| 1930/2000 [00:46&lt;00:02, 27.30it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  97%|█████████▋| 1933/2000 [00:46&lt;00:02, 27.26it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  97%|█████████▋| 1936/2000 [00:46&lt;00:02, 27.23it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  97%|█████████▋| 1939/2000 [00:46&lt;00:02, 27.21it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  97%|█████████▋| 1942/2000 [00:46&lt;00:02, 27.23it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  97%|█████████▋| 1945/2000 [00:46&lt;00:02, 27.21it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  97%|█████████▋| 1948/2000 [00:46&lt;00:01, 27.21it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  98%|█████████▊| 1951/2000 [00:47&lt;00:01, 27.23it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  98%|█████████▊| 1954/2000 [00:47&lt;00:01, 27.21it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  98%|█████████▊| 1957/2000 [00:47&lt;00:01, 27.20it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  98%|█████████▊| 1960/2000 [00:47&lt;00:01, 27.23it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  98%|█████████▊| 1963/2000 [00:47&lt;00:01, 27.23it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  98%|█████████▊| 1966/2000 [00:47&lt;00:01, 27.25it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  98%|█████████▊| 1969/2000 [00:47&lt;00:01, 27.24it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  99%|█████████▊| 1972/2000 [00:47&lt;00:01, 27.26it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  99%|█████████▉| 1975/2000 [00:47&lt;00:00, 27.22it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  99%|█████████▉| 1978/2000 [00:48&lt;00:00, 27.26it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  99%|█████████▉| 1981/2000 [00:48&lt;00:00, 27.23it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  99%|█████████▉| 1984/2000 [00:48&lt;00:00, 27.23it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample:  99%|█████████▉| 1987/2000 [00:48&lt;00:00, 27.24it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample: 100%|█████████▉| 1990/2000 [00:48&lt;00:00, 27.23it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample: 100%|█████████▉| 1993/2000 [00:48&lt;00:00, 27.23it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample: 100%|█████████▉| 1996/2000 [00:48&lt;00:00, 27.25it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample: 100%|█████████▉| 1999/2000 [00:48&lt;00:00, 27.23it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]sample: 100%|██████████| 2000/2000 [00:48&lt;00:00, 40.96it/s, 1023 steps of size 3.99e-05. acc. prob=0.98]\n\n\nProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3,\n                                                                              10],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[7, 365.25]),\n                                 '^$')],\n             inference_engine=MCMCInferenceEngine(num_warmup=1000),\n             trend=PiecewiseLinearTrend(changepoint_interval=500,\n                                        changepoint_prior_scale=1e-05,\n                                        changepoint_range=-500))Please rerun this cell to show the HTML repr or trust the notebook.ProphetverseProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3,\n                                                                              10],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[7, 365.25]),\n                                 '^$')],\n             inference_engine=MCMCInferenceEngine(num_warmup=1000),\n             trend=PiecewiseLinearTrend(changepoint_interval=500,\n                                        changepoint_prior_scale=1e-05,\n                                        changepoint_range=-500))effectsPiecewiseLinearTrendPiecewiseLinearTrend(changepoint_interval=500, changepoint_prior_scale=1e-05,\n                     changepoint_range=-500)LinearFourierSeasonalityLinearFourierSeasonality(effect_mode='multiplicative',\n                         fourier_terms_list=[3, 10], freq='D', prior_scale=0.1,\n                         sp_list=[7, 365.25])inference_engineMCMCInferenceEngineMCMCInferenceEngine(num_warmup=1000)\n\n\n\nquantiles = model.predict_quantiles(fh=forecast_horizon, alpha=[0.75, 0.25])\nfig, ax = plt.subplots(figsize=(8, 5))\n# Plot area between quantiles\nax.fill_between(\n    quantiles.index.to_timestamp(),\n    quantiles.iloc[:, 0],\n    quantiles.iloc[:, -1],\n    alpha=0.5,\n)\nax.scatter(y.index, y, marker=\"o\", color=\"k\", s=2, alpha=1)\n\n\n\n\n\n\n\n\nOne interesting feature of MCMC is that it allows us to obtain samples from the posterior distribution of the parameters. In other words, we can also obtain probabilistic forecasts for the TS components.\n\nsamples = model.predict_component_samples(fh=forecast_horizon)\nsamples\n\n\n\n\n\n\n\n\n\nmean\nobs\nseasonality\ntrend\n\n\nsample\n\n\n\n\n\n\n\n\n\n0\n2007-01-01\n9.891352\n9.867616\n1.636687\n8.254665\n\n\n2007-01-02\n9.353775\n9.653303\n1.099111\n8.254665\n\n\n2007-01-03\n8.951501\n8.886124\n0.696836\n8.254665\n\n\n2007-01-04\n8.902651\n9.016373\n0.647986\n8.254665\n\n\n2007-01-05\n8.845720\n9.285078\n0.591056\n8.254665\n\n\n...\n...\n...\n...\n...\n...\n\n\n999\n2017-12-28\n7.470278\n7.294347\n0.295118\n7.175160\n\n\n2017-12-29\n7.463235\n8.397017\n0.288778\n7.174457\n\n\n2017-12-30\n7.259590\n7.470821\n0.085837\n7.173753\n\n\n2017-12-31\n7.656203\n7.988126\n0.483153\n7.173050\n\n\n2018-01-01\n7.946797\n7.518693\n0.774450\n7.172347\n\n\n\n\n4019000 rows × 4 columns",
    "crumbs": [
      "Univariate forecasting"
    ]
  },
  {
    "objectID": "tutorials/univariate.html#extra-syntax-sugar",
    "href": "tutorials/univariate.html#extra-syntax-sugar",
    "title": "Basic Univariate Forecasting with Prophetverse",
    "section": "Extra: syntax sugar",
    "text": "Extra: syntax sugar\nIn Prophetverse, we’ve implemented the &gt;&gt; operator, which makes it easier to set trend, exogenous_effects and inference_engine parameters.\n\ntrend = PiecewiseLinearTrend(\n    changepoint_interval=300,\n    changepoint_prior_scale=0.0001,\n    changepoint_range=0.8,\n)\nexogenous_effects = [\n    (\n        \"seasonality\",\n        LinearFourierSeasonality(\n            freq=\"D\",\n            sp_list=[7, 365.25],\n            fourier_terms_list=[3, 10],\n            prior_scale=0.1,\n            effect_mode=\"multiplicative\",\n        ),\n        no_input_columns,\n    ),\n]\ninference_engine = MAPInferenceEngine()\n\nmodel = Prophetverse() &gt;&gt; trend &gt;&gt; exogenous_effects &gt;&gt; inference_engine\nmodel.fit(y=y)\n\nProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3,\n                                                                              10],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[7, 365.25]),\n                                 '^$')],\n             inference_engine=MAPInferenceEngine(),\n             trend=PiecewiseLinearTrend(changepoint_interval=300,\n                                        changepoint_prior_scale=0.0001))Please rerun this cell to show the HTML repr or trust the notebook.ProphetverseProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3,\n                                                                              10],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[7, 365.25]),\n                                 '^$')],\n             inference_engine=MAPInferenceEngine(),\n             trend=PiecewiseLinearTrend(changepoint_interval=300,\n                                        changepoint_prior_scale=0.0001))effectsPiecewiseLinearTrendPiecewiseLinearTrend(changepoint_interval=300, changepoint_prior_scale=0.0001)LinearFourierSeasonalityLinearFourierSeasonality(effect_mode='multiplicative',\n                         fourier_terms_list=[3, 10], freq='D', prior_scale=0.1,\n                         sp_list=[7, 365.25])inference_engineMAPInferenceEngineMAPInferenceEngine()\n\n\n\nforecast_horizon = pd.period_range(\"2007-01-01\", \"2018-01-01\", freq=\"D\")\nfig, ax = plt.subplots(figsize=(8, 5))\npreds = model.predict(fh=forecast_horizon)\npreds.plot.line(ax=ax)\nax.scatter(y.index, y, marker=\"o\", color=\"k\", s=2, alpha=0.5)",
    "crumbs": [
      "Univariate forecasting"
    ]
  },
  {
    "objectID": "tutorials/index.html",
    "href": "tutorials/index.html",
    "title": "Tutorials",
    "section": "",
    "text": "In this section, you can find a collection of tutorials that will help you understand the basics of Prophetverse for forecasting."
  },
  {
    "objectID": "reference/sktime.Prophetverse.html",
    "href": "reference/sktime.Prophetverse.html",
    "title": "sktime.Prophetverse",
    "section": "",
    "text": "sktime.Prophetverse(\n    self,\n    trend='linear',\n    exogenous_effects=None,\n    default_effect=None,\n    feature_transformer=None,\n    noise_scale=None,\n    likelihood='normal',\n    scale=None,\n    rng_key=None,\n    inference_engine=None,\n)\nUnivariate Prophetverse forecaster with multiple likelihood options.\nThis forecaster implements a univariate model with support for different likelihoods. It differs from Facebook’s Prophet in several ways: - Logistic trend is parametrized differently, inferring capacity from data. - Arbitrary sktime transformers can be used (e.g., FourierFeatures or HolidayFeatures). - No default weekly or yearly seasonality; these must be provided via the feature_transformer. - Uses ‘changepoint_interval’ instead of ‘n_changepoints’ for selecting changepoints. - Allows for configuring distinct functions for each exogenous variable effect.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntrend\nUnion[str, BaseEffect]\nType of trend to use. Either “linear” (default) or “logistic”, or a custom effect object.\n'linear'\n\n\nexogenous_effects\nOptional[List[BaseEffect]]\nList of effect objects defining the exogenous effects.\nNone\n\n\ndefault_effect\nOptional[BaseEffect]\nThe default effect for variables without a specified effect.\nNone\n\n\nfeature_transformer\nsktime transformer\nTransformer object to generate additional features (e.g., Fourier terms).\nNone\n\n\nnoise_scale\nfloat\nScale parameter for the observation noise. Must be greater than 0. (default: 0.05)\nNone\n\n\nlikelihood\nstr\nThe likelihood model to use. One of “normal”, “gamma”, or “negbinomial”. (default: “normal”)\n'normal'\n\n\nscale\noptional\nScaling value inferred from the data.\nNone\n\n\nrng_key\noptional\nA jax.random.PRNGKey instance, or None.\nNone\n\n\ninference_engine\noptional\nAn inference engine for running the model.\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf noise_scale is not greater than 0 or an unsupported likelihood is provided.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nget_test_params\nReturn parameters to be used in sktime unit tests.\n\n\n\n\n\nsktime.Prophetverse.get_test_params(parameter_set='default')\nReturn parameters to be used in sktime unit tests.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nparameter_set\nstr\nThe parameter set name (currently ignored).\n'default'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nList[dict[str, int]]\nA list of dictionaries containing test parameters.",
    "crumbs": [
      "Sktime",
      "sktime.Prophetverse"
    ]
  },
  {
    "objectID": "reference/sktime.Prophetverse.html#parameters",
    "href": "reference/sktime.Prophetverse.html#parameters",
    "title": "sktime.Prophetverse",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ntrend\nUnion[str, BaseEffect]\nType of trend to use. Either “linear” (default) or “logistic”, or a custom effect object.\n'linear'\n\n\nexogenous_effects\nOptional[List[BaseEffect]]\nList of effect objects defining the exogenous effects.\nNone\n\n\ndefault_effect\nOptional[BaseEffect]\nThe default effect for variables without a specified effect.\nNone\n\n\nfeature_transformer\nsktime transformer\nTransformer object to generate additional features (e.g., Fourier terms).\nNone\n\n\nnoise_scale\nfloat\nScale parameter for the observation noise. Must be greater than 0. (default: 0.05)\nNone\n\n\nlikelihood\nstr\nThe likelihood model to use. One of “normal”, “gamma”, or “negbinomial”. (default: “normal”)\n'normal'\n\n\nscale\noptional\nScaling value inferred from the data.\nNone\n\n\nrng_key\noptional\nA jax.random.PRNGKey instance, or None.\nNone\n\n\ninference_engine\noptional\nAn inference engine for running the model.\nNone",
    "crumbs": [
      "Sktime",
      "sktime.Prophetverse"
    ]
  },
  {
    "objectID": "reference/sktime.Prophetverse.html#raises",
    "href": "reference/sktime.Prophetverse.html#raises",
    "title": "sktime.Prophetverse",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nValueError\nIf noise_scale is not greater than 0 or an unsupported likelihood is provided.",
    "crumbs": [
      "Sktime",
      "sktime.Prophetverse"
    ]
  },
  {
    "objectID": "reference/sktime.Prophetverse.html#methods",
    "href": "reference/sktime.Prophetverse.html#methods",
    "title": "sktime.Prophetverse",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nget_test_params\nReturn parameters to be used in sktime unit tests.\n\n\n\n\n\nsktime.Prophetverse.get_test_params(parameter_set='default')\nReturn parameters to be used in sktime unit tests.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nparameter_set\nstr\nThe parameter set name (currently ignored).\n'default'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nList[dict[str, int]]\nA list of dictionaries containing test parameters.",
    "crumbs": [
      "Sktime",
      "sktime.Prophetverse"
    ]
  },
  {
    "objectID": "reference/sktime.HierarchicalProphet.html",
    "href": "reference/sktime.HierarchicalProphet.html",
    "title": "sktime.HierarchicalProphet",
    "section": "",
    "text": "sktime.HierarchicalProphet(\n    self,\n    trend='linear',\n    feature_transformer=None,\n    exogenous_effects=None,\n    default_effect=None,\n    shared_features=None,\n    noise_scale=0.05,\n    correlation_matrix_concentration=1.0,\n    rng_key=None,\n    inference_engine=None,\n    likelihood=None,\n)\nA Bayesian hierarchical time series forecasting model based on Meta’s Prophet.\nThis method forecasts all bottom series in a hierarchy at once, using a MultivariateNormal as the likelihood function and LKJ priors for the correlation matrix.\nThis forecaster is particularly interesting if you want to fit shared coefficients across series. In that case, shared_features parameter should be a list of feature names that should have that behaviour.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntrend\nUnion[BaseEffect, str]\nTrend component of the model.\n\"linear\"\n\n\nfeature_transformer\nBaseTransformer\nTransformer for features preprocessing.\nNone\n\n\nexogenous_effects\noptional\nEffects to model exogenous variables.\nNone\n\n\ndefault_effect\noptional\nDefault effect specification.\nNone\n\n\nshared_features\noptional\nFeatures shared across time series.\nNone\n\n\nnoise_scale\nfloat\nScale parameter for the noise distribution.\n0.05\n\n\ncorrelation_matrix_concentration\nfloat\nConcentration parameter for the correlation matrix.\n1.0\n\n\nrng_key\noptional\nRandom number generator key.\nNone\n\n\ninference_engine\noptional\nEngine used for inference.\nNone\n\n\n\n\n\n\n&gt;&gt;&gt; from sktime.forecasting.naive import NaiveForecaster\n&gt;&gt;&gt; from sktime.transformations.hierarchical.aggregate import Aggregator\n&gt;&gt;&gt; from sktime.utils._testing.hierarchical import _bottom_hier_datagen\n&gt;&gt;&gt; from prophetverse.sktime.multivariate import HierarchicalProphet\n&gt;&gt;&gt; agg = Aggregator()\n&gt;&gt;&gt; y = _bottom_hier_datagen(\n...     no_bottom_nodes=3,\n...     no_levels=1,\n...     random_seed=123,\n...     length=7,\n... )\n&gt;&gt;&gt; y = agg.fit_transform(y)\n&gt;&gt;&gt; forecaster = HierarchicalProphet()\n&gt;&gt;&gt; forecaster = forecaster.fit(y)\n&gt;&gt;&gt; y_pred = forecaster.predict(fh=[1])\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nn_series\nGet the number of series.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nget_test_params\nParams to be used in sktime unit tests.\n\n\npredict_samples\nGenerate samples for the given exogenous variables and forecasting horizon.\n\n\n\n\n\nsktime.HierarchicalProphet.get_test_params(parameter_set='default')\nParams to be used in sktime unit tests.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nparameter_set\nstr\nThe parameter set to be used (ignored in this implementation)\n'default'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nList[dict[str, int]]\nA list of dictionaries containing the test parameters.\n\n\n\n\n\n\n\nsktime.HierarchicalProphet.predict_samples(fh, X=None)\nGenerate samples for the given exogenous variables and forecasting horizon.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\npd.DataFrame\nExogenous variables.\nNone\n\n\nfh\nForecastingHorizon\nForecasting horizon.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nPredicted samples.",
    "crumbs": [
      "Sktime",
      "sktime.HierarchicalProphet"
    ]
  },
  {
    "objectID": "reference/sktime.HierarchicalProphet.html#parameters",
    "href": "reference/sktime.HierarchicalProphet.html#parameters",
    "title": "sktime.HierarchicalProphet",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ntrend\nUnion[BaseEffect, str]\nTrend component of the model.\n\"linear\"\n\n\nfeature_transformer\nBaseTransformer\nTransformer for features preprocessing.\nNone\n\n\nexogenous_effects\noptional\nEffects to model exogenous variables.\nNone\n\n\ndefault_effect\noptional\nDefault effect specification.\nNone\n\n\nshared_features\noptional\nFeatures shared across time series.\nNone\n\n\nnoise_scale\nfloat\nScale parameter for the noise distribution.\n0.05\n\n\ncorrelation_matrix_concentration\nfloat\nConcentration parameter for the correlation matrix.\n1.0\n\n\nrng_key\noptional\nRandom number generator key.\nNone\n\n\ninference_engine\noptional\nEngine used for inference.\nNone",
    "crumbs": [
      "Sktime",
      "sktime.HierarchicalProphet"
    ]
  },
  {
    "objectID": "reference/sktime.HierarchicalProphet.html#examples",
    "href": "reference/sktime.HierarchicalProphet.html#examples",
    "title": "sktime.HierarchicalProphet",
    "section": "",
    "text": "&gt;&gt;&gt; from sktime.forecasting.naive import NaiveForecaster\n&gt;&gt;&gt; from sktime.transformations.hierarchical.aggregate import Aggregator\n&gt;&gt;&gt; from sktime.utils._testing.hierarchical import _bottom_hier_datagen\n&gt;&gt;&gt; from prophetverse.sktime.multivariate import HierarchicalProphet\n&gt;&gt;&gt; agg = Aggregator()\n&gt;&gt;&gt; y = _bottom_hier_datagen(\n...     no_bottom_nodes=3,\n...     no_levels=1,\n...     random_seed=123,\n...     length=7,\n... )\n&gt;&gt;&gt; y = agg.fit_transform(y)\n&gt;&gt;&gt; forecaster = HierarchicalProphet()\n&gt;&gt;&gt; forecaster = forecaster.fit(y)\n&gt;&gt;&gt; y_pred = forecaster.predict(fh=[1])",
    "crumbs": [
      "Sktime",
      "sktime.HierarchicalProphet"
    ]
  },
  {
    "objectID": "reference/sktime.HierarchicalProphet.html#attributes",
    "href": "reference/sktime.HierarchicalProphet.html#attributes",
    "title": "sktime.HierarchicalProphet",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nn_series\nGet the number of series.",
    "crumbs": [
      "Sktime",
      "sktime.HierarchicalProphet"
    ]
  },
  {
    "objectID": "reference/sktime.HierarchicalProphet.html#methods",
    "href": "reference/sktime.HierarchicalProphet.html#methods",
    "title": "sktime.HierarchicalProphet",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nget_test_params\nParams to be used in sktime unit tests.\n\n\npredict_samples\nGenerate samples for the given exogenous variables and forecasting horizon.\n\n\n\n\n\nsktime.HierarchicalProphet.get_test_params(parameter_set='default')\nParams to be used in sktime unit tests.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nparameter_set\nstr\nThe parameter set to be used (ignored in this implementation)\n'default'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nList[dict[str, int]]\nA list of dictionaries containing the test parameters.\n\n\n\n\n\n\n\nsktime.HierarchicalProphet.predict_samples(fh, X=None)\nGenerate samples for the given exogenous variables and forecasting horizon.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\npd.DataFrame\nExogenous variables.\nNone\n\n\nfh\nForecastingHorizon\nForecasting horizon.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nPredicted samples.",
    "crumbs": [
      "Sktime",
      "sktime.HierarchicalProphet"
    ]
  },
  {
    "objectID": "reference/index.html",
    "href": "reference/index.html",
    "title": "Function reference",
    "section": "",
    "text": "Sktime models\n\n\n\nProphetverse\nUnivariate Prophetverse forecaster with multiple likelihood options.\n\n\nHierarchicalProphet\nA Bayesian hierarchical time series forecasting model based on Meta’s Prophet.\n\n\n\n\n\n\nExogenous effects\n\n\n\nLinearEffect\nRepresents a linear effect in a hierarchical prophet model.\n\n\nLinearFourierSeasonality\nLinear Fourier Seasonality effect.\n\n\nLogEffect\nRepresents a log effect as effect = scale * log(rate * data + 1).\n\n\nHillEffect\nRepresents a Hill effect in a time series model.\n\n\nChainedEffects\nChains multiple effects sequentially, applying them one after the other.\n\n\nGeometricAdstockEffect\nRepresents a Geometric Adstock effect in a time series model.\n\n\n\n\n\n\nMMM Likelihoods\n\n\n\nLiftExperimentLikelihood\nWrap an effect and applies a normal likelihood to its output.\n\n\nExactLikelihood\nWrap an effect and applies a normal likelihood to its output.\n\n\n\n\n\n\nTrends\n\n\n\nPiecewiseLinearTrend\nPiecewise Linear Trend model.\n\n\nPiecewiseLogisticTrend\nPiecewise logistic trend model.\n\n\nFlatTrend\nFlat trend model.\n\n\n\n\n\n\nLikelihoods for the target variable\n\n\n\nMultivariateNormal\nBase class for effects.\n\n\nNormalTargetLikelihood\n\n\n\nGammaTargetLikelihood\n\n\n\nNegativeBinomialTargetLikelihood",
    "crumbs": [
      "Function reference"
    ]
  },
  {
    "objectID": "reference/index.html#sktime",
    "href": "reference/index.html#sktime",
    "title": "Function reference",
    "section": "",
    "text": "Sktime models\n\n\n\nProphetverse\nUnivariate Prophetverse forecaster with multiple likelihood options.\n\n\nHierarchicalProphet\nA Bayesian hierarchical time series forecasting model based on Meta’s Prophet.",
    "crumbs": [
      "Function reference"
    ]
  },
  {
    "objectID": "reference/Prophetverse.html",
    "href": "reference/Prophetverse.html",
    "title": "Prophetverse",
    "section": "",
    "text": "sktime.Prophetverse(\n    self,\n    trend='linear',\n    exogenous_effects=None,\n    default_effect=None,\n    feature_transformer=None,\n    noise_scale=None,\n    likelihood='normal',\n    scale=None,\n    rng_key=None,\n    inference_engine=None,\n)\nUnivariate Prophetverse forecaster with multiple likelihood options.\nThis forecaster implements a univariate model with support for different likelihoods. It differs from Facebook’s Prophet in several ways: - Logistic trend is parametrized differently, inferring capacity from data. - Arbitrary sktime transformers can be used (e.g., FourierFeatures or HolidayFeatures). - No default weekly or yearly seasonality; these must be provided via the feature_transformer. - Uses ‘changepoint_interval’ instead of ‘n_changepoints’ for selecting changepoints. - Allows for configuring distinct functions for each exogenous variable effect.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntrend\nUnion[str, BaseEffect]\nType of trend to use. Either “linear” (default) or “logistic”, or a custom effect object.\n'linear'\n\n\nexogenous_effects\nOptional[List[BaseEffect]]\nList of effect objects defining the exogenous effects.\nNone\n\n\ndefault_effect\nOptional[BaseEffect]\nThe default effect for variables without a specified effect.\nNone\n\n\nfeature_transformer\nsktime transformer\nTransformer object to generate additional features (e.g., Fourier terms).\nNone\n\n\nnoise_scale\nfloat\nScale parameter for the observation noise. Must be greater than 0. (default: 0.05)\nNone\n\n\nlikelihood\nstr\nThe likelihood model to use. One of “normal”, “gamma”, or “negbinomial”. (default: “normal”)\n'normal'\n\n\nscale\noptional\nScaling value inferred from the data.\nNone\n\n\nrng_key\noptional\nA jax.random.PRNGKey instance, or None.\nNone\n\n\ninference_engine\noptional\nAn inference engine for running the model.\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf noise_scale is not greater than 0 or an unsupported likelihood is provided.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nget_test_params\nReturn parameters to be used in sktime unit tests.\n\n\n\n\n\nsktime.Prophetverse.get_test_params(parameter_set='default')\nReturn parameters to be used in sktime unit tests.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nparameter_set\nstr\nThe parameter set name (currently ignored).\n'default'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nList[dict[str, int]]\nA list of dictionaries containing test parameters.",
    "crumbs": [
      "Sktime",
      "Prophetverse"
    ]
  },
  {
    "objectID": "reference/Prophetverse.html#parameters",
    "href": "reference/Prophetverse.html#parameters",
    "title": "Prophetverse",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ntrend\nUnion[str, BaseEffect]\nType of trend to use. Either “linear” (default) or “logistic”, or a custom effect object.\n'linear'\n\n\nexogenous_effects\nOptional[List[BaseEffect]]\nList of effect objects defining the exogenous effects.\nNone\n\n\ndefault_effect\nOptional[BaseEffect]\nThe default effect for variables without a specified effect.\nNone\n\n\nfeature_transformer\nsktime transformer\nTransformer object to generate additional features (e.g., Fourier terms).\nNone\n\n\nnoise_scale\nfloat\nScale parameter for the observation noise. Must be greater than 0. (default: 0.05)\nNone\n\n\nlikelihood\nstr\nThe likelihood model to use. One of “normal”, “gamma”, or “negbinomial”. (default: “normal”)\n'normal'\n\n\nscale\noptional\nScaling value inferred from the data.\nNone\n\n\nrng_key\noptional\nA jax.random.PRNGKey instance, or None.\nNone\n\n\ninference_engine\noptional\nAn inference engine for running the model.\nNone",
    "crumbs": [
      "Sktime",
      "Prophetverse"
    ]
  },
  {
    "objectID": "reference/Prophetverse.html#raises",
    "href": "reference/Prophetverse.html#raises",
    "title": "Prophetverse",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nValueError\nIf noise_scale is not greater than 0 or an unsupported likelihood is provided.",
    "crumbs": [
      "Sktime",
      "Prophetverse"
    ]
  },
  {
    "objectID": "reference/Prophetverse.html#methods",
    "href": "reference/Prophetverse.html#methods",
    "title": "Prophetverse",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nget_test_params\nReturn parameters to be used in sktime unit tests.\n\n\n\n\n\nsktime.Prophetverse.get_test_params(parameter_set='default')\nReturn parameters to be used in sktime unit tests.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nparameter_set\nstr\nThe parameter set name (currently ignored).\n'default'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nList[dict[str, int]]\nA list of dictionaries containing test parameters.",
    "crumbs": [
      "Sktime",
      "Prophetverse"
    ]
  },
  {
    "objectID": "reference/HierarchicalProphet.html",
    "href": "reference/HierarchicalProphet.html",
    "title": "HierarchicalProphet",
    "section": "",
    "text": "sktime.HierarchicalProphet(\n    self,\n    trend='linear',\n    feature_transformer=None,\n    exogenous_effects=None,\n    default_effect=None,\n    shared_features=None,\n    noise_scale=0.05,\n    correlation_matrix_concentration=1.0,\n    rng_key=None,\n    inference_engine=None,\n    likelihood=None,\n)\nA Bayesian hierarchical time series forecasting model based on Meta’s Prophet.\nThis method forecasts all bottom series in a hierarchy at once, using a MultivariateNormal as the likelihood function and LKJ priors for the correlation matrix.\nThis forecaster is particularly interesting if you want to fit shared coefficients across series. In that case, shared_features parameter should be a list of feature names that should have that behaviour.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntrend\nUnion[BaseEffect, str]\nTrend component of the model.\n\"linear\"\n\n\nfeature_transformer\nBaseTransformer\nTransformer for features preprocessing.\nNone\n\n\nexogenous_effects\noptional\nEffects to model exogenous variables.\nNone\n\n\ndefault_effect\noptional\nDefault effect specification.\nNone\n\n\nshared_features\noptional\nFeatures shared across time series.\nNone\n\n\nnoise_scale\nfloat\nScale parameter for the noise distribution.\n0.05\n\n\ncorrelation_matrix_concentration\nfloat\nConcentration parameter for the correlation matrix.\n1.0\n\n\nrng_key\noptional\nRandom number generator key.\nNone\n\n\ninference_engine\noptional\nEngine used for inference.\nNone\n\n\n\n\n\n\n&gt;&gt;&gt; from sktime.forecasting.naive import NaiveForecaster\n&gt;&gt;&gt; from sktime.transformations.hierarchical.aggregate import Aggregator\n&gt;&gt;&gt; from sktime.utils._testing.hierarchical import _bottom_hier_datagen\n&gt;&gt;&gt; from prophetverse.sktime.multivariate import HierarchicalProphet\n&gt;&gt;&gt; agg = Aggregator()\n&gt;&gt;&gt; y = _bottom_hier_datagen(\n...     no_bottom_nodes=3,\n...     no_levels=1,\n...     random_seed=123,\n...     length=7,\n... )\n&gt;&gt;&gt; y = agg.fit_transform(y)\n&gt;&gt;&gt; forecaster = HierarchicalProphet()\n&gt;&gt;&gt; forecaster = forecaster.fit(y)\n&gt;&gt;&gt; y_pred = forecaster.predict(fh=[1])\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nn_series\nGet the number of series.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nget_test_params\nParams to be used in sktime unit tests.\n\n\npredict_samples\nGenerate samples for the given exogenous variables and forecasting horizon.\n\n\n\n\n\nsktime.HierarchicalProphet.get_test_params(parameter_set='default')\nParams to be used in sktime unit tests.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nparameter_set\nstr\nThe parameter set to be used (ignored in this implementation)\n'default'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nList[dict[str, int]]\nA list of dictionaries containing the test parameters.\n\n\n\n\n\n\n\nsktime.HierarchicalProphet.predict_samples(fh, X=None)\nGenerate samples for the given exogenous variables and forecasting horizon.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\npd.DataFrame\nExogenous variables.\nNone\n\n\nfh\nForecastingHorizon\nForecasting horizon.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nPredicted samples.",
    "crumbs": [
      "Sktime",
      "HierarchicalProphet"
    ]
  },
  {
    "objectID": "reference/HierarchicalProphet.html#parameters",
    "href": "reference/HierarchicalProphet.html#parameters",
    "title": "HierarchicalProphet",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ntrend\nUnion[BaseEffect, str]\nTrend component of the model.\n\"linear\"\n\n\nfeature_transformer\nBaseTransformer\nTransformer for features preprocessing.\nNone\n\n\nexogenous_effects\noptional\nEffects to model exogenous variables.\nNone\n\n\ndefault_effect\noptional\nDefault effect specification.\nNone\n\n\nshared_features\noptional\nFeatures shared across time series.\nNone\n\n\nnoise_scale\nfloat\nScale parameter for the noise distribution.\n0.05\n\n\ncorrelation_matrix_concentration\nfloat\nConcentration parameter for the correlation matrix.\n1.0\n\n\nrng_key\noptional\nRandom number generator key.\nNone\n\n\ninference_engine\noptional\nEngine used for inference.\nNone",
    "crumbs": [
      "Sktime",
      "HierarchicalProphet"
    ]
  },
  {
    "objectID": "reference/HierarchicalProphet.html#examples",
    "href": "reference/HierarchicalProphet.html#examples",
    "title": "HierarchicalProphet",
    "section": "",
    "text": "&gt;&gt;&gt; from sktime.forecasting.naive import NaiveForecaster\n&gt;&gt;&gt; from sktime.transformations.hierarchical.aggregate import Aggregator\n&gt;&gt;&gt; from sktime.utils._testing.hierarchical import _bottom_hier_datagen\n&gt;&gt;&gt; from prophetverse.sktime.multivariate import HierarchicalProphet\n&gt;&gt;&gt; agg = Aggregator()\n&gt;&gt;&gt; y = _bottom_hier_datagen(\n...     no_bottom_nodes=3,\n...     no_levels=1,\n...     random_seed=123,\n...     length=7,\n... )\n&gt;&gt;&gt; y = agg.fit_transform(y)\n&gt;&gt;&gt; forecaster = HierarchicalProphet()\n&gt;&gt;&gt; forecaster = forecaster.fit(y)\n&gt;&gt;&gt; y_pred = forecaster.predict(fh=[1])",
    "crumbs": [
      "Sktime",
      "HierarchicalProphet"
    ]
  },
  {
    "objectID": "reference/HierarchicalProphet.html#attributes",
    "href": "reference/HierarchicalProphet.html#attributes",
    "title": "HierarchicalProphet",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nn_series\nGet the number of series.",
    "crumbs": [
      "Sktime",
      "HierarchicalProphet"
    ]
  },
  {
    "objectID": "reference/HierarchicalProphet.html#methods",
    "href": "reference/HierarchicalProphet.html#methods",
    "title": "HierarchicalProphet",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nget_test_params\nParams to be used in sktime unit tests.\n\n\npredict_samples\nGenerate samples for the given exogenous variables and forecasting horizon.\n\n\n\n\n\nsktime.HierarchicalProphet.get_test_params(parameter_set='default')\nParams to be used in sktime unit tests.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nparameter_set\nstr\nThe parameter set to be used (ignored in this implementation)\n'default'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nList[dict[str, int]]\nA list of dictionaries containing the test parameters.\n\n\n\n\n\n\n\nsktime.HierarchicalProphet.predict_samples(fh, X=None)\nGenerate samples for the given exogenous variables and forecasting horizon.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\npd.DataFrame\nExogenous variables.\nNone\n\n\nfh\nForecastingHorizon\nForecasting horizon.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nPredicted samples.",
    "crumbs": [
      "Sktime",
      "HierarchicalProphet"
    ]
  },
  {
    "objectID": "reference/GeometricAdstockEffect.html",
    "href": "reference/GeometricAdstockEffect.html",
    "title": "GeometricAdstockEffect",
    "section": "",
    "text": "effects.GeometricAdstockEffect(\n    self,\n    decay_prior=None,\n    raise_error_if_fh_changes=False,\n)\nRepresents a Geometric Adstock effect in a time series model.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndecay_prior\nDistribution\nPrior distribution for the decay parameter (controls the rate of decay).\nNone\n\n\nrase_error_if_fh_changes\nbool\nWhether to raise an error if the forecasting horizon changes during predict\nrequired",
    "crumbs": [
      "Exogenous effects",
      "GeometricAdstockEffect"
    ]
  },
  {
    "objectID": "reference/GeometricAdstockEffect.html#parameters",
    "href": "reference/GeometricAdstockEffect.html#parameters",
    "title": "GeometricAdstockEffect",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ndecay_prior\nDistribution\nPrior distribution for the decay parameter (controls the rate of decay).\nNone\n\n\nrase_error_if_fh_changes\nbool\nWhether to raise an error if the forecasting horizon changes during predict\nrequired",
    "crumbs": [
      "Exogenous effects",
      "GeometricAdstockEffect"
    ]
  },
  {
    "objectID": "reference/ChainedEffects.html",
    "href": "reference/ChainedEffects.html",
    "title": "ChainedEffects",
    "section": "",
    "text": "effects.ChainedEffects(self, steps)\nChains multiple effects sequentially, applying them one after the other.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsteps\nList[BaseEffect]\nA list of effects to be applied sequentially.\nrequired",
    "crumbs": [
      "Exogenous effects",
      "ChainedEffects"
    ]
  },
  {
    "objectID": "reference/ChainedEffects.html#parameters",
    "href": "reference/ChainedEffects.html#parameters",
    "title": "ChainedEffects",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nsteps\nList[BaseEffect]\nA list of effects to be applied sequentially.\nrequired",
    "crumbs": [
      "Exogenous effects",
      "ChainedEffects"
    ]
  },
  {
    "objectID": "reference/PiecewiseLinearTrend.html",
    "href": "reference/PiecewiseLinearTrend.html",
    "title": "PiecewiseLinearTrend",
    "section": "",
    "text": "effects.PiecewiseLinearTrend(\n    self,\n    changepoint_interval=25,\n    changepoint_range=0.8,\n    changepoint_prior_scale=0.001,\n    offset_prior_scale=0.1,\n    squeeze_if_single_series=True,\n    remove_seasonality_before_suggesting_initial_vals=True,\n    global_rate_prior_loc=None,\n    offset_prior_loc=None,\n)\nPiecewise Linear Trend model.\nThis model assumes that the trend is piecewise linear, with changepoints at regular intervals. The number of changepoints is determined by the changepoint_interval and changepoint_range parameters. The changepoint_interval parameter specifies the interval between changepoints, while the changepoint_range parameter specifies the range of the changepoints.\nThis implementation is based on the Prophet_ library. The initial values (global rate and global offset) are suggested using the maximum and minimum values of the time series data.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nchangepoint_interval\nint\nThe interval between changepoints.\n25\n\n\nchangepoint_range\nint\nThe range of the changepoints.\n0.8\n\n\nchangepoint_prior_scale\ndist.Distribution\nThe prior scale for the changepoints.\n0.001\n\n\noffset_prior_scale\nfloat\nThe prior scale for the offset. Default is 0.1.\n0.1\n\n\nsqueeze_if_single_series\nbool\nIf True, squeeze the output if there is only one series. Default is True.\nTrue\n\n\nremove_seasonality_before_suggesting_initial_vals\nbool\nIf True, remove seasonality before suggesting initial values, using sktime’s detrender. Default is True.\nTrue\n\n\nglobal_rate_prior_loc\nfloat\nThe prior location for the global rate. Default is suggested empirically from data.\nNone\n\n\noffset_prior_loc\nfloat\nThe prior location for the offset. Default is suggested empirically from data.\nNone\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nn_changepoint_per_series\nGet the number of changepoints per series.\n\n\nn_changepoints\nGet the total number of changepoints.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nget_changepoint_matrix\nReturn the changepoint matrix for the given index.\n\n\n\n\n\neffects.PiecewiseLinearTrend.get_changepoint_matrix(idx)\nReturn the changepoint matrix for the given index.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nidx\npd.PeriodIndex\nThe index for which to compute the changepoint matrix.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\njnp.ndarray: The changepoint matrix.",
    "crumbs": [
      "Trends",
      "PiecewiseLinearTrend"
    ]
  },
  {
    "objectID": "reference/PiecewiseLinearTrend.html#parameters",
    "href": "reference/PiecewiseLinearTrend.html#parameters",
    "title": "PiecewiseLinearTrend",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nchangepoint_interval\nint\nThe interval between changepoints.\n25\n\n\nchangepoint_range\nint\nThe range of the changepoints.\n0.8\n\n\nchangepoint_prior_scale\ndist.Distribution\nThe prior scale for the changepoints.\n0.001\n\n\noffset_prior_scale\nfloat\nThe prior scale for the offset. Default is 0.1.\n0.1\n\n\nsqueeze_if_single_series\nbool\nIf True, squeeze the output if there is only one series. Default is True.\nTrue\n\n\nremove_seasonality_before_suggesting_initial_vals\nbool\nIf True, remove seasonality before suggesting initial values, using sktime’s detrender. Default is True.\nTrue\n\n\nglobal_rate_prior_loc\nfloat\nThe prior location for the global rate. Default is suggested empirically from data.\nNone\n\n\noffset_prior_loc\nfloat\nThe prior location for the offset. Default is suggested empirically from data.\nNone",
    "crumbs": [
      "Trends",
      "PiecewiseLinearTrend"
    ]
  },
  {
    "objectID": "reference/PiecewiseLinearTrend.html#attributes",
    "href": "reference/PiecewiseLinearTrend.html#attributes",
    "title": "PiecewiseLinearTrend",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nn_changepoint_per_series\nGet the number of changepoints per series.\n\n\nn_changepoints\nGet the total number of changepoints.",
    "crumbs": [
      "Trends",
      "PiecewiseLinearTrend"
    ]
  },
  {
    "objectID": "reference/PiecewiseLinearTrend.html#methods",
    "href": "reference/PiecewiseLinearTrend.html#methods",
    "title": "PiecewiseLinearTrend",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nget_changepoint_matrix\nReturn the changepoint matrix for the given index.\n\n\n\n\n\neffects.PiecewiseLinearTrend.get_changepoint_matrix(idx)\nReturn the changepoint matrix for the given index.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nidx\npd.PeriodIndex\nThe index for which to compute the changepoint matrix.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\njnp.ndarray: The changepoint matrix.",
    "crumbs": [
      "Trends",
      "PiecewiseLinearTrend"
    ]
  },
  {
    "objectID": "reference/FlatTrend.html",
    "href": "reference/FlatTrend.html",
    "title": "FlatTrend",
    "section": "",
    "text": "effects.FlatTrend(self, changepoint_prior_scale=0.1)\nFlat trend model.\nThe mean of the target variable is used as the prior location for the trend.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nchangepoint_prior_scale\nfloat\nThe scale of the prior distribution on the trend changepoints. Defaults to 0.1.\n0.1",
    "crumbs": [
      "Trends",
      "FlatTrend"
    ]
  },
  {
    "objectID": "reference/FlatTrend.html#parameters",
    "href": "reference/FlatTrend.html#parameters",
    "title": "FlatTrend",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nchangepoint_prior_scale\nfloat\nThe scale of the prior distribution on the trend changepoints. Defaults to 0.1.\n0.1",
    "crumbs": [
      "Trends",
      "FlatTrend"
    ]
  },
  {
    "objectID": "reference/HillEffect.html",
    "href": "reference/HillEffect.html",
    "title": "HillEffect",
    "section": "",
    "text": "effects.HillEffect(\n    self,\n    effect_mode='multiplicative',\n    half_max_prior=None,\n    slope_prior=None,\n    max_effect_prior=None,\n    offset_slope=0.0,\n    input_scale=1.0,\n    base_effect_name='trend',\n)\nRepresents a Hill effect in a time series model.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nhalf_max_prior\nDistribution\nPrior distribution for the half-maximum parameter\nNone\n\n\nslope_prior\nDistribution\nPrior distribution for the slope parameter\nNone\n\n\nmax_effect_prior\nDistribution\nPrior distribution for the maximum effect parameter\nNone\n\n\neffect_mode\neffects_application\nMode of the effect (either “additive” or “multiplicative”)\n'multiplicative'",
    "crumbs": [
      "Exogenous effects",
      "HillEffect"
    ]
  },
  {
    "objectID": "reference/HillEffect.html#parameters",
    "href": "reference/HillEffect.html#parameters",
    "title": "HillEffect",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nhalf_max_prior\nDistribution\nPrior distribution for the half-maximum parameter\nNone\n\n\nslope_prior\nDistribution\nPrior distribution for the slope parameter\nNone\n\n\nmax_effect_prior\nDistribution\nPrior distribution for the maximum effect parameter\nNone\n\n\neffect_mode\neffects_application\nMode of the effect (either “additive” or “multiplicative”)\n'multiplicative'",
    "crumbs": [
      "Exogenous effects",
      "HillEffect"
    ]
  },
  {
    "objectID": "reference/PiecewiseLogisticTrend.html",
    "href": "reference/PiecewiseLogisticTrend.html",
    "title": "PiecewiseLogisticTrend",
    "section": "",
    "text": "effects.PiecewiseLogisticTrend(\n    self,\n    changepoint_interval=25,\n    changepoint_range=0.8,\n    changepoint_prior_scale=0.001,\n    offset_prior_scale=10,\n    capacity_prior=None,\n    squeeze_if_single_series=True,\n    remove_seasonality_before_suggesting_initial_vals=True,\n    global_rate_prior_loc=None,\n    offset_prior_loc=None,\n)\nPiecewise logistic trend model.\nThis logistic trend differs from the original Prophet logistic trend in that it considers a capacity prior distribution. The capacity prior distribution is used to estimate the maximum value that the time series trend can reach.\nIt uses internally the piecewise linear trend model, and then applies a logistic function to the output of the linear trend model.\nThe initial values (global rate and global offset) are suggested using the maximum and minimum values of the time series data.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nchangepoint_interval\nint\nThe interval between changepoints.\n25\n\n\nchangepoint_range\nint\nThe range of the changepoints.\n0.8\n\n\nchangepoint_prior_scale\ndist.Distribution\nThe prior scale for the changepoints.\n0.001\n\n\noffset_prior_scale\nfloat\nThe prior scale for the offset. Default is 0.1.\n10\n\n\nsqueeze_if_single_series\nbool\nIf True, squeeze the output if there is only one series. Default is True.\nTrue\n\n\nremove_seasonality_before_suggesting_initial_vals\nbool\nIf True, remove seasonality before suggesting initial values, using sktime’s detrender. Default is True.\nTrue\n\n\ncapacity_prior\ndist.Distribution\nThe prior distribution for the capacity. Default is a HalfNormal distribution with loc=1.05 and scale=1.\nNone",
    "crumbs": [
      "Trends",
      "PiecewiseLogisticTrend"
    ]
  },
  {
    "objectID": "reference/PiecewiseLogisticTrend.html#parameters",
    "href": "reference/PiecewiseLogisticTrend.html#parameters",
    "title": "PiecewiseLogisticTrend",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nchangepoint_interval\nint\nThe interval between changepoints.\n25\n\n\nchangepoint_range\nint\nThe range of the changepoints.\n0.8\n\n\nchangepoint_prior_scale\ndist.Distribution\nThe prior scale for the changepoints.\n0.001\n\n\noffset_prior_scale\nfloat\nThe prior scale for the offset. Default is 0.1.\n10\n\n\nsqueeze_if_single_series\nbool\nIf True, squeeze the output if there is only one series. Default is True.\nTrue\n\n\nremove_seasonality_before_suggesting_initial_vals\nbool\nIf True, remove seasonality before suggesting initial values, using sktime’s detrender. Default is True.\nTrue\n\n\ncapacity_prior\ndist.Distribution\nThe prior distribution for the capacity. Default is a HalfNormal distribution with loc=1.05 and scale=1.\nNone",
    "crumbs": [
      "Trends",
      "PiecewiseLogisticTrend"
    ]
  },
  {
    "objectID": "reference/ExactLikelihood.html",
    "href": "reference/ExactLikelihood.html",
    "title": "ExactLikelihood",
    "section": "",
    "text": "effects.ExactLikelihood(self, effect_name, reference_df, prior_scale)\nWrap an effect and applies a normal likelihood to its output.\nThis class uses an input as a reference for the effect, and applies a normal likelihood to the output of the effect.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\neffect_name\nstr\nThe effect to use in the likelihood.\nrequired\n\n\nreference_df\npd.DataFrame\nA dataframe with the reference values. Should be in sktime format, and must have the same index as the input data.\nrequired\n\n\nprior_scale\nfloat\nThe scale of the prior distribution for the likelihood.\nrequired",
    "crumbs": [
      "MMM Likelihoods",
      "ExactLikelihood"
    ]
  },
  {
    "objectID": "reference/ExactLikelihood.html#parameters",
    "href": "reference/ExactLikelihood.html#parameters",
    "title": "ExactLikelihood",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\neffect_name\nstr\nThe effect to use in the likelihood.\nrequired\n\n\nreference_df\npd.DataFrame\nA dataframe with the reference values. Should be in sktime format, and must have the same index as the input data.\nrequired\n\n\nprior_scale\nfloat\nThe scale of the prior distribution for the likelihood.\nrequired",
    "crumbs": [
      "MMM Likelihoods",
      "ExactLikelihood"
    ]
  },
  {
    "objectID": "reference/index.html#exogenous-effects",
    "href": "reference/index.html#exogenous-effects",
    "title": "Function reference",
    "section": "",
    "text": "Exogenous effects\n\n\n\nLinearEffect\nRepresents a linear effect in a hierarchical prophet model.\n\n\nLinearFourierSeasonality\nLinear Fourier Seasonality effect.\n\n\nLogEffect\nRepresents a log effect as effect = scale * log(rate * data + 1).\n\n\nHillEffect\nRepresents a Hill effect in a time series model.\n\n\nChainedEffects\nChains multiple effects sequentially, applying them one after the other.\n\n\nGeometricAdstockEffect\nRepresents a Geometric Adstock effect in a time series model.",
    "crumbs": [
      "Function reference"
    ]
  },
  {
    "objectID": "reference/index.html#mmm-likelihoods",
    "href": "reference/index.html#mmm-likelihoods",
    "title": "Function reference",
    "section": "",
    "text": "MMM Likelihoods\n\n\n\nLiftExperimentLikelihood\nWrap an effect and applies a normal likelihood to its output.\n\n\nExactLikelihood\nWrap an effect and applies a normal likelihood to its output.",
    "crumbs": [
      "Function reference"
    ]
  },
  {
    "objectID": "reference/index.html#trends",
    "href": "reference/index.html#trends",
    "title": "Function reference",
    "section": "",
    "text": "Trends\n\n\n\nPiecewiseLinearTrend\nPiecewise Linear Trend model.\n\n\nPiecewiseLogisticTrend\nPiecewise logistic trend model.\n\n\nFlatTrend\nFlat trend model.",
    "crumbs": [
      "Function reference"
    ]
  },
  {
    "objectID": "reference/LiftExperimentLikelihood.html",
    "href": "reference/LiftExperimentLikelihood.html",
    "title": "LiftExperimentLikelihood",
    "section": "",
    "text": "effects.LiftExperimentLikelihood(\n    self,\n    effect,\n    lift_test_results,\n    prior_scale,\n    likelihood_scale=1,\n)\nWrap an effect and applies a normal likelihood to its output.\nThis class uses an input as a reference for the effect, and applies a normal likelihood to the output of the effect.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\neffect\nBaseEffect\nThe effect to wrap.\nrequired\n\n\nlift_test_results\npd.DataFrame\nA dataframe with the lift test results. Should be in sktime format, and must have the same index as the input data.\nrequired\n\n\nprior_scale\nfloat\nThe scale of the prior distribution for the likelihood.\nrequired",
    "crumbs": [
      "MMM Likelihoods",
      "LiftExperimentLikelihood"
    ]
  },
  {
    "objectID": "reference/LiftExperimentLikelihood.html#parameters",
    "href": "reference/LiftExperimentLikelihood.html#parameters",
    "title": "LiftExperimentLikelihood",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\neffect\nBaseEffect\nThe effect to wrap.\nrequired\n\n\nlift_test_results\npd.DataFrame\nA dataframe with the lift test results. Should be in sktime format, and must have the same index as the input data.\nrequired\n\n\nprior_scale\nfloat\nThe scale of the prior distribution for the likelihood.\nrequired",
    "crumbs": [
      "MMM Likelihoods",
      "LiftExperimentLikelihood"
    ]
  },
  {
    "objectID": "reference/LinearEffect.html",
    "href": "reference/LinearEffect.html",
    "title": "LinearEffect",
    "section": "",
    "text": "effects.LinearEffect(\n    self,\n    effect_mode='multiplicative',\n    prior=None,\n    broadcast=False,\n)\nRepresents a linear effect in a hierarchical prophet model.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nprior\nDistribution\nA numpyro distribution to use as prior. Defaults to dist.Normal(0, 1)\nNone\n\n\neffect_mode\neffects_application\nEither “multiplicative” or “additive” by default “multiplicative”.\n'multiplicative'",
    "crumbs": [
      "Exogenous effects",
      "LinearEffect"
    ]
  },
  {
    "objectID": "reference/LinearEffect.html#parameters",
    "href": "reference/LinearEffect.html#parameters",
    "title": "LinearEffect",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nprior\nDistribution\nA numpyro distribution to use as prior. Defaults to dist.Normal(0, 1)\nNone\n\n\neffect_mode\neffects_application\nEither “multiplicative” or “additive” by default “multiplicative”.\n'multiplicative'",
    "crumbs": [
      "Exogenous effects",
      "LinearEffect"
    ]
  },
  {
    "objectID": "reference/LogEffect.html",
    "href": "reference/LogEffect.html",
    "title": "LogEffect",
    "section": "",
    "text": "effects.LogEffect(\n    self,\n    effect_mode='multiplicative',\n    scale_prior=None,\n    rate_prior=None,\n)\nRepresents a log effect as effect = scale * log(rate * data + 1).\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nscale_prior\nOptional[Distribution]\nThe prior distribution for the scale parameter., by default Gamma\nNone\n\n\nrate_prior\nOptional[Distribution]\nThe prior distribution for the rate parameter., by default Gamma\nNone\n\n\neffect_mode\neffects_application\nEither “additive” or “multiplicative”, by default “multiplicative”\n'multiplicative'",
    "crumbs": [
      "Exogenous effects",
      "LogEffect"
    ]
  },
  {
    "objectID": "reference/LogEffect.html#parameters",
    "href": "reference/LogEffect.html#parameters",
    "title": "LogEffect",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nscale_prior\nOptional[Distribution]\nThe prior distribution for the scale parameter., by default Gamma\nNone\n\n\nrate_prior\nOptional[Distribution]\nThe prior distribution for the rate parameter., by default Gamma\nNone\n\n\neffect_mode\neffects_application\nEither “additive” or “multiplicative”, by default “multiplicative”\n'multiplicative'",
    "crumbs": [
      "Exogenous effects",
      "LogEffect"
    ]
  },
  {
    "objectID": "reference/NormalTargetLikelihood.html",
    "href": "reference/NormalTargetLikelihood.html",
    "title": "NormalTargetLikelihood",
    "section": "",
    "text": "NormalTargetLikelihood\neffects.NormalTargetLikelihood(self, noise_scale=0.05)",
    "crumbs": [
      "Target Likelihoods",
      "NormalTargetLikelihood"
    ]
  },
  {
    "objectID": "reference/NegativeBinomialTargetLikelihood.html",
    "href": "reference/NegativeBinomialTargetLikelihood.html",
    "title": "NegativeBinomialTargetLikelihood",
    "section": "",
    "text": "NegativeBinomialTargetLikelihood\neffects.NegativeBinomialTargetLikelihood(self, noise_scale=0.05, epsilon=1e-05)",
    "crumbs": [
      "Target Likelihoods",
      "NegativeBinomialTargetLikelihood"
    ]
  },
  {
    "objectID": "reference/GammaTargetLikelihood.html",
    "href": "reference/GammaTargetLikelihood.html",
    "title": "GammaTargetLikelihood",
    "section": "",
    "text": "GammaTargetLikelihood\neffects.GammaTargetLikelihood(self, noise_scale=0.05, epsilon=1e-05)",
    "crumbs": [
      "Target Likelihoods",
      "GammaTargetLikelihood"
    ]
  },
  {
    "objectID": "reference/MultivariateNormal.html",
    "href": "reference/MultivariateNormal.html",
    "title": "MultivariateNormal",
    "section": "",
    "text": "MultivariateNormal\neffects.MultivariateNormal(\n    self,\n    noise_scale=0.05,\n    correlation_matrix_concentration=1,\n)\nBase class for effects.",
    "crumbs": [
      "Target Likelihoods",
      "MultivariateNormal"
    ]
  },
  {
    "objectID": "reference/index.html#target-likelihoods",
    "href": "reference/index.html#target-likelihoods",
    "title": "Function reference",
    "section": "",
    "text": "Likelihoods for the target variable\n\n\n\nMultivariateNormal\nBase class for effects.\n\n\nNormalTargetLikelihood\n\n\n\nGammaTargetLikelihood\n\n\n\nNegativeBinomialTargetLikelihood",
    "crumbs": [
      "Function reference"
    ]
  },
  {
    "objectID": "reference/LinearFourierSeasonality.html",
    "href": "reference/LinearFourierSeasonality.html",
    "title": "LinearFourierSeasonality",
    "section": "",
    "text": "effects.LinearFourierSeasonality(\n    self,\n    sp_list,\n    fourier_terms_list,\n    freq,\n    prior_scale=1.0,\n    effect_mode='additive',\n)\nLinear Fourier Seasonality effect.\nCompute the linear seasonality using Fourier features.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsp_list\nList[float]\nList of seasonal periods.\nrequired\n\n\nfourier_terms_list\nList[int]\nList of number of Fourier terms to use for each seasonal period.\nrequired\n\n\nfreq\nstr\nFrequency of the time series. Example: “D” for daily, “W” for weekly, etc.\nrequired\n\n\nprior_scale\nfloat\nScale of the prior distribution for the effect, by default 1.0.\n1.0\n\n\neffect_mode\nstr\nEither “multiplicative” or “additive” by default “additive”.\n'additive'",
    "crumbs": [
      "Exogenous effects",
      "LinearFourierSeasonality"
    ]
  },
  {
    "objectID": "reference/LinearFourierSeasonality.html#parameters",
    "href": "reference/LinearFourierSeasonality.html#parameters",
    "title": "LinearFourierSeasonality",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nsp_list\nList[float]\nList of seasonal periods.\nrequired\n\n\nfourier_terms_list\nList[int]\nList of number of Fourier terms to use for each seasonal period.\nrequired\n\n\nfreq\nstr\nFrequency of the time series. Example: “D” for daily, “W” for weekly, etc.\nrequired\n\n\nprior_scale\nfloat\nScale of the prior distribution for the effect, by default 1.0.\n1.0\n\n\neffect_mode\nstr\nEither “multiplicative” or “additive” by default “additive”.\n'additive'",
    "crumbs": [
      "Exogenous effects",
      "LinearFourierSeasonality"
    ]
  },
  {
    "objectID": "development.html",
    "href": "development.html",
    "title": "Contributing to Prophetverse",
    "section": "",
    "text": "If you are brand new to Prophetverse or open-source development, we recommend searching the GitHub “issues” tab to find issues that interest you. Unassigned issues labeled Docs and good first issue are typically good for newer contributors.\nOnce you’ve found an interesting issue, it’s a good idea to assign the issue to yourself, so nobody else duplicates the work on it. On the Github issue, a comment with the exact text take to automatically assign you the issue (this will take seconds and may require refreshing the page to see it).\nIf for whatever reason you are not able to continue working with the issue, please unassign it, so other people know it’s available again. You can check the list of assigned issues, since people may not be working in them anymore. If you want to work on one that is assigned, feel free to kindly ask the current assignee if you can take it (please allow at least a week of inactivity before considering work in the issue discontinued). To submit your contribution make a pull request"
  },
  {
    "objectID": "development.html#finding-an-issue-to-contribute-to",
    "href": "development.html#finding-an-issue-to-contribute-to",
    "title": "Contributing to Prophetverse",
    "section": "",
    "text": "If you are brand new to Prophetverse or open-source development, we recommend searching the GitHub “issues” tab to find issues that interest you. Unassigned issues labeled Docs and good first issue are typically good for newer contributors.\nOnce you’ve found an interesting issue, it’s a good idea to assign the issue to yourself, so nobody else duplicates the work on it. On the Github issue, a comment with the exact text take to automatically assign you the issue (this will take seconds and may require refreshing the page to see it).\nIf for whatever reason you are not able to continue working with the issue, please unassign it, so other people know it’s available again. You can check the list of assigned issues, since people may not be working in them anymore. If you want to work on one that is assigned, feel free to kindly ask the current assignee if you can take it (please allow at least a week of inactivity before considering work in the issue discontinued). To submit your contribution make a pull request"
  },
  {
    "objectID": "development.html#tips-for-a-successful-pull-request",
    "href": "development.html#tips-for-a-successful-pull-request",
    "title": "Contributing to Prophetverse",
    "section": "Tips for a successful pull request",
    "text": "Tips for a successful pull request\nIf you have made it to the Making a pull request phase, one of the core contributors may take a look."
  },
  {
    "objectID": "development.html#what-is-a-good-pull-request",
    "href": "development.html#what-is-a-good-pull-request",
    "title": "Contributing to Prophetverse",
    "section": "What is a good pull request?",
    "text": "What is a good pull request?\n\nReference an open issue for non-trivial changes to clarify the PR’s purpose\nEnsure you have appropriate tests. These should be the first part of any PR\nKeep your pull requests as simple as possible. Larger PRs take longer to review\nEnsure that CI is in a green state. Reviewers may not even look otherwise\nKeep Updating your pull request, either by request or every few days"
  },
  {
    "objectID": "development.html#creating-a-development-environment",
    "href": "development.html#creating-a-development-environment",
    "title": "Contributing to Prophetverse",
    "section": "Creating a development environment",
    "text": "Creating a development environment\n\nStep 1: install python\nStep 1: install poetry\nStep 2: install dev dependencies  poetry install --extras dev"
  },
  {
    "objectID": "development.html#contributing-to-the-documentation",
    "href": "development.html#contributing-to-the-documentation",
    "title": "Contributing to Prophetverse",
    "section": "Contributing to the documentation",
    "text": "Contributing to the documentation\nContributing to the documentation benefits everyone who uses Prophetverse. We encourage you to help us improve the documentation, and you don’t have to be an expert on Prophetverse to do so! In fact, there are sections of the docs that are worse off after being written by experts. If something in the docs doesn’t make sense to you, updating the relevant section after you figure it out is a great way to ensure it will help the next person."
  },
  {
    "objectID": "development.html#about-the-documentation",
    "href": "development.html#about-the-documentation",
    "title": "Contributing to Prophetverse",
    "section": "About the documentation:",
    "text": "About the documentation:\nThe documentation is written in mkdocs, you can learn more at mkdocs getting started guide."
  },
  {
    "objectID": "development.html#contributor-community",
    "href": "development.html#contributor-community",
    "title": "Contributing to Prophetverse",
    "section": "Contributor community :",
    "text": "Contributor community :\nCommunity slack: None yet."
  },
  {
    "objectID": "development.html#contributing-to-the-code-base",
    "href": "development.html#contributing-to-the-code-base",
    "title": "Contributing to Prophetverse",
    "section": "Contributing to the code base",
    "text": "Contributing to the code base\n\nCode standards\nWriting good code is not just about what you write. It is also about how you write it. During Continuous Integration testing, several tools will be run to check your code for stylistic errors. Generating any warnings will cause the test to fail. Thus, good style is a requirement for submitting code to Prophetverse.There are of tools in Prophetverse to help contributors verify their changes before contributing to the project\n\nPytest\nYou can test your code with pytest integration with the poetry command  poetry run pytest\nThe CI tests are computationally intensive, so if you want to do a faster test you can run a smoke test with the command  poetry run pytest -m \"not ci\"\nIf you also wanna run the tests even faster feel free to parallel processing the tests with pytest-xdist.\n\n\nPre-commit\nAdditionally, Continuous Integration will run code formatting checks like black, isort, and mypy and more using pre-commit hooks. Any warnings from these checks will cause the Continuous Integration to fail; therefore, it is helpful to run the check yourself before submitting code. This can be done by installing pre-commit (which should already have happened if you followed the instructions in Setting up your development environment) and then running:\n\npre-commit install\nfrom the root of the Prophetverse repository. Now all of the styling checks will be run each time you commit changes without your needing to run each one manually. In addition, using pre-commit will also allow you to more easily remain up-to-date with our code checks as they change.\n\n\npre-commit usage\nNote that if needed, you can skip these checks with git commit –no-verify.\nIf you don’t want to use pre-commit as part of your workflow, you can still use it to run its checks with one of the following:\n pre-commit run --files &lt;files you have modified&gt;   pre-commit run --from-ref=upstream/main --to-ref=HEAD --all-files \nwithout needing to have done pre-commit install beforehand.\nFinally, we also have some slow pre-commit checks, which don’t run on each commit but which do run during continuous integration. You can trigger them manually with:\n pre-commit run --hook-stage manual --all-files"
  },
  {
    "objectID": "howto/effect_api_intro.html",
    "href": "howto/effect_api_intro.html",
    "title": "Customizing exogenous effects",
    "section": "",
    "text": "The exogenous effect API allows you to create custom exogenous components for the Prophetverse model. This is useful when we want to model specific patterns or relationships between the exogenous variables and the target variable. For example, enforcing a positive effect of a variable on the mean, or modeling a non-linear relationship.\nIf you have read the theory section, by effect we mean each function \\(f_i\\). You can implement those custom functions by subclassing the BaseEffect class, and then use them in the Prophetverse model. Some effects are already implemented in the library, and you can find them in the prophetverse.effects module.\nWhen creating a model instance, effects can be specified through exogenous_effects parameter of the Prophetverse model. This parameter is a list of tuples of three values: the name, the effect object, and a regex to filter columns related to that effect. The regex is what defines \\(x_i\\) in the previous section. The prophetverse.utils.regex module provides some useful functions to create regex patterns for common use cases, include starts_with, ends_with, contains, and no_input_columns.\nFor example:\nThe effects can be any object that implements the BaseEffect interface, and you can create your own effects by subclassing BaseEffect and implementing _fit, _transform and _predict methods.",
    "crumbs": [
      "Introduction to Effects"
    ]
  },
  {
    "objectID": "howto/effect_api_intro.html#example",
    "href": "howto/effect_api_intro.html#example",
    "title": "Customizing exogenous effects",
    "section": "Example",
    "text": "Example\n\nLog Effect\nThe BaseAdditiveOrMultiplicativeEffect provides an init argument effect_mode that allows you to specify if the effect is additive or multiplicative. Let’s take as an example the LogEffect:\n#prophetverse/effects/log.py\n\nfrom typing import Dict, Optional\n\nimport jax.numpy as jnp\nimport numpyro\nfrom numpyro import distributions as dist\nfrom numpyro.distributions import Distribution\n\nfrom prophetverse.effects.base import (\n    EFFECT_APPLICATION_TYPE,\n    BaseAdditiveOrMultiplicativeEffect,\n)\n\n__all__ = [\"LogEffect\"]\n\n\nclass LogEffect(BaseAdditiveOrMultiplicativeEffect):\n    \"\"\"Represents a log effect as effect = scale * log(rate * data + 1).\n\n    Parameters\n    ----------\n    scale_prior : Optional[Distribution], optional\n        The prior distribution for the scale parameter., by default Gamma\n    rate_prior : Optional[Distribution], optional\n        The prior distribution for the rate parameter., by default Gamma\n    effect_mode : effects_application, optional\n        Either \"additive\" or \"multiplicative\", by default \"multiplicative\"\n    \"\"\"\n\n    def __init__(\n        self,\n        effect_mode: EFFECT_APPLICATION_TYPE = \"multiplicative\",\n        scale_prior: Optional[Distribution] = None,\n        rate_prior: Optional[Distribution] = None,\n    ):\n        self.scale_prior = scale_prior or dist.Gamma(1, 1)\n        self.rate_prior = rate_prior or dist.Gamma(1, 1)\n        super().__init__(effect_mode=effect_mode)\n\n    def _predict(  # type: ignore[override]\n        self,\n        data: jnp.ndarray,\n        predicted_effects: Optional[Dict[str, jnp.ndarray]] = None,\n    ) -&gt; jnp.ndarray:\n        \"\"\"Apply and return the effect values.\n\n        Parameters\n        ----------\n        data : Any\n            Data obtained from the transformed method.\n\n        predicted_effects : Dict[str, jnp.ndarray], optional\n            A dictionary containing the predicted effects, by default None.\n\n        Returns\n        -------\n        jnp.ndarray\n            An array with shape (T,1) for univariate timeseries, or (N, T, 1) for\n            multivariate timeseries, where T is the number of timepoints and N is the\n            number of series.\n        \"\"\"\n        scale = numpyro.sample(\"log_scale\", self.scale_prior)\n        rate = numpyro.sample(\"log_rate\", self.rate_prior)\n        effect = scale * jnp.log(jnp.clip(rate * data + 1, 1e-8, None))\n\n        return effect\nThe _fit and _transform methods are not implemented, and the default behaviour is preserved (the columns of the dataframe that match the regex pattern are selected, and the result is converted to a jnp.ndarray with key “data”).\n\n\nComposition of effects\nWe can go further and create a custom effect that adds a likelihood term to the model. The LiftExperimentLikelihood tackles the use case of having a lift experiment, and wanting to incorporate it to guide the exogenous effect. The likelihood term is added in the _predict method, and the observed lift preprocessed in _transform method. The attribute input_feature_column_names is also overriden to return the input feature columns of the inner effect.\n\n\n\"\"\"Composition of effects (Effects that wrap other effects).\"\"\"\n\nfrom typing import Any, Dict, List\n\nimport jax.numpy as jnp\nimport numpyro\nimport numpyro.distributions as dist\nimport pandas as pd\n\nfrom prophetverse.utils.frame_to_array import series_to_tensor_or_array\n\nfrom .base import BaseEffect\n\n__all__ = [\"LiftExperimentLikelihood\"]\n\n\nclass LiftExperimentLikelihood(BaseEffect):\n    \"\"\"Wrap an effect and applies a normal likelihood to its output.\n\n    This class uses an input as a reference for the effect, and applies a normal\n    likelihood to the output of the effect.\n\n    Parameters\n    ----------\n    effect : BaseEffect\n        The effect to wrap.\n    lift_test_results : pd.DataFrame\n        A dataframe with the lift test results. Should be in sktime format, and must\n        have the same index as the input data.\n    prior_scale : float\n        The scale of the prior distribution for the likelihood.\n    \"\"\"\n\n    _tags = {\"requires_X\": False, \"capability:panel\": False}\n\n    def __init__(\n        self,\n        effect: BaseEffect,\n        lift_test_results: pd.DataFrame,\n        prior_scale: float,\n    ):\n\n        self.effect = effect\n        self.lift_test_results = lift_test_results\n        self.prior_scale = prior_scale\n\n        assert self.prior_scale &gt; 0, \"prior_scale must be greater than 0\"\n\n        super().__init__()\n\n    def fit(self, y: pd.DataFrame, X: pd.DataFrame, scale: float = 1):\n        \"\"\"Initialize the effect.\n\n        This method is called during `fit()` of the forecasting model.\n        It receives the Exogenous variables DataFrame and should be used to initialize\n        any necessary parameters or data structures, such as detecting the columns that\n        match the regex pattern.\n\n\n        Parameters\n        ----------\n        y : pd.DataFrame\n            The timeseries dataframe\n\n        X : pd.DataFrame\n            The DataFrame to initialize the effect.\n\n        scale : float, optional\n            The scale of the timeseries. For multivariate timeseries, this is\n            a dataframe. For univariate, it is a simple float.\n\n        Returns\n        -------\n        None\n        \"\"\"\n        self.effect.fit(X=X, y=y, scale=scale)\n        self.timeseries_scale = scale\n        super().fit(X=X, y=y, scale=scale)\n\n    def _transform(self, X: pd.DataFrame, fh: pd.Index) -&gt; Dict[str, Any]:\n        \"\"\"Prepare input data to be passed to numpyro model.\n\n        Returns a dictionary with the data for the lift and for the inner effect.\n\n        Parameters\n        ----------\n        X : pd.DataFrame\n            The input DataFrame containing the exogenous variables for the training\n            time indexes, if passed during fit, or for the forecasting time indexes, if\n            passed during predict.\n\n        fh : pd.Index\n            The forecasting horizon as a pandas Index.\n\n        Returns\n        -------\n        Dict[str, Any]\n            Dictionary with data for the lift and for the inner effect\n        \"\"\"\n        data_dict = {}\n        data_dict[\"inner_effect_data\"] = self.effect._transform(X, fh=fh)\n\n        X_lift = self.lift_test_results.reindex(fh, fill_value=jnp.nan)\n        lift_array = series_to_tensor_or_array(X_lift)\n        data_dict[\"observed_lift\"] = lift_array / self.timeseries_scale\n        data_dict[\"obs_mask\"] = ~jnp.isnan(data_dict[\"observed_lift\"])\n\n        return data_dict\n\n    def _predict(\n        self, data: Dict, predicted_effects: Dict[str, jnp.ndarray]\n    ) -&gt; jnp.ndarray:\n        \"\"\"Apply and return the effect values.\n\n        Parameters\n        ----------\n        data : Any\n            Data obtained from the transformed method.\n\n        predicted_effects : Dict[str, jnp.ndarray], optional\n            A dictionary containing the predicted effects, by default None.\n\n        Returns\n        -------\n        jnp.ndarray\n            An array with shape (T,1) for univariate timeseries.\n        \"\"\"\n        observed_lift = data[\"observed_lift\"]\n        obs_mask = data[\"obs_mask\"]\n\n        x = self.effect.predict(\n            data=data[\"inner_effect_data\"], predicted_effects=predicted_effects\n        )\n\n        numpyro.sample(\n            \"lift_experiment\",\n            dist.Normal(x, self.prior_scale),\n            obs=observed_lift,\n            obs_mask=obs_mask,\n        )\n\n        return x\n\n    @property\n    def input_feature_column_names(self) -&gt; List[str]:\n        \"\"\"Return the input feature columns names.\"\"\"\n        return self.effect._input_feature_column_names\nTo see more, check the custom effect example.",
    "crumbs": [
      "Introduction to Effects"
    ]
  }
]